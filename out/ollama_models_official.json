{
  "scraped_at": "2025-11-01T05:16:42.203354+00:00",
  "completed_at": "2025-11-01T05:16:54.412099+00:00",
  "duration_seconds": 12.21,
  "model_type": "official",
  "models": [
    {
      "slug": "llama3.1",
      "capabilities": [
        "reasoning",
        "tools"
      ],
      "pulls": 105200000,
      "pulls_text": "105.2M \n  Downloads",
      "blurb": "llama3.1 Llama 3.1 is a new state-of-the-art model from Meta available in 8B, 70B and 405B parameter sizes. 8b 70b 405b 93 Tags Updated 11 months ago",
      "name": "Llama 3.1 is a new state-of-the-art model from Meta available in 8B, 70B and 405B parameter sizes.",
      "description": "Readme Meta Llama 3.1 Llama 3.1 family of models available: 8B 70B 405B Llama 3.1 405B is the first openly available model that rivals the top AI models when it comes to state-of-the-art capabilities in general knowledge, steerability, math, tool use, and multilingual translation. The upgraded versions of the 8B and 70B models are multilingual and have a significantly longer context length of 128K, state-of-the-art tool use, and overall stronger reasoning capabilities. This enables Meta’s latest models to support advanced use cases, such as long-form text summarization, multilingual conversational agents, and coding assistants. Meta also has made changes to their license, allowing developers to use the outputs from Llama models, including the 405B model, to improve other models. Model evaluations For this release, Meta has evaluation the performance on over 150 benchmark datasets that span a wide range of languages. In addition, Meta performed extensive human evaluations that compare Llama 3.1 with competing models in real-world scenarios. Meta’s experimental evaluation suggests that our flagship model is competitive with leading foundation models across a range of tasks, including GPT-4, GPT-4o, and Claude 3.5 Sonnet. Additionally, Meta’s smaller models are competitive with closed and open models that have a similar number of parameters. References Meta AI Llama 3.1 launch blog post Write Preview ### Meta Llama 3.1 ![image.png](https://ollama.com/assets/mchiang0610/mikey3.1/4d0cab8e-952b-4c75-b110-1514d8db8fae) **Llama 3.1** family of models available: - **8B** - **70B** - **405B** Llama 3.1 405B is the first openly available model that rivals the top AI models when it comes to state-of-the-art capabilities in general knowledge, steerability, math, tool use, and multilingual translation. The upgraded versions of the 8B and 70B models are multilingual and have a significantly longer context length of 128K, state-of-the-art tool use, and overall stronger reasoning ca",
      "variants": [
        {
          "tag": "llama3.1:latest",
          "size_text": "4.9GB",
          "size_bytes": 5261334937,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:8b",
          "size_text": "4.9GB",
          "size_bytes": 5261334937,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:70b",
          "size_text": "43GB",
          "size_bytes": 46170898432,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:405b",
          "size_text": "243GB",
          "size_bytes": 260919263232,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:8b-instruct-q2_K",
          "size_text": "3.2GB",
          "size_bytes": 3435973836,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:8b-instruct-q3_K_S",
          "size_text": "3.7GB",
          "size_bytes": 3972844748,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:8b-instruct-q3_K_M",
          "size_text": "4.0GB",
          "size_bytes": 4294967296,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:8b-instruct-q3_K_L",
          "size_text": "4.3GB",
          "size_bytes": 4617089843,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:8b-instruct-q4_0",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:8b-instruct-q4_1",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:8b-instruct-q4_K_S",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:8b-instruct-q4_K_M",
          "size_text": "4.9GB",
          "size_bytes": 5261334937,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:8b-instruct-q5_0",
          "size_text": "5.6GB",
          "size_bytes": 6012954214,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:8b-instruct-q5_1",
          "size_text": "6.1GB",
          "size_bytes": 6549825126,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:8b-instruct-q5_K_S",
          "size_text": "5.6GB",
          "size_bytes": 6012954214,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:8b-instruct-q5_K_M",
          "size_text": "5.7GB",
          "size_bytes": 6120328396,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:8b-instruct-q6_K",
          "size_text": "6.6GB",
          "size_bytes": 7086696038,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:8b-instruct-q8_0",
          "size_text": "8.5GB",
          "size_bytes": 9126805504,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:8b-instruct-fp16",
          "size_text": "16GB",
          "size_bytes": 17179869184,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:8b-text-q2_K",
          "size_text": "3.2GB",
          "size_bytes": 3435973836,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:8b-text-q3_K_S",
          "size_text": "3.7GB",
          "size_bytes": 3972844748,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:8b-text-q3_K_M",
          "size_text": "4.0GB",
          "size_bytes": 4294967296,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:8b-text-q3_K_L",
          "size_text": "4.3GB",
          "size_bytes": 4617089843,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:8b-text-q4_0",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:8b-text-q4_1",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:8b-text-q4_K_S",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:8b-text-q4_K_M",
          "size_text": "4.9GB",
          "size_bytes": 5261334937,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:8b-text-q5_0",
          "size_text": "5.6GB",
          "size_bytes": 6012954214,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:8b-text-q5_1",
          "size_text": "6.1GB",
          "size_bytes": 6549825126,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:8b-text-q5_K_S",
          "size_text": "5.6GB",
          "size_bytes": 6012954214,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:8b-text-q5_K_M",
          "size_text": "5.7GB",
          "size_bytes": 6120328396,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:8b-text-q6_K",
          "size_text": "6.6GB",
          "size_bytes": 7086696038,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:8b-text-q8_0",
          "size_text": "8.5GB",
          "size_bytes": 9126805504,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:8b-text-fp16",
          "size_text": "16GB",
          "size_bytes": 17179869184,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:70b-instruct-q2_K",
          "size_text": "26GB",
          "size_bytes": 27917287424,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:70b-instruct-q3_K_S",
          "size_text": "31GB",
          "size_bytes": 33285996544,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:70b-instruct-q3_K_M",
          "size_text": "34GB",
          "size_bytes": 36507222016,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:70b-instruct-q3_K_L",
          "size_text": "37GB",
          "size_bytes": 39728447488,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:70b-instruct-q4_0",
          "size_text": "40GB",
          "size_bytes": 42949672960,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:70b-instruct-q4_K_S",
          "size_text": "40GB",
          "size_bytes": 42949672960,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:70b-instruct-q4_K_M",
          "size_text": "43GB",
          "size_bytes": 46170898432,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:70b-instruct-q5_0",
          "size_text": "49GB",
          "size_bytes": 52613349376,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:70b-instruct-q5_1",
          "size_text": "53GB",
          "size_bytes": 56908316672,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:70b-instruct-q5_K_S",
          "size_text": "49GB",
          "size_bytes": 52613349376,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:70b-instruct-q5_K_M",
          "size_text": "50GB",
          "size_bytes": 53687091200,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:70b-instruct-q6_K",
          "size_text": "58GB",
          "size_bytes": 62277025792,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:70b-instruct-q8_0",
          "size_text": "75GB",
          "size_bytes": 80530636800,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:70b-instruct-fp16",
          "size_text": "141GB",
          "size_bytes": 151397597184,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:70b-text-q2_K",
          "size_text": "26GB",
          "size_bytes": 27917287424,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:70b-text-q3_K_S",
          "size_text": "31GB",
          "size_bytes": 33285996544,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:70b-text-q3_K_M",
          "size_text": "34GB",
          "size_bytes": 36507222016,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:70b-text-q3_K_L",
          "size_text": "37GB",
          "size_bytes": 39728447488,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:70b-text-q4_0",
          "size_text": "40GB",
          "size_bytes": 42949672960,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:70b-text-q4_1",
          "size_text": "44GB",
          "size_bytes": 47244640256,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:70b-text-q4_K_S",
          "size_text": "40GB",
          "size_bytes": 42949672960,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:70b-text-q4_K_M",
          "size_text": "43GB",
          "size_bytes": 46170898432,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:70b-text-q5_0",
          "size_text": "49GB",
          "size_bytes": 52613349376,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:70b-text-q5_1",
          "size_text": "53GB",
          "size_bytes": 56908316672,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:70b-text-q5_K_S",
          "size_text": "49GB",
          "size_bytes": 52613349376,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:70b-text-q5_K_M",
          "size_text": "50GB",
          "size_bytes": 53687091200,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:70b-text-q6_K",
          "size_text": "58GB",
          "size_bytes": 62277025792,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:70b-text-q8_0",
          "size_text": "75GB",
          "size_bytes": 80530636800,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:70b-text-fp16",
          "size_text": "141GB",
          "size_bytes": 151397597184,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:405b-instruct-q2_K",
          "size_text": "149GB",
          "size_bytes": 159987531776,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:405b-instruct-q3_K_S",
          "size_text": "175GB",
          "size_bytes": 187904819200,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:405b-instruct-q3_K_M",
          "size_text": "195GB",
          "size_bytes": 209379655680,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:405b-instruct-q3_K_L",
          "size_text": "213GB",
          "size_bytes": 228707008512,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:405b-instruct-q4_0",
          "size_text": "229GB",
          "size_bytes": 245886877696,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:405b-instruct-q4_1",
          "size_text": "254GB",
          "size_bytes": 272730423296,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:405b-instruct-q4_K_S",
          "size_text": "231GB",
          "size_bytes": 248034361344,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:405b-instruct-q4_K_M",
          "size_text": "243GB",
          "size_bytes": 260919263232,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:405b-instruct-q5_0",
          "size_text": "279GB",
          "size_bytes": 299573968896,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:405b-instruct-q5_1",
          "size_text": "305GB",
          "size_bytes": 327491256320,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:405b-instruct-q5_K_S",
          "size_text": "279GB",
          "size_bytes": 299573968896,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:405b-instruct-q5_K_M",
          "size_text": "287GB",
          "size_bytes": 308163903488,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:405b-instruct-q6_K",
          "size_text": "333GB",
          "size_bytes": 357556027392,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:405b-instruct-q8_0",
          "size_text": "431GB",
          "size_bytes": 462782726144,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:405b-instruct-fp16",
          "size_text": "812GB",
          "size_bytes": 871878361088,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:405b-text-q2_K",
          "size_text": "149GB",
          "size_bytes": 159987531776,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:405b-text-q3_K_S",
          "size_text": "175GB",
          "size_bytes": 187904819200,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:405b-text-q3_K_M",
          "size_text": "195GB",
          "size_bytes": 209379655680,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:405b-text-q3_K_L",
          "size_text": "213GB",
          "size_bytes": 228707008512,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:405b-text-q4_0",
          "size_text": "229GB",
          "size_bytes": 245886877696,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:405b-text-q4_1",
          "size_text": "254GB",
          "size_bytes": 272730423296,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:405b-text-q4_K_S",
          "size_text": "231GB",
          "size_bytes": 248034361344,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:405b-text-q4_K_M",
          "size_text": "243GB",
          "size_bytes": 260919263232,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:405b-text-q5_0",
          "size_text": "279GB",
          "size_bytes": 299573968896,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:405b-text-q5_1",
          "size_text": "305GB",
          "size_bytes": 327491256320,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:405b-text-q5_K_S",
          "size_text": "279GB",
          "size_bytes": 299573968896,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:405b-text-q5_K_M",
          "size_text": "287GB",
          "size_bytes": 308163903488,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:405b-text-q6_K",
          "size_text": "333GB",
          "size_bytes": 357556027392,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:405b-text-q8_0",
          "size_text": "431GB",
          "size_bytes": 462782726144,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:405b-text-fp16",
          "size_text": "812GB",
          "size_bytes": 871878361088,
          "context": "128K",
          "input": "Text"
        }
      ],
      "tags_count": 93
    },
    {
      "slug": "deepseek-r1",
      "capabilities": [
        "reasoning",
        "thinking",
        "tools"
      ],
      "pulls": 68700000,
      "pulls_text": "68.7M \n  Downloads",
      "blurb": "deepseek-r1 DeepSeek-R1 is a family of open models with performance approaching that of leading models, such as O3 and Gemini 2.5 Pro. 1.5b 7b 8b 14b 32b 70b 671b 35 Tags Updated 4 months ago",
      "name": "DeepSeek-R1 is a family of open reasoning models with performance approaching that of leading models, such as O3 and Gemini 2.5 Pro.",
      "description": "Readme DeepSeek-R1 has received a minor version upgrade to DeepSeek-R1-0528 for the 8 billion parameter distilled model and the full 671 billion parameter model. In this update, DeepSeek R1 has significantly improved its reasoning and inference capabilities. The model has demonstrated outstanding performance across various benchmark evaluations, including mathematics, programming, and general logic. Its overall performance is now approaching that of leading models, such as O3 and Gemini 2.5 Pro. Models DeepSeek-R1-0528-Qwen3-8B ollama run deepseek-r1 DeepSeek-R1 ollama run deepseek-r1:671b Note: to update the model from an older version, run ollama pull deepseek-r1 Distilled models DeepSeek team has demonstrated that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. Below are the models created via fine-tuning against several dense models widely used in the research community using reasoning data generated by DeepSeek-R1. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. DeepSeek-R1-0528-Qwen3-8B ollama run deepseek-r1:8b DeepSeek-R1-Distill-Qwen-1.5B ollama run deepseek-r1:1.5b DeepSeek-R1-Distill-Qwen-7B ollama run deepseek-r1:7b DeepSeek-R1-Distill-Qwen-14B ollama run deepseek-r1:14b DeepSeek-R1-Distill-Qwen-32B ollama run deepseek-r1:32b DeepSeek-R1-Distill-Llama-70B ollama run deepseek-r1:70b License The model weights are licensed under the MIT License. DeepSeek-R1 series support commercial use, allow for any modifications and derivative works, including, but not limited to, distillation for training other LLMs. Please note that: The Qwen distilled models are derived from Qwen-2.5 series, which are originally licensed under Apache 2.0 License, and now finetuned with 800k samples curated with DeepSeek-R1. The Llama 8B distilled model is derived from Llama3.1-8B-Bas",
      "variants": [
        {
          "tag": "deepseek-r1:latest",
          "size_text": "5.2GB",
          "size_bytes": 5583457484,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "deepseek-r1:1.5b",
          "size_text": "1.1GB",
          "size_bytes": 1181116006,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "deepseek-r1:7b",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "deepseek-r1:8b",
          "size_text": "5.2GB",
          "size_bytes": 5583457484,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "deepseek-r1:14b",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "deepseek-r1:32b",
          "size_text": "20GB",
          "size_bytes": 21474836480,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "deepseek-r1:70b",
          "size_text": "43GB",
          "size_bytes": 46170898432,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "deepseek-r1:671b",
          "size_text": "404GB",
          "size_bytes": 433791696896,
          "context": "160K",
          "input": "Text"
        },
        {
          "tag": "deepseek-r1:1.5b-qwen-distill-q4_K_M",
          "size_text": "1.1GB",
          "size_bytes": 1181116006,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "deepseek-r1:1.5b-qwen-distill-q8_0",
          "size_text": "1.9GB",
          "size_bytes": 2040109465,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "deepseek-r1:1.5b-qwen-distill-fp16",
          "size_text": "3.6GB",
          "size_bytes": 3865470566,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "deepseek-r1:7b-qwen-distill-q4_K_M",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "deepseek-r1:7b-qwen-distill-q8_0",
          "size_text": "8.1GB",
          "size_bytes": 8697308774,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "deepseek-r1:7b-qwen-distill-fp16",
          "size_text": "15GB",
          "size_bytes": 16106127360,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "deepseek-r1:8b-0528-qwen3-q4_K_M",
          "size_text": "5.2GB",
          "size_bytes": 5583457484,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "deepseek-r1:8b-0528-qwen3-q8_0",
          "size_text": "8.9GB",
          "size_bytes": 9556302233,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "deepseek-r1:8b-0528-qwen3-fp16",
          "size_text": "16GB",
          "size_bytes": 17179869184,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "deepseek-r1:8b-llama-distill-q4_K_M",
          "size_text": "4.9GB",
          "size_bytes": 5261334937,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "deepseek-r1:8b-llama-distill-q8_0",
          "size_text": "8.5GB",
          "size_bytes": 9126805504,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "deepseek-r1:8b-llama-distill-fp16",
          "size_text": "16GB",
          "size_bytes": 17179869184,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "deepseek-r1:14b-qwen-distill-q4_K_M",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "deepseek-r1:14b-qwen-distill-q8_0",
          "size_text": "16GB",
          "size_bytes": 17179869184,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "deepseek-r1:14b-qwen-distill-fp16",
          "size_text": "30GB",
          "size_bytes": 32212254720,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "deepseek-r1:32b-qwen-distill-q4_K_M",
          "size_text": "20GB",
          "size_bytes": 21474836480,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "deepseek-r1:32b-qwen-distill-q8_0",
          "size_text": "35GB",
          "size_bytes": 37580963840,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "deepseek-r1:32b-qwen-distill-fp16",
          "size_text": "66GB",
          "size_bytes": 70866960384,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "deepseek-r1:70b-llama-distill-q4_K_M",
          "size_text": "43GB",
          "size_bytes": 46170898432,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "deepseek-r1:70b-llama-distill-q8_0",
          "size_text": "75GB",
          "size_bytes": 80530636800,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "deepseek-r1:70b-llama-distill-fp16",
          "size_text": "141GB",
          "size_bytes": 151397597184,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "deepseek-r1:671b-0528-q4_K_M",
          "size_text": "404GB",
          "size_bytes": 433791696896,
          "context": "160K",
          "input": "Text"
        },
        {
          "tag": "deepseek-r1:671b-0528-q8_0",
          "size_text": "713GB",
          "size_bytes": 765577920512,
          "context": "160K",
          "input": "Text"
        },
        {
          "tag": "deepseek-r1:671b-0528-fp16",
          "size_text": null,
          "size_bytes": null,
          "context": "160K",
          "input": "Text"
        },
        {
          "tag": "deepseek-r1:671b-q4_K_M",
          "size_text": "404GB",
          "size_bytes": 433791696896,
          "context": "160K",
          "input": "Text"
        },
        {
          "tag": "deepseek-r1:671b-q8_0",
          "size_text": "713GB",
          "size_bytes": 765577920512,
          "context": "160K",
          "input": "Text"
        },
        {
          "tag": "deepseek-r1:671b-fp16",
          "size_text": null,
          "size_bytes": null,
          "context": "160K",
          "input": "Text"
        }
      ],
      "tags_count": 35
    },
    {
      "slug": "nomic-embed-text",
      "capabilities": [
        "embedding"
      ],
      "pulls": 44600000,
      "pulls_text": "44.6M \n  Downloads",
      "blurb": "nomic-embed-text A high-performing open model with a large token context window. 3 Tags Updated 1 year ago",
      "name": "A high-performing open embedding model with a large token context window.",
      "description": "Readme Note: this model requires Ollama 0.1.26 or later. Download it here . It can only be used to generate embeddings. nomic-embed-text is a large context length text encoder that surpasses OpenAI text-embedding-ada-002 and text-embedding-3-small performance on short and long context tasks. Usage This model is an embedding model, meaning it can only be used to generate embeddings. REST API curl http://localhost:11434/api/embeddings -d '{ \"model\": \"nomic-embed-text\", \"prompt\": \"The sky is blue because of Rayleigh scattering\" }' Python library ollama.embeddings(model='nomic-embed-text', prompt='The sky is blue because of rayleigh scattering') Javascript library ollama.embeddings({ model: 'nomic-embed-text', prompt: 'The sky is blue because of rayleigh scattering' }) References HuggingFace Blog Post Write Preview ![nomic_logo](https://github.com/ollama/ollama/assets/251292/bf242e43-3c1a-4590-887d-abcab76cb304) > Note: this model requires Ollama 0.1.26 or later. [Download it here](https://ollama.com/download). It can only be used to generate embeddings. `nomic-embed-text` is a large context length text encoder that surpasses OpenAI `text-embedding-ada-002` and `text-embedding-3-small` performance on short and long context tasks. ## Usage This model is an embedding model, meaning it can only be used to generate embeddings. ### REST API ``` curl http://localhost:11434/api/embeddings -d '{ \"model\": \"nomic-embed-text\", \"prompt\": \"The sky is blue because of Rayleigh scattering\" }' ``` ### Python library ``` ollama.embeddings(model='nomic-embed-text', prompt='The sky is blue because of rayleigh scattering') ``` ### Javascript library ``` ollama.embeddings({ model: 'nomic-embed-text', prompt: 'The sky is blue because of rayleigh scattering' }) ``` ## References [HuggingFace](https://huggingface.co/nomic-ai/nomic-embed-text-v1.5) [Blog Post](https://blog.nomic.ai/posts/nomic-embed-text-v1) Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "nomic-embed-text:latest",
          "size_text": "274MB",
          "size_bytes": 287309824,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "nomic-embed-text:v1.5",
          "size_text": "274MB",
          "size_bytes": 287309824,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "nomic-embed-text:137m-v1.5-fp16",
          "size_text": "274MB",
          "size_bytes": 287309824,
          "context": "2K",
          "input": "Text"
        }
      ],
      "tags_count": 3
    },
    {
      "slug": "llama3.2",
      "capabilities": [
        "tools"
      ],
      "pulls": 43000000,
      "pulls_text": "43M \n  Downloads",
      "blurb": "llama3.2 Meta's Llama 3.2 goes small with 1B and 3B models. 1b 3b 63 Tags Updated 1 year ago",
      "name": "Meta's Llama 3.2 goes small with 1B and 3B models.",
      "description": "Readme The Meta Llama 3.2 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction-tuned generative models in 1B and 3B sizes (text in/text out). The Llama 3.2 instruction-tuned text only models are optimized for multilingual dialogue use cases, including agentic retrieval and summarization tasks. They outperform many of the available open source and closed chat models on common industry benchmarks. Sizes 3B parameters (default) The 3B model outperforms the Gemma 2 2.6B and Phi 3.5-mini models on tasks such as: Following instructions Summarization Prompt rewriting Tool use ollama run llama3.2 1B parameters The 1B model is competitive with other 1-3B parameter models. It’s use cases include: Personal information management Multilingual knowledge retrieval Rewriting tasks running locally on edge ollama run llama3.2:1b Benchmarks Supported Languages: English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai are officially supported. Llama 3.2 has been trained on a broader collection of languages than these 8 supported languages. Write Preview <img src=\"/assets/library/llama3.2/be01fadf-7fbd-404d-929b-50a77249b030\" width=\"280\" /> The Meta Llama 3.2 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction-tuned generative models in 1B and 3B sizes (text in/text out). The Llama 3.2 instruction-tuned text only models are optimized for multilingual dialogue use cases, including agentic retrieval and summarization tasks. They outperform many of the available open source and closed chat models on common industry benchmarks. ## Sizes ### 3B parameters (default) The 3B model outperforms the Gemma 2 2.6B and Phi 3.5-mini models on tasks such as: * Following instructions * Summarization * Prompt rewriting * Tool use ``` ollama run llama3.2 ``` ### 1B parameters The 1B model is competitive with other 1-3B parameter models. It's use cases include: * Personal information management",
      "variants": [
        {
          "tag": "llama3.2:latest",
          "size_text": "2.0GB",
          "size_bytes": 2147483648,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:1b",
          "size_text": "1.3GB",
          "size_bytes": 1395864371,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:3b",
          "size_text": "2.0GB",
          "size_bytes": 2147483648,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:1b-instruct-q2_K",
          "size_text": "581MB",
          "size_bytes": 609222656,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:1b-instruct-q3_K_S",
          "size_text": "642MB",
          "size_bytes": 673185792,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:1b-instruct-q3_K_M",
          "size_text": "691MB",
          "size_bytes": 724566016,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:1b-instruct-q3_K_L",
          "size_text": "733MB",
          "size_bytes": 768606208,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:1b-instruct-q4_0",
          "size_text": "771MB",
          "size_bytes": 808452096,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:1b-instruct-q4_1",
          "size_text": "832MB",
          "size_bytes": 872415232,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:1b-instruct-q4_K_S",
          "size_text": "776MB",
          "size_bytes": 813694976,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:1b-instruct-q4_K_M",
          "size_text": "808MB",
          "size_bytes": 847249408,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:1b-instruct-q5_0",
          "size_text": "893MB",
          "size_bytes": 936378368,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:1b-instruct-q5_1",
          "size_text": "953MB",
          "size_bytes": 999292928,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:1b-instruct-q5_K_S",
          "size_text": "893MB",
          "size_bytes": 936378368,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:1b-instruct-q5_K_M",
          "size_text": "912MB",
          "size_bytes": 956301312,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:1b-instruct-q6_K",
          "size_text": "1.0GB",
          "size_bytes": 1073741824,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:1b-instruct-q8_0",
          "size_text": "1.3GB",
          "size_bytes": 1395864371,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:1b-instruct-fp16",
          "size_text": "2.5GB",
          "size_bytes": 2684354560,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:1b-text-q2_K",
          "size_text": "581MB",
          "size_bytes": 609222656,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:1b-text-q3_K_S",
          "size_text": "642MB",
          "size_bytes": 673185792,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:1b-text-q3_K_M",
          "size_text": "691MB",
          "size_bytes": 724566016,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:1b-text-q3_K_L",
          "size_text": "733MB",
          "size_bytes": 768606208,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:1b-text-q4_0",
          "size_text": "771MB",
          "size_bytes": 808452096,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:1b-text-q4_1",
          "size_text": "832MB",
          "size_bytes": 872415232,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:1b-text-q4_K_S",
          "size_text": "776MB",
          "size_bytes": 813694976,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:1b-text-q4_K_M",
          "size_text": "808MB",
          "size_bytes": 847249408,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:1b-text-q5_0",
          "size_text": "893MB",
          "size_bytes": 936378368,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:1b-text-q5_1",
          "size_text": "953MB",
          "size_bytes": 999292928,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:1b-text-q5_K_S",
          "size_text": "893MB",
          "size_bytes": 936378368,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:1b-text-q5_K_M",
          "size_text": "912MB",
          "size_bytes": 956301312,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:1b-text-q6_K",
          "size_text": "1.0GB",
          "size_bytes": 1073741824,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:1b-text-q8_0",
          "size_text": "1.3GB",
          "size_bytes": 1395864371,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:1b-text-fp16",
          "size_text": "2.5GB",
          "size_bytes": 2684354560,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:3b-instruct-q2_K",
          "size_text": "1.4GB",
          "size_bytes": 1503238553,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:3b-instruct-q3_K_S",
          "size_text": "1.5GB",
          "size_bytes": 1610612736,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:3b-instruct-q3_K_M",
          "size_text": "1.7GB",
          "size_bytes": 1825361100,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:3b-instruct-q3_K_L",
          "size_text": "1.8GB",
          "size_bytes": 1932735283,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:3b-instruct-q4_0",
          "size_text": "1.9GB",
          "size_bytes": 2040109465,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:3b-instruct-q4_1",
          "size_text": "2.1GB",
          "size_bytes": 2254857830,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:3b-instruct-q4_K_S",
          "size_text": "1.9GB",
          "size_bytes": 2040109465,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:3b-instruct-q4_K_M",
          "size_text": "2.0GB",
          "size_bytes": 2147483648,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:3b-instruct-q5_0",
          "size_text": "2.3GB",
          "size_bytes": 2469606195,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:3b-instruct-q5_1",
          "size_text": "2.4GB",
          "size_bytes": 2576980377,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:3b-instruct-q5_K_S",
          "size_text": "2.3GB",
          "size_bytes": 2469606195,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:3b-instruct-q5_K_M",
          "size_text": "2.3GB",
          "size_bytes": 2469606195,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:3b-instruct-q6_K",
          "size_text": "2.6GB",
          "size_bytes": 2791728742,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:3b-instruct-q8_0",
          "size_text": "3.4GB",
          "size_bytes": 3650722201,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:3b-instruct-fp16",
          "size_text": "6.4GB",
          "size_bytes": 6871947673,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:3b-text-q2_K",
          "size_text": "1.4GB",
          "size_bytes": 1503238553,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:3b-text-q3_K_S",
          "size_text": "1.5GB",
          "size_bytes": 1610612736,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:3b-text-q3_K_M",
          "size_text": "1.7GB",
          "size_bytes": 1825361100,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:3b-text-q3_K_L",
          "size_text": "1.8GB",
          "size_bytes": 1932735283,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:3b-text-q4_0",
          "size_text": "1.9GB",
          "size_bytes": 2040109465,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:3b-text-q4_1",
          "size_text": "2.1GB",
          "size_bytes": 2254857830,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:3b-text-q4_K_S",
          "size_text": "1.9GB",
          "size_bytes": 2040109465,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:3b-text-q4_K_M",
          "size_text": "2.0GB",
          "size_bytes": 2147483648,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:3b-text-q5_0",
          "size_text": "2.3GB",
          "size_bytes": 2469606195,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:3b-text-q5_1",
          "size_text": "2.4GB",
          "size_bytes": 2576980377,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:3b-text-q5_K_S",
          "size_text": "2.3GB",
          "size_bytes": 2469606195,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:3b-text-q5_K_M",
          "size_text": "2.3GB",
          "size_bytes": 2469606195,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:3b-text-q6_K",
          "size_text": "2.6GB",
          "size_bytes": 2791728742,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:3b-text-q8_0",
          "size_text": "3.4GB",
          "size_bytes": 3650722201,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:3b-text-fp16",
          "size_text": "6.4GB",
          "size_bytes": 6871947673,
          "context": "128K",
          "input": "Text"
        }
      ],
      "tags_count": 63
    },
    {
      "slug": "gemma3",
      "capabilities": [
        "multimodal",
        "reasoning",
        "vision"
      ],
      "pulls": 23500000,
      "pulls_text": "23.5M \n  Downloads",
      "blurb": "gemma3 The current, most capable model that runs on a single GPU. 270m 1b 4b 12b 27b 26 Tags Updated 2 months ago",
      "name": "The current, most capable model that runs on a single GPU.",
      "description": "Readme This model requires Ollama 0.6 or later. Download Ollama Gemma is a lightweight, family of models from Google built on Gemini technology. The Gemma 3 models are multimodal—processing text and images—and feature a 128K context window with support for over 140 languages. Available in 270M, 1B, 4B, 12B, and 27B parameter sizes, they excel in tasks like question answering, summarization, and reasoning, while their compact design allows deployment on resource-limited devices. Models Text 270M parameter model (32k context window) ollama run gemma3:270m 1B parameter model (32k context window) ollama run gemma3:1b Multimodal (Vision) 4B parameter model (128k context window) ollama run gemma3:4b 12B parameter model (128k context window) ollama run gemma3:12b 27B parameter model (128k context window) ollama run gemma3:27b Quantization aware trained models (QAT) The quantization aware trained Gemma 3 models preserves similar quality as half precision models (BF16) while maintaining a lower memory footprint (3x less compared to non-quantized models). 1B parameter model ollama run gemma3:1b-it-qat 4B parameter model ollama run gemma3:4b-it-qat 12B parameter model ollama run gemma3:12b-it-qat 27B parameter model ollama run gemma3:27b-it-qat Evaluation Benchmark Results Gemma 3 270M Benchmark n-shot Gemma 3 270m instruction tuned HellaSwag 0-shot 37.7 PIQA 0-shot 66.2 ARC-c 0-shot 28.2 WinoGrande 0-shot 52.3 BIG-Bench Hard few-shot 26.7 IF Eval 0-shot 51.2 These models were evaluated against a large collection of different datasets and metrics to cover different aspects of text generation: Reasoning, logic and code capabilities Benchmark Metric Gemma 3 PT 1B Gemma 3 PT 4B Gemma 3 PT 12B Gemma 3 PT 27B HellaSwag 10-shot 62.3 77.2 84.2 85.6 BoolQ 0-shot 63.2 72.3 78.8 82.4 PIQA 0-shot 73.8 79.6 81.8 83.3 SocialIQA 0-shot 48.9 51.9 53.4 54.9 TriviaQA 5-shot 39.8 65.8 78.2 85.5 Natural Questions 5-shot 9.48 20.0 31.4 36.1 ARC-c 25-shot 38.4 56.2 68.9 70.6 ARC-e 0-shot 73.0 82.4",
      "variants": [
        {
          "tag": "gemma3:latest",
          "size_text": "3.3GB",
          "size_bytes": 3543348019,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "gemma3:270m",
          "size_text": "292MB",
          "size_bytes": 306184192,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "gemma3:1b",
          "size_text": "815MB",
          "size_bytes": 854589440,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "gemma3:4b",
          "size_text": "3.3GB",
          "size_bytes": 3543348019,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "gemma3:12b",
          "size_text": "8.1GB",
          "size_bytes": 8697308774,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "gemma3:27b",
          "size_text": "17GB",
          "size_bytes": 18253611008,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "gemma3:270m-it-qat",
          "size_text": "241MB",
          "size_bytes": 252706816,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "gemma3:270m-it-q8_0",
          "size_text": "292MB",
          "size_bytes": 306184192,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "gemma3:270m-it-fp16",
          "size_text": "543MB",
          "size_bytes": 569376768,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "gemma3:270m-it-bf16",
          "size_text": "543MB",
          "size_bytes": 569376768,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "gemma3:1b-it-qat",
          "size_text": "1.0GB",
          "size_bytes": 1073741824,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "gemma3:1b-it-q4_K_M",
          "size_text": "815MB",
          "size_bytes": 854589440,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "gemma3:1b-it-q8_0",
          "size_text": "1.1GB",
          "size_bytes": 1181116006,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "gemma3:1b-it-fp16",
          "size_text": "2.0GB",
          "size_bytes": 2147483648,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "gemma3:4b-it-qat",
          "size_text": "4.0GB",
          "size_bytes": 4294967296,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "gemma3:4b-it-q4_K_M",
          "size_text": "3.3GB",
          "size_bytes": 3543348019,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "gemma3:4b-it-q8_0",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "gemma3:4b-it-fp16",
          "size_text": "8.6GB",
          "size_bytes": 9234179686,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "gemma3:12b-it-qat",
          "size_text": "8.9GB",
          "size_bytes": 9556302233,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "gemma3:12b-it-q4_K_M",
          "size_text": "8.1GB",
          "size_bytes": 8697308774,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "gemma3:12b-it-q8_0",
          "size_text": "13GB",
          "size_bytes": 13958643712,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "gemma3:12b-it-fp16",
          "size_text": "24GB",
          "size_bytes": 25769803776,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "gemma3:27b-it-qat",
          "size_text": "18GB",
          "size_bytes": 19327352832,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "gemma3:27b-it-q4_K_M",
          "size_text": "17GB",
          "size_bytes": 18253611008,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "gemma3:27b-it-q8_0",
          "size_text": "30GB",
          "size_bytes": 32212254720,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "gemma3:27b-it-fp16",
          "size_text": "55GB",
          "size_bytes": 59055800320,
          "context": "128K",
          "input": "Text"
        }
      ],
      "tags_count": 26
    },
    {
      "slug": "mistral",
      "capabilities": [
        "tools"
      ],
      "pulls": 21400000,
      "pulls_text": "21.4M \n  Downloads",
      "blurb": "mistral The 7B model released by Mistral AI, updated to version 0.3. 7b 84 Tags Updated 3 months ago",
      "name": "The 7B model released by Mistral AI, updated to version 0.3.",
      "description": "Readme Mistral is a 7B parameter model, distributed with the Apache license. It is available in both instruct (instruction following) and text completion. The Mistral AI team has noted that Mistral 7B: Outperforms Llama 2 13B on all benchmarks Outperforms Llama 1 34B on many benchmarks Approaches CodeLlama 7B performance on code, while remaining good at English tasks Versions Tag Date Notes v0.3 latest 05/22/2024 A new version of Mistral 7B that supports function calling. v0.2 03/23/2024 A minor release of Mistral 7B v0.1 09/27/2023 Initial release Function calling Mistral 0.3 supports function calling with Ollama’s raw mode . Example raw prompt [AVAILABLE_TOOLS] [{\"type\": \"function\", \"function\": {\"name\": \"get_current_weather\", \"description\": \"Get the current weather\", \"parameters\": {\"type\": \"object\", \"properties\": {\"location\": {\"type\": \"string\", \"description\": \"The city and state, e.g. San Francisco, CA\"}, \"format\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"], \"description\": \"The temperature unit to use. Infer this from the users location.\"}}, \"required\": [\"location\", \"format\"]}}}][/AVAILABLE_TOOLS][INST] What is the weather like today in San Francisco [/INST] Example response [TOOL_CALLS] [{\"name\": \"get_current_weather\", \"arguments\": {\"location\": \"San Francisco, CA\", \"format\": \"celsius\"}}] For more information on raw mode, see the API documentation . Variations instruct Instruct models follow instructions text Text models are the base foundation model without any fine-tuning for conversations, and are best used for simple text completion. Usage CLI Instruct: ollama run mistral API Example: curl -X POST http://localhost:11434/api/generate -d '{ \"model\": \"mistral\", \"prompt\":\"Here is a story about llamas eating grass\" }' References HuggingFace Mistral AI News Release Write Preview <img src=\"https://github.com/jmorganca/ollama/assets/3325447/d6be0694-eb35-417b-8f08-47d3b6c2a171\" width=\"200\"/> Mistral is a 7B parameter model, distributed with the Apache licens",
      "variants": [
        {
          "tag": "mistral:latest",
          "size_text": "4.4GB",
          "size_bytes": 4724464025,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:instruct",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:text",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "mistral:v0.1",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:v0.2",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:v0.3",
          "size_text": "4.4GB",
          "size_bytes": 4724464025,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b",
          "size_text": "4.4GB",
          "size_bytes": 4724464025,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct",
          "size_text": "4.4GB",
          "size_bytes": 4724464025,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct-q2_K",
          "size_text": "3.1GB",
          "size_bytes": 3328599654,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct-v0.2-q2_K",
          "size_text": "3.1GB",
          "size_bytes": 3328599654,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct-q3_K_S",
          "size_text": "3.2GB",
          "size_bytes": 3435973836,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct-v0.2-q3_K_S",
          "size_text": "3.2GB",
          "size_bytes": 3435973836,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct-q3_K_M",
          "size_text": "3.5GB",
          "size_bytes": 3758096384,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct-v0.2-q3_K_M",
          "size_text": "3.5GB",
          "size_bytes": 3758096384,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct-q3_K_L",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct-v0.2-q3_K_L",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct-q4_0",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct-v0.2-q4_0",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct-q4_1",
          "size_text": "4.6GB",
          "size_bytes": 4939212390,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct-v0.2-q4_1",
          "size_text": "4.6GB",
          "size_bytes": 4939212390,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct-q4_K_S",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct-v0.2-q4_K_S",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct-v0.2-q4_K_M",
          "size_text": "4.4GB",
          "size_bytes": 4724464025,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct-v0.2-q5_0",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct-v0.2-q5_1",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct-v0.2-q5_K_S",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct-v0.2-q5_K_M",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct-v0.2-q6_K",
          "size_text": "5.9GB",
          "size_bytes": 6335076761,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct-v0.2-q8_0",
          "size_text": "7.7GB",
          "size_bytes": 8267812044,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct-v0.2-fp16",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct-v0.3-q2_K",
          "size_text": "2.7GB",
          "size_bytes": 2899102924,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct-v0.3-q3_K_S",
          "size_text": "3.2GB",
          "size_bytes": 3435973836,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct-v0.3-q3_K_M",
          "size_text": "3.5GB",
          "size_bytes": 3758096384,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct-v0.3-q3_K_L",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct-v0.3-q4_0",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct-v0.3-q4_1",
          "size_text": "4.6GB",
          "size_bytes": 4939212390,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct-v0.3-q4_K_S",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct-q4_K_M",
          "size_text": "4.4GB",
          "size_bytes": 4724464025,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct-v0.3-q4_K_M",
          "size_text": "4.4GB",
          "size_bytes": 4724464025,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct-q5_0",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct-v0.3-q5_0",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct-q5_1",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct-v0.3-q5_1",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct-q5_K_S",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct-v0.3-q5_K_S",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct-q5_K_M",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct-v0.3-q5_K_M",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct-q6_K",
          "size_text": "5.9GB",
          "size_bytes": 6335076761,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct-v0.3-q6_K",
          "size_text": "5.9GB",
          "size_bytes": 6335076761,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct-q8_0",
          "size_text": "7.7GB",
          "size_bytes": 8267812044,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct-v0.3-q8_0",
          "size_text": "7.7GB",
          "size_bytes": 8267812044,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct-fp16",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct-v0.3-fp16",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-text",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-text-q2_K",
          "size_text": "3.1GB",
          "size_bytes": 3328599654,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-text-v0.2-q2_K",
          "size_text": "2.7GB",
          "size_bytes": 2899102924,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-text-q3_K_S",
          "size_text": "3.2GB",
          "size_bytes": 3435973836,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-text-v0.2-q3_K_S",
          "size_text": "3.2GB",
          "size_bytes": 3435973836,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-text-q3_K_M",
          "size_text": "3.5GB",
          "size_bytes": 3758096384,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-text-v0.2-q3_K_M",
          "size_text": "3.5GB",
          "size_bytes": 3758096384,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-text-q3_K_L",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-text-v0.2-q3_K_L",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-text-q4_0",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-text-v0.2-q4_0",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-text-q4_1",
          "size_text": "4.6GB",
          "size_bytes": 4939212390,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-text-v0.2-q4_1",
          "size_text": "4.6GB",
          "size_bytes": 4939212390,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-text-q4_K_S",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-text-v0.2-q4_K_S",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-text-q4_K_M",
          "size_text": "4.4GB",
          "size_bytes": 4724464025,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-text-v0.2-q4_K_M",
          "size_text": "4.4GB",
          "size_bytes": 4724464025,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-text-q5_0",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-text-v0.2-q5_0",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-text-q5_1",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-text-v0.2-q5_1",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-text-q5_K_S",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-text-v0.2-q5_K_S",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-text-q5_K_M",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-text-v0.2-q5_K_M",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-text-q6_K",
          "size_text": "5.9GB",
          "size_bytes": 6335076761,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-text-v0.2-q6_K",
          "size_text": "5.9GB",
          "size_bytes": 6335076761,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-text-q8_0",
          "size_text": "7.7GB",
          "size_bytes": 8267812044,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-text-v0.2-q8_0",
          "size_text": "7.7GB",
          "size_bytes": 8267812044,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-text-fp16",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-text-v0.2-fp16",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "16K",
          "input": "Text"
        }
      ],
      "tags_count": 84
    },
    {
      "slug": "qwen2.5",
      "capabilities": [
        "tools"
      ],
      "pulls": 15900000,
      "pulls_text": "15.9M \n  Downloads",
      "blurb": "qwen2.5 Qwen2.5 models are pretrained on Alibaba's latest large-scale dataset, encompassing up to 18 trillion tokens. The model supports up to 128K tokens and has multilingual support. 0.5b 1.5b 3b 7b 14b 32b 72b 133 Tags Updated 1 year ago",
      "name": "Qwen2.5 models are pretrained on Alibaba's latest large-scale dataset, encompassing up to 18 trillion tokens. The model supports up to 128K tokens and has multilingual support.",
      "description": "Readme Qwen2.5 is the latest series of Qwen large language models. For Qwen2.5, a range of base language models and instruction-tuned models are released, with sizes ranging from 0.5 to 72 billion parameters. Qwen2.5 introduces the following improvements over Qwen2: It possesses significantly more knowledge and has greatly enhanced capabilities in coding and mathematics , due to specialized expert models in these domains. It demonstrates significant advancements in instruction following , long-text generation (over 8K tokens), understanding structured data (e.g., tables), and generating structured outputs , especially in JSON format. It is also more resilient to diverse system prompts , improving role-play and condition-setting for chatbots. It supports long contexts of up to 128K tokens and can generate up to 8K tokens. It offers multilingual support for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more. Please note: all models except the 3B and 72B are released under the Apache 2.0 license, while the 3B and 72B models are under the Qwen license. References GitHub Blog post HuggingFace Write Preview <img src=\"https://ollama.com/assets/library/qwen2.5/4b4f719f-c327-489e-8dc1-89a455c21e89\" width=\"320\" /> Qwen2.5 is the latest series of Qwen large language models. For Qwen2.5, a range of base language models and instruction-tuned models are released, with sizes ranging from 0.5 to 72 billion parameters. Qwen2.5 introduces the following improvements over Qwen2: - It possesses **significantly more knowledge** and has greatly enhanced capabilities in **coding** and **mathematics**, due to specialized expert models in these domains. - It demonstrates significant advancements in **instruction following**, **long-text generation** (over 8K tokens), **understanding structured data** (e.g., tables), and **generating structured outputs**, especially in JSON format. It is al",
      "variants": [
        {
          "tag": "qwen2.5:latest",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:0.5b",
          "size_text": "398MB",
          "size_bytes": 417333248,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:1.5b",
          "size_text": "986MB",
          "size_bytes": 1033895936,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:3b",
          "size_text": "1.9GB",
          "size_bytes": 2040109465,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:7b",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:14b",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:32b",
          "size_text": "20GB",
          "size_bytes": 21474836480,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:72b",
          "size_text": "47GB",
          "size_bytes": 50465865728,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:0.5b-base",
          "size_text": "398MB",
          "size_bytes": 417333248,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:0.5b-base-q2_K",
          "size_text": "339MB",
          "size_bytes": 355467264,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:0.5b-base-q3_K_S",
          "size_text": "338MB",
          "size_bytes": 354418688,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:0.5b-base-q3_K_M",
          "size_text": "355MB",
          "size_bytes": 372244480,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:0.5b-base-q3_K_L",
          "size_text": "369MB",
          "size_bytes": 386924544,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:0.5b-base-q4_0",
          "size_text": "352MB",
          "size_bytes": 369098752,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:0.5b-base-q4_1",
          "size_text": "375MB",
          "size_bytes": 393216000,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:0.5b-base-q4_K_S",
          "size_text": "385MB",
          "size_bytes": 403701760,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:0.5b-base-q4_K_M",
          "size_text": "398MB",
          "size_bytes": 417333248,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:0.5b-base-q5_0",
          "size_text": "397MB",
          "size_bytes": 416284672,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:0.5b-base-q5_1",
          "size_text": "419MB",
          "size_bytes": 439353344,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:0.5b-base-q5_K_S",
          "size_text": "413MB",
          "size_bytes": 433061888,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:0.5b-base-q8_0",
          "size_text": "531MB",
          "size_bytes": 556793856,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:0.5b-instruct",
          "size_text": "398MB",
          "size_bytes": 417333248,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:0.5b-instruct-q2_K",
          "size_text": "339MB",
          "size_bytes": 355467264,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:0.5b-instruct-q3_K_S",
          "size_text": "338MB",
          "size_bytes": 354418688,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:0.5b-instruct-q3_K_M",
          "size_text": "355MB",
          "size_bytes": 372244480,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:0.5b-instruct-q3_K_L",
          "size_text": "369MB",
          "size_bytes": 386924544,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:0.5b-instruct-q4_0",
          "size_text": "352MB",
          "size_bytes": 369098752,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:0.5b-instruct-q4_1",
          "size_text": "375MB",
          "size_bytes": 393216000,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:0.5b-instruct-q4_K_S",
          "size_text": "385MB",
          "size_bytes": 403701760,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:0.5b-instruct-q4_K_M",
          "size_text": "398MB",
          "size_bytes": 417333248,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:0.5b-instruct-q5_0",
          "size_text": "397MB",
          "size_bytes": 416284672,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:0.5b-instruct-q5_1",
          "size_text": "419MB",
          "size_bytes": 439353344,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:0.5b-instruct-q5_K_S",
          "size_text": "413MB",
          "size_bytes": 433061888,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:0.5b-instruct-q5_K_M",
          "size_text": "420MB",
          "size_bytes": 440401920,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:0.5b-instruct-q6_K",
          "size_text": "506MB",
          "size_bytes": 530579456,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:0.5b-instruct-q8_0",
          "size_text": "531MB",
          "size_bytes": 556793856,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:0.5b-instruct-fp16",
          "size_text": "994MB",
          "size_bytes": 1042284544,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:1.5b-instruct",
          "size_text": "986MB",
          "size_bytes": 1033895936,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:1.5b-instruct-q2_K",
          "size_text": "676MB",
          "size_bytes": 708837376,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:1.5b-instruct-q3_K_S",
          "size_text": "761MB",
          "size_bytes": 797966336,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:1.5b-instruct-q3_K_M",
          "size_text": "824MB",
          "size_bytes": 864026624,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:1.5b-instruct-q3_K_L",
          "size_text": "880MB",
          "size_bytes": 922746880,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:1.5b-instruct-q4_0",
          "size_text": "935MB",
          "size_bytes": 980418560,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:1.5b-instruct-q4_1",
          "size_text": "1.0GB",
          "size_bytes": 1073741824,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:1.5b-instruct-q4_K_S",
          "size_text": "940MB",
          "size_bytes": 985661440,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:1.5b-instruct-q4_K_M",
          "size_text": "986MB",
          "size_bytes": 1033895936,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:1.5b-instruct-q5_0",
          "size_text": "1.1GB",
          "size_bytes": 1181116006,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:1.5b-instruct-q5_1",
          "size_text": "1.2GB",
          "size_bytes": 1288490188,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:1.5b-instruct-q5_K_S",
          "size_text": "1.1GB",
          "size_bytes": 1181116006,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:1.5b-instruct-q5_K_M",
          "size_text": "1.1GB",
          "size_bytes": 1181116006,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:1.5b-instruct-q6_K",
          "size_text": "1.3GB",
          "size_bytes": 1395864371,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:1.5b-instruct-q8_0",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:1.5b-instruct-fp16",
          "size_text": "3.1GB",
          "size_bytes": 3328599654,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:3b-instruct",
          "size_text": "1.9GB",
          "size_bytes": 2040109465,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:3b-instruct-q2_K",
          "size_text": "1.3GB",
          "size_bytes": 1395864371,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:3b-instruct-q3_K_S",
          "size_text": "1.5GB",
          "size_bytes": 1610612736,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:3b-instruct-q3_K_M",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:3b-instruct-q3_K_L",
          "size_text": "1.7GB",
          "size_bytes": 1825361100,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:3b-instruct-q4_0",
          "size_text": "1.8GB",
          "size_bytes": 1932735283,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:3b-instruct-q4_1",
          "size_text": "2.0GB",
          "size_bytes": 2147483648,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:3b-instruct-q4_K_S",
          "size_text": "1.8GB",
          "size_bytes": 1932735283,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:3b-instruct-q4_K_M",
          "size_text": "1.9GB",
          "size_bytes": 2040109465,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:3b-instruct-q5_0",
          "size_text": "2.2GB",
          "size_bytes": 2362232012,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:3b-instruct-q5_1",
          "size_text": "2.3GB",
          "size_bytes": 2469606195,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:3b-instruct-q5_K_S",
          "size_text": "2.2GB",
          "size_bytes": 2362232012,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:3b-instruct-q5_K_M",
          "size_text": "2.2GB",
          "size_bytes": 2362232012,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:3b-instruct-q6_K",
          "size_text": "2.5GB",
          "size_bytes": 2684354560,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:3b-instruct-q8_0",
          "size_text": "3.3GB",
          "size_bytes": 3543348019,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:3b-instruct-fp16",
          "size_text": "6.2GB",
          "size_bytes": 6657199308,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:7b-instruct",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:7b-instruct-q2_K",
          "size_text": "3.0GB",
          "size_bytes": 3221225472,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:7b-instruct-q3_K_S",
          "size_text": "3.5GB",
          "size_bytes": 3758096384,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:7b-instruct-q3_K_M",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:7b-instruct-q3_K_L",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:7b-instruct-q4_0",
          "size_text": "4.4GB",
          "size_bytes": 4724464025,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:7b-instruct-q4_1",
          "size_text": "4.9GB",
          "size_bytes": 5261334937,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:7b-instruct-q4_K_S",
          "size_text": "4.5GB",
          "size_bytes": 4831838208,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:7b-instruct-q4_K_M",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:7b-instruct-q5_0",
          "size_text": "5.3GB",
          "size_bytes": 5690831667,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:7b-instruct-q5_1",
          "size_text": "5.8GB",
          "size_bytes": 6227702579,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:7b-instruct-q5_K_S",
          "size_text": "5.3GB",
          "size_bytes": 5690831667,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:7b-instruct-q5_K_M",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:7b-instruct-q6_K",
          "size_text": "6.3GB",
          "size_bytes": 6764573491,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:7b-instruct-q8_0",
          "size_text": "8.1GB",
          "size_bytes": 8697308774,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:7b-instruct-fp16",
          "size_text": "15GB",
          "size_bytes": 16106127360,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:14b-instruct",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:14b-instruct-q2_K",
          "size_text": "5.8GB",
          "size_bytes": 6227702579,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:14b-instruct-q3_K_S",
          "size_text": "6.7GB",
          "size_bytes": 7194070220,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:14b-instruct-q3_K_M",
          "size_text": "7.3GB",
          "size_bytes": 7838315315,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:14b-instruct-q3_K_L",
          "size_text": "7.9GB",
          "size_bytes": 8482560409,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:14b-instruct-q4_0",
          "size_text": "8.5GB",
          "size_bytes": 9126805504,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:14b-instruct-q4_1",
          "size_text": "9.4GB",
          "size_bytes": 10093173145,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:14b-instruct-q4_K_S",
          "size_text": "8.6GB",
          "size_bytes": 9234179686,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:14b-instruct-q4_K_M",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:14b-instruct-q5_0",
          "size_text": "10GB",
          "size_bytes": 10737418240,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:14b-instruct-q5_1",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:14b-instruct-q5_K_S",
          "size_text": "10GB",
          "size_bytes": 10737418240,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:14b-instruct-q5_K_M",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:14b-instruct-q6_K",
          "size_text": "12GB",
          "size_bytes": 12884901888,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:14b-instruct-q8_0",
          "size_text": "16GB",
          "size_bytes": 17179869184,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:14b-instruct-fp16",
          "size_text": "30GB",
          "size_bytes": 32212254720,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:32b-instruct",
          "size_text": "20GB",
          "size_bytes": 21474836480,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:32b-instruct-q2_K",
          "size_text": "12GB",
          "size_bytes": 12884901888,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:32b-instruct-q3_K_S",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:32b-instruct-q3_K_M",
          "size_text": "16GB",
          "size_bytes": 17179869184,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:32b-instruct-q3_K_L",
          "size_text": "17GB",
          "size_bytes": 18253611008,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:32b-instruct-q4_0",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:32b-instruct-q4_1",
          "size_text": "21GB",
          "size_bytes": 22548578304,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:32b-instruct-q4_K_S",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:32b-instruct-q4_K_M",
          "size_text": "20GB",
          "size_bytes": 21474836480,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:32b-instruct-q5_0",
          "size_text": "23GB",
          "size_bytes": 24696061952,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:32b-instruct-q5_1",
          "size_text": "25GB",
          "size_bytes": 26843545600,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:32b-instruct-q5_K_S",
          "size_text": "23GB",
          "size_bytes": 24696061952,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:32b-instruct-q5_K_M",
          "size_text": "23GB",
          "size_bytes": 24696061952,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:32b-instruct-q6_K",
          "size_text": "27GB",
          "size_bytes": 28991029248,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:32b-instruct-q8_0",
          "size_text": "35GB",
          "size_bytes": 37580963840,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:32b-instruct-fp16",
          "size_text": "66GB",
          "size_bytes": 70866960384,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:72b-instruct",
          "size_text": "47GB",
          "size_bytes": 50465865728,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:72b-instruct-q2_K",
          "size_text": "30GB",
          "size_bytes": 32212254720,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:72b-instruct-q3_K_S",
          "size_text": "34GB",
          "size_bytes": 36507222016,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:72b-instruct-q3_K_M",
          "size_text": "38GB",
          "size_bytes": 40802189312,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:72b-instruct-q3_K_L",
          "size_text": "40GB",
          "size_bytes": 42949672960,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:72b-instruct-q4_0",
          "size_text": "41GB",
          "size_bytes": 44023414784,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:72b-instruct-q4_1",
          "size_text": "46GB",
          "size_bytes": 49392123904,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:72b-instruct-q4_K_S",
          "size_text": "44GB",
          "size_bytes": 47244640256,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:72b-instruct-q4_K_M",
          "size_text": "47GB",
          "size_bytes": 50465865728,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:72b-instruct-q5_0",
          "size_text": "50GB",
          "size_bytes": 53687091200,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:72b-instruct-q5_1",
          "size_text": "55GB",
          "size_bytes": 59055800320,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:72b-instruct-q5_K_S",
          "size_text": "51GB",
          "size_bytes": 54760833024,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:72b-instruct-q5_K_M",
          "size_text": "54GB",
          "size_bytes": 57982058496,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:72b-instruct-q6_K",
          "size_text": "64GB",
          "size_bytes": 68719476736,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:72b-instruct-q8_0",
          "size_text": "77GB",
          "size_bytes": 82678120448,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:72b-instruct-fp16",
          "size_text": "145GB",
          "size_bytes": 155692564480,
          "context": "32K",
          "input": "Text"
        }
      ],
      "tags_count": 133
    },
    {
      "slug": "phi3",
      "capabilities": [
        "multimodal",
        "reasoning"
      ],
      "pulls": 12700000,
      "pulls_text": "12.7M \n  Downloads",
      "blurb": "phi3 Phi-3 is a family of lightweight 3B (Mini) and 14B (Medium) state-of-the-art open models by Microsoft. 3.8b 14b 72 Tags Updated 1 year ago",
      "name": "Phi-3 is a family of lightweight 3B (Mini) and 14B (Medium) state-of-the-art open models by Microsoft.",
      "description": "Readme Phi-3 is a family of open AI models developed by Microsoft. Parameter sizes Phi-3 Mini – 3B parameters – ollama run phi3:mini Phi-3 Medium – 14B parameters – ollama run phi3:medium Context window sizes Note: the 128k version of this model requires Ollama 0.1.39 or later. 4k ollama run phi3:mini ollama run phi3:medium 128k ollama run phi3:medium-128k Phi-3 Mini Phi-3 Mini is a 3.8B parameters, lightweight, state-of-the-art open model trained with the Phi-3 datasets that includes both synthetic data and the filtered publicly available websites data with a focus on high-quality and reasoning dense properties. The model has underwent a post-training process that incorporates both supervised fine-tuning and direct preference optimization to ensure precise instruction adherence and robust safety measures. When assessed against benchmarks testing common sense, language understanding, math, code, long context and logical reasoning, Phi-3 Mini-4K-Instruct showcased a robust and state-of-the-art performance among models with less than 13 billion parameters. Phi-3 Medium Phi-3 Medium is a 14B parameter language model, and outperforms Gemini 1.0 Pro. Intended Uses Primary use cases The model is intended for commercial and research use in English. The model provides uses for applications which require 1) memory/compute constrained environments 2) latency bound scenarios 3) strong reasoning (especially math and logic) 4) long context Our model is designed to accelerate research on language and multimodal models, for use as a building block for generative AI powered features. Use case considerations Our models are not specifically designed or evaluated for all downstream purposes. Developers should consider common limitations of language models as they select use cases, and evaluate and mitigate for accuracy, safety, and fariness before using within a specific downstream use case, particularly for high risk scenarios. Developers should be aware of and adhere to applicable l",
      "variants": [
        {
          "tag": "phi3:latest",
          "size_text": "2.2GB",
          "size_bytes": 2362232012,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "phi3:instruct",
          "size_text": "2.2GB",
          "size_bytes": 2362232012,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "phi3:medium",
          "size_text": "7.9GB",
          "size_bytes": 8482560409,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "phi3:mini",
          "size_text": "2.2GB",
          "size_bytes": 2362232012,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "phi3:3.8b",
          "size_text": "2.2GB",
          "size_bytes": 2362232012,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "phi3:14b",
          "size_text": "7.9GB",
          "size_bytes": 8482560409,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "phi3:3.8b-instruct",
          "size_text": "2.2GB",
          "size_bytes": 2362232012,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "phi3:3.8b-mini-128k-instruct-q2_K",
          "size_text": "1.4GB",
          "size_bytes": 1503238553,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:3.8b-mini-128k-instruct-q3_K_S",
          "size_text": "1.7GB",
          "size_bytes": 1825361100,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:3.8b-mini-128k-instruct-q3_K_M",
          "size_text": "2.0GB",
          "size_bytes": 2147483648,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:3.8b-mini-128k-instruct-q3_K_L",
          "size_text": "2.1GB",
          "size_bytes": 2254857830,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:3.8b-mini-128k-instruct-q4_0",
          "size_text": "2.2GB",
          "size_bytes": 2362232012,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "phi3:3.8b-mini-128k-instruct-q4_1",
          "size_text": "2.4GB",
          "size_bytes": 2576980377,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:3.8b-mini-128k-instruct-q4_K_S",
          "size_text": "2.2GB",
          "size_bytes": 2362232012,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:3.8b-mini-128k-instruct-q4_K_M",
          "size_text": "2.4GB",
          "size_bytes": 2576980377,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:3.8b-mini-128k-instruct-q5_0",
          "size_text": "2.6GB",
          "size_bytes": 2791728742,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:3.8b-mini-128k-instruct-q5_1",
          "size_text": "2.9GB",
          "size_bytes": 3113851289,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:3.8b-mini-128k-instruct-q5_K_S",
          "size_text": "2.6GB",
          "size_bytes": 2791728742,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:3.8b-mini-128k-instruct-q5_K_M",
          "size_text": "2.8GB",
          "size_bytes": 3006477107,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:3.8b-mini-128k-instruct-q6_K",
          "size_text": "3.1GB",
          "size_bytes": 3328599654,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:3.8b-mini-128k-instruct-q8_0",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:3.8b-mini-128k-instruct-fp16",
          "size_text": "7.6GB",
          "size_bytes": 8160437862,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:3.8b-mini-4k-instruct-q2_K",
          "size_text": "1.4GB",
          "size_bytes": 1503238553,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:3.8b-mini-4k-instruct-q3_K_S",
          "size_text": "1.7GB",
          "size_bytes": 1825361100,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:3.8b-mini-4k-instruct-q3_K_M",
          "size_text": "2.0GB",
          "size_bytes": 2147483648,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:3.8b-mini-4k-instruct-q3_K_L",
          "size_text": "2.1GB",
          "size_bytes": 2254857830,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:3.8b-mini-4k-instruct-q4_0",
          "size_text": "2.2GB",
          "size_bytes": 2362232012,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:3.8b-mini-4k-instruct-q4_1",
          "size_text": "2.4GB",
          "size_bytes": 2576980377,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:3.8b-mini-4k-instruct-q4_K_S",
          "size_text": "2.2GB",
          "size_bytes": 2362232012,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:3.8b-mini-4k-instruct-q4_K_M",
          "size_text": "2.4GB",
          "size_bytes": 2576980377,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:3.8b-mini-4k-instruct-q5_0",
          "size_text": "2.6GB",
          "size_bytes": 2791728742,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:3.8b-mini-4k-instruct-q5_1",
          "size_text": "2.9GB",
          "size_bytes": 3113851289,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:3.8b-mini-4k-instruct-q5_K_S",
          "size_text": "2.6GB",
          "size_bytes": 2791728742,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:3.8b-mini-4k-instruct-q5_K_M",
          "size_text": "2.8GB",
          "size_bytes": 3006477107,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:3.8b-mini-4k-instruct-q6_K",
          "size_text": "3.1GB",
          "size_bytes": 3328599654,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:3.8b-mini-4k-instruct-q8_0",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:3.8b-mini-4k-instruct-fp16",
          "size_text": "7.6GB",
          "size_bytes": 8160437862,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:14b-instruct",
          "size_text": "7.9GB",
          "size_bytes": 8482560409,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "phi3:14b-medium-128k-instruct-q2_K",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:14b-medium-128k-instruct-q3_K_S",
          "size_text": "6.1GB",
          "size_bytes": 6549825126,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:14b-medium-128k-instruct-q3_K_M",
          "size_text": "6.9GB",
          "size_bytes": 7408818585,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:14b-medium-128k-instruct-q3_K_L",
          "size_text": "7.5GB",
          "size_bytes": 8053063680,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:14b-medium-128k-instruct-q4_0",
          "size_text": "7.9GB",
          "size_bytes": 8482560409,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "phi3:14b-medium-128k-instruct-q4_1",
          "size_text": "8.8GB",
          "size_bytes": 9448928051,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:14b-medium-128k-instruct-q4_K_S",
          "size_text": "8.0GB",
          "size_bytes": 8589934592,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:14b-medium-128k-instruct-q4_K_M",
          "size_text": "8.6GB",
          "size_bytes": 9234179686,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:14b-medium-128k-instruct-q5_0",
          "size_text": "9.6GB",
          "size_bytes": 10307921510,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:14b-medium-128k-instruct-q5_1",
          "size_text": "10GB",
          "size_bytes": 10737418240,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:14b-medium-128k-instruct-q5_K_S",
          "size_text": "9.6GB",
          "size_bytes": 10307921510,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:14b-medium-128k-instruct-q5_K_M",
          "size_text": "10GB",
          "size_bytes": 10737418240,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:14b-medium-128k-instruct-q6_K",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:14b-medium-128k-instruct-q8_0",
          "size_text": "15GB",
          "size_bytes": 16106127360,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:14b-medium-128k-instruct-fp16",
          "size_text": "28GB",
          "size_bytes": 30064771072,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:14b-medium-4k-instruct-q2_K",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:14b-medium-4k-instruct-q3_K_S",
          "size_text": "6.1GB",
          "size_bytes": 6549825126,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:14b-medium-4k-instruct-q3_K_M",
          "size_text": "6.9GB",
          "size_bytes": 7408818585,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:14b-medium-4k-instruct-q3_K_L",
          "size_text": "7.5GB",
          "size_bytes": 8053063680,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:14b-medium-4k-instruct-q4_0",
          "size_text": "7.9GB",
          "size_bytes": 8482560409,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:14b-medium-4k-instruct-q4_1",
          "size_text": "8.8GB",
          "size_bytes": 9448928051,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:14b-medium-4k-instruct-q4_K_S",
          "size_text": "8.0GB",
          "size_bytes": 8589934592,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:14b-medium-4k-instruct-q4_K_M",
          "size_text": "8.6GB",
          "size_bytes": 9234179686,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:14b-medium-4k-instruct-q5_0",
          "size_text": "9.6GB",
          "size_bytes": 10307921510,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:14b-medium-4k-instruct-q5_1",
          "size_text": "10GB",
          "size_bytes": 10737418240,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:14b-medium-4k-instruct-q5_K_S",
          "size_text": "9.6GB",
          "size_bytes": 10307921510,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:14b-medium-4k-instruct-q5_K_M",
          "size_text": "10GB",
          "size_bytes": 10737418240,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:14b-medium-4k-instruct-q6_K",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:14b-medium-4k-instruct-q8_0",
          "size_text": "15GB",
          "size_bytes": 16106127360,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:14b-medium-4k-instruct-fp16",
          "size_text": "28GB",
          "size_bytes": 30064771072,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:medium-128k",
          "size_text": "7.9GB",
          "size_bytes": 8482560409,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "phi3:medium-4k",
          "size_text": "7.9GB",
          "size_bytes": 8482560409,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:mini-128k",
          "size_text": "2.2GB",
          "size_bytes": 2362232012,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "phi3:mini-4k",
          "size_text": "2.4GB",
          "size_bytes": 2576980377,
          "context": "4K",
          "input": "Text"
        }
      ],
      "tags_count": 72
    },
    {
      "slug": "qwen3",
      "capabilities": [
        "reasoning",
        "thinking",
        "tools"
      ],
      "pulls": 12000000,
      "pulls_text": "12M \n  Downloads",
      "blurb": "qwen3 Qwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models. 0.6b 1.7b 4b 8b 14b 30b 32b 235b 58 Tags Updated 3 weeks ago",
      "name": "Qwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models.",
      "description": "Readme Qwen 3 is the latest generation of large language models in Qwen series, with newly updated versions of the 30B and 235B models: New 30B model ollama run qwen3:30b New 235B model ollama run qwen3:235b Overview The Qwen 3 family is a comprehensive suite of dense and mixture-of-experts (MoE) models. The flagship model, Qwen3-235B-A22B , achieves competitive results in benchmark evaluations of coding, math, general capabilities, etc., when compared to other top-tier models such as DeepSeek-R1, o1, o3-mini, Grok-3, and Gemini-2.5-Pro. Additionally, the small MoE model, Qwen3-30B-A3B , outcompetes QwQ-32B with 10 times of activated parameters, and even a tiny model like Qwen3-4B can rival the performance of Qwen2.5-72B-Instruct. Significantly enhancement in its reasoning capabilities , surpassing previous QwQ (in thinking mode) and Qwen2.5 instruct models (in non-thinking mode) on mathematics, code generation, and commonsense logical reasoning. Superior human preference alignment , excelling in creative writing, role-playing, multi-turn dialogues, and instruction following, to deliver a more natural, engaging, and immersive conversational experience. Expertise in agent capabilities , enabling precise integration with external tools in both thinking and unthinking modes and achieving leading performance among open-source models in complex agent-based tasks. Support of 100+ languages and dialects with strong capabilities for multilingual instruction following and translation . Reference Blog Write Preview ![Qwen 3 logo](/assets/library/qwen3/a5541098-87ba-4184-a5af-2b63312c2522) **Qwen 3** is the latest generation of large language models in Qwen series, with newly updated versions of the 30B and 235B models: ### New 30B model ``` ollama run qwen3:30b ``` ![Qwen3-30B-A3B-Instruct-2507.jpg](/assets/library/qwen3/bc0ddfea-95b5-49fc-a36e-c817f98a5de0) ### New 235B model ``` ollama run qwen3:235b ``` ![0d7zztq4GB7G2ZYowO-dQ.jpg](/assets/library/qwen3/8426a459-dd88-49cd-",
      "variants": [
        {
          "tag": "qwen3:latest",
          "size_text": "5.2GB",
          "size_bytes": 5583457484,
          "context": "40K",
          "input": "Text"
        },
        {
          "tag": "qwen3:0.6b",
          "size_text": "523MB",
          "size_bytes": 548405248,
          "context": "40K",
          "input": "Text"
        },
        {
          "tag": "qwen3:1.7b",
          "size_text": "1.4GB",
          "size_bytes": 1503238553,
          "context": "40K",
          "input": "Text"
        },
        {
          "tag": "qwen3:4b",
          "size_text": "2.5GB",
          "size_bytes": 2684354560,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3:8b",
          "size_text": "5.2GB",
          "size_bytes": 5583457484,
          "context": "40K",
          "input": "Text"
        },
        {
          "tag": "qwen3:14b",
          "size_text": "9.3GB",
          "size_bytes": 9985798963,
          "context": "40K",
          "input": "Text"
        },
        {
          "tag": "qwen3:30b",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3:32b",
          "size_text": "20GB",
          "size_bytes": 21474836480,
          "context": "40K",
          "input": "Text"
        },
        {
          "tag": "qwen3:235b",
          "size_text": "142GB",
          "size_bytes": 152471339008,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3:0.6b-q4_K_M",
          "size_text": "523MB",
          "size_bytes": 548405248,
          "context": "40K",
          "input": "Text"
        },
        {
          "tag": "qwen3:0.6b-q8_0",
          "size_text": "832MB",
          "size_bytes": 872415232,
          "context": "40K",
          "input": "Text"
        },
        {
          "tag": "qwen3:0.6b-fp16",
          "size_text": "1.5GB",
          "size_bytes": 1610612736,
          "context": "40K",
          "input": "Text"
        },
        {
          "tag": "qwen3:1.7b-q4_K_M",
          "size_text": "1.4GB",
          "size_bytes": 1503238553,
          "context": "40K",
          "input": "Text"
        },
        {
          "tag": "qwen3:1.7b-q8_0",
          "size_text": "2.2GB",
          "size_bytes": 2362232012,
          "context": "40K",
          "input": "Text"
        },
        {
          "tag": "qwen3:1.7b-fp16",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "40K",
          "input": "Text"
        },
        {
          "tag": "qwen3:4b-instruct",
          "size_text": "2.5GB",
          "size_bytes": 2684354560,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3:4b-instruct-2507-q4_K_M",
          "size_text": "2.5GB",
          "size_bytes": 2684354560,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3:4b-instruct-2507-q8_0",
          "size_text": "4.3GB",
          "size_bytes": 4617089843,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3:4b-instruct-2507-fp16",
          "size_text": "8.1GB",
          "size_bytes": 8697308774,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3:4b-thinking",
          "size_text": "2.5GB",
          "size_bytes": 2684354560,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3:4b-thinking-2507-q4_K_M",
          "size_text": "2.5GB",
          "size_bytes": 2684354560,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3:4b-thinking-2507-q8_0",
          "size_text": "4.3GB",
          "size_bytes": 4617089843,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3:4b-thinking-2507-fp16",
          "size_text": "8.1GB",
          "size_bytes": 8697308774,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3:4b-q4_K_M",
          "size_text": "2.6GB",
          "size_bytes": 2791728742,
          "context": "40K",
          "input": "Text"
        },
        {
          "tag": "qwen3:4b-q8_0",
          "size_text": "4.4GB",
          "size_bytes": 4724464025,
          "context": "40K",
          "input": "Text"
        },
        {
          "tag": "qwen3:4b-fp16",
          "size_text": "8.1GB",
          "size_bytes": 8697308774,
          "context": "40K",
          "input": "Text"
        },
        {
          "tag": "qwen3:8b-q4_K_M",
          "size_text": "5.2GB",
          "size_bytes": 5583457484,
          "context": "40K",
          "input": "Text"
        },
        {
          "tag": "qwen3:8b-q8_0",
          "size_text": "8.9GB",
          "size_bytes": 9556302233,
          "context": "40K",
          "input": "Text"
        },
        {
          "tag": "qwen3:8b-fp16",
          "size_text": "16GB",
          "size_bytes": 17179869184,
          "context": "40K",
          "input": "Text"
        },
        {
          "tag": "qwen3:14b-q4_K_M",
          "size_text": "9.3GB",
          "size_bytes": 9985798963,
          "context": "40K",
          "input": "Text"
        },
        {
          "tag": "qwen3:14b-q8_0",
          "size_text": "16GB",
          "size_bytes": 17179869184,
          "context": "40K",
          "input": "Text"
        },
        {
          "tag": "qwen3:14b-fp16",
          "size_text": "30GB",
          "size_bytes": 32212254720,
          "context": "40K",
          "input": "Text"
        },
        {
          "tag": "qwen3:30b-a3b",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3:30b-a3b-instruct-2507-q4_K_M",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3:30b-a3b-q4_K_M",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "40K",
          "input": "Text"
        },
        {
          "tag": "qwen3:30b-a3b-instruct-2507-q8_0",
          "size_text": "32GB",
          "size_bytes": 34359738368,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3:30b-a3b-thinking-2507-q4_K_M",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3:30b-a3b-q8_0",
          "size_text": "33GB",
          "size_bytes": 35433480192,
          "context": "40K",
          "input": "Text"
        },
        {
          "tag": "qwen3:30b-a3b-thinking-2507-q8_0",
          "size_text": "32GB",
          "size_bytes": 34359738368,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3:30b-a3b-fp16",
          "size_text": "61GB",
          "size_bytes": 65498251264,
          "context": "40K",
          "input": "Text"
        },
        {
          "tag": "qwen3:30b-a3b-instruct-2507-fp16",
          "size_text": "61GB",
          "size_bytes": 65498251264,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3:30b-a3b-thinking-2507-fp16",
          "size_text": "61GB",
          "size_bytes": 65498251264,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3:30b-instruct",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3:30b-thinking",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3:32b-q4_K_M",
          "size_text": "20GB",
          "size_bytes": 21474836480,
          "context": "40K",
          "input": "Text"
        },
        {
          "tag": "qwen3:32b-q8_0",
          "size_text": "35GB",
          "size_bytes": 37580963840,
          "context": "40K",
          "input": "Text"
        },
        {
          "tag": "qwen3:32b-fp16",
          "size_text": "66GB",
          "size_bytes": 70866960384,
          "context": "40K",
          "input": "Text"
        },
        {
          "tag": "qwen3:235b-a22b",
          "size_text": "142GB",
          "size_bytes": 152471339008,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3:235b-a22b-instruct-2507-q4_K_M",
          "size_text": "142GB",
          "size_bytes": 152471339008,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3:235b-a22b-q4_K_M",
          "size_text": "142GB",
          "size_bytes": 152471339008,
          "context": "40K",
          "input": "Text"
        },
        {
          "tag": "qwen3:235b-a22b-instruct-2507-q8_0",
          "size_text": "250GB",
          "size_bytes": 268435456000,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3:235b-a22b-thinking-2507-q4_K_M",
          "size_text": "142GB",
          "size_bytes": 152471339008,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3:235b-a22b-q8_0",
          "size_text": "250GB",
          "size_bytes": 268435456000,
          "context": "40K",
          "input": "Text"
        },
        {
          "tag": "qwen3:235b-a22b-thinking-2507-q8_0",
          "size_text": "250GB",
          "size_bytes": 268435456000,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3:235b-a22b-fp16",
          "size_text": "470GB",
          "size_bytes": 504658657280,
          "context": "40K",
          "input": "Text"
        },
        {
          "tag": "qwen3:235b-a22b-thinking-2507-fp16",
          "size_text": "470GB",
          "size_bytes": 504658657280,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3:235b-instruct",
          "size_text": "142GB",
          "size_bytes": 152471339008,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3:235b-thinking",
          "size_text": "142GB",
          "size_bytes": 152471339008,
          "context": "256K",
          "input": "Text"
        }
      ],
      "tags_count": 58
    },
    {
      "slug": "llama3",
      "capabilities": [],
      "pulls": 11500000,
      "pulls_text": "11.5M \n  Downloads",
      "blurb": "llama3 Meta Llama 3: The most capable openly available LLM to date 8b 70b 68 Tags Updated 1 year ago",
      "name": "Meta Llama 3: The most capable openly available LLM to date",
      "description": "Readme Llama 3 The most capable openly available LLM to date. Meta Llama 3, a family of models developed by Meta Inc. are new state-of-the-art , available in both 8B and 70B parameter sizes (pre-trained or instruction-tuned). Llama 3 instruction-tuned models are fine-tuned and optimized for dialogue/chat use cases and outperform many of the available open-source chat models on common benchmarks. CLI Open the terminal and run ollama run llama3 API Example using curl: curl -X POST http://localhost:11434/api/generate -d '{ \"model\": \"llama3\", \"prompt\":\"Why is the sky blue?\" }' API documentation Model variants Instruct is fine-tuned for chat/dialogue use cases. Example: ollama run llama3 ollama run llama3:70b Pre-trained is the base model. Example: ollama run llama3:text ollama run llama3:70b-text References Introducing Meta Llama 3: The most capable openly available LLM to date Write Preview # Llama 3 The most capable openly available LLM to date. <img src=\"https://github.com/ollama/ollama/assets/3325447/15750d75-668c-42bd-aaf2-d0d203136d55\" width=\"660\" /> Meta Llama 3, a family of models developed by Meta Inc. are new state-of-the-art , available in both **8B** and **70B** parameter sizes (pre-trained or instruction-tuned). Llama 3 instruction-tuned models are fine-tuned and optimized for dialogue/chat use cases and outperform many of the available open-source chat models on common benchmarks. <img src=\"https://github.com/ollama/ollama/assets/3325447/8910aebc-cd9e-4d2d-b9c2-258b5ac3eeac\" /> <img src=\"https://github.com/ollama/ollama/assets/3325447/f6df22a6-fd54-4aa2-876b-2b9354821ec6\" /> ### CLI Open the terminal and run `ollama run llama3` ### API Example using curl: ```bash curl -X POST http://localhost:11434/api/generate -d '{ \"model\": \"llama3\", \"prompt\":\"Why is the sky blue?\" }' ``` [API documentation](https://github.com/ollama/ollama/blob/main/docs/api.md) ## Model variants **Instruct** is fine-tuned for chat/dialogue use cases. *Example:* `ollama run llama3` `oll",
      "variants": [
        {
          "tag": "llama3:latest",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:instruct",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:text",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:8b",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:70b",
          "size_text": "40GB",
          "size_bytes": 42949672960,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:8b-instruct-q2_K",
          "size_text": "3.2GB",
          "size_bytes": 3435973836,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:8b-instruct-q3_K_S",
          "size_text": "3.7GB",
          "size_bytes": 3972844748,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:8b-instruct-q3_K_M",
          "size_text": "4.0GB",
          "size_bytes": 4294967296,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:8b-instruct-q3_K_L",
          "size_text": "4.3GB",
          "size_bytes": 4617089843,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:8b-instruct-q4_0",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:8b-instruct-q4_1",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:8b-instruct-q4_K_S",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:8b-instruct-q4_K_M",
          "size_text": "4.9GB",
          "size_bytes": 5261334937,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:8b-instruct-q5_0",
          "size_text": "5.6GB",
          "size_bytes": 6012954214,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:8b-instruct-q5_1",
          "size_text": "6.1GB",
          "size_bytes": 6549825126,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:8b-instruct-q5_K_S",
          "size_text": "5.6GB",
          "size_bytes": 6012954214,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:8b-instruct-q5_K_M",
          "size_text": "5.7GB",
          "size_bytes": 6120328396,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:8b-instruct-q6_K",
          "size_text": "6.6GB",
          "size_bytes": 7086696038,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:8b-instruct-q8_0",
          "size_text": "8.5GB",
          "size_bytes": 9126805504,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:8b-instruct-fp16",
          "size_text": "16GB",
          "size_bytes": 17179869184,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:8b-text",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:8b-text-q2_K",
          "size_text": "3.2GB",
          "size_bytes": 3435973836,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:8b-text-q3_K_S",
          "size_text": "3.7GB",
          "size_bytes": 3972844748,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:8b-text-q3_K_M",
          "size_text": "4.0GB",
          "size_bytes": 4294967296,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:8b-text-q3_K_L",
          "size_text": "4.3GB",
          "size_bytes": 4617089843,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:8b-text-q4_0",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:8b-text-q4_1",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:8b-text-q4_K_S",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:8b-text-q4_K_M",
          "size_text": "4.9GB",
          "size_bytes": 5261334937,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:8b-text-q5_0",
          "size_text": "5.6GB",
          "size_bytes": 6012954214,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:8b-text-q5_1",
          "size_text": "6.1GB",
          "size_bytes": 6549825126,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:8b-text-q5_K_S",
          "size_text": "5.6GB",
          "size_bytes": 6012954214,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:8b-text-q5_K_M",
          "size_text": "5.7GB",
          "size_bytes": 6120328396,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:8b-text-q6_K",
          "size_text": "6.6GB",
          "size_bytes": 7086696038,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:8b-text-q8_0",
          "size_text": "8.5GB",
          "size_bytes": 9126805504,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:8b-text-fp16",
          "size_text": "16GB",
          "size_bytes": 17179869184,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:70b-instruct",
          "size_text": "40GB",
          "size_bytes": 42949672960,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:70b-instruct-q2_K",
          "size_text": "26GB",
          "size_bytes": 27917287424,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:70b-instruct-q3_K_S",
          "size_text": "31GB",
          "size_bytes": 33285996544,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:70b-instruct-q3_K_M",
          "size_text": "34GB",
          "size_bytes": 36507222016,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:70b-instruct-q3_K_L",
          "size_text": "37GB",
          "size_bytes": 39728447488,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:70b-instruct-q4_0",
          "size_text": "40GB",
          "size_bytes": 42949672960,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:70b-instruct-q4_1",
          "size_text": "44GB",
          "size_bytes": 47244640256,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:70b-instruct-q4_K_S",
          "size_text": "40GB",
          "size_bytes": 42949672960,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:70b-instruct-q4_K_M",
          "size_text": "43GB",
          "size_bytes": 46170898432,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:70b-instruct-q5_0",
          "size_text": "49GB",
          "size_bytes": 52613349376,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:70b-instruct-q5_1",
          "size_text": "53GB",
          "size_bytes": 56908316672,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:70b-instruct-q5_K_S",
          "size_text": "49GB",
          "size_bytes": 52613349376,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:70b-instruct-q5_K_M",
          "size_text": "50GB",
          "size_bytes": 53687091200,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:70b-instruct-q6_K",
          "size_text": "58GB",
          "size_bytes": 62277025792,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:70b-instruct-q8_0",
          "size_text": "75GB",
          "size_bytes": 80530636800,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:70b-instruct-fp16",
          "size_text": "141GB",
          "size_bytes": 151397597184,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:70b-text",
          "size_text": "40GB",
          "size_bytes": 42949672960,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:70b-text-q2_K",
          "size_text": "26GB",
          "size_bytes": 27917287424,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:70b-text-q3_K_S",
          "size_text": "31GB",
          "size_bytes": 33285996544,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:70b-text-q3_K_M",
          "size_text": "34GB",
          "size_bytes": 36507222016,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:70b-text-q3_K_L",
          "size_text": "37GB",
          "size_bytes": 39728447488,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:70b-text-q4_0",
          "size_text": "40GB",
          "size_bytes": 42949672960,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:70b-text-q4_1",
          "size_text": "44GB",
          "size_bytes": 47244640256,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:70b-text-q4_K_S",
          "size_text": "40GB",
          "size_bytes": 42949672960,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:70b-text-q4_K_M",
          "size_text": "43GB",
          "size_bytes": 46170898432,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:70b-text-q5_0",
          "size_text": "49GB",
          "size_bytes": 52613349376,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:70b-text-q5_1",
          "size_text": "53GB",
          "size_bytes": 56908316672,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:70b-text-q5_K_S",
          "size_text": "49GB",
          "size_bytes": 52613349376,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:70b-text-q5_K_M",
          "size_text": "50GB",
          "size_bytes": 53687091200,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:70b-text-q6_K",
          "size_text": "58GB",
          "size_bytes": 62277025792,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:70b-text-q8_0",
          "size_text": "75GB",
          "size_bytes": 80530636800,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:70b-text-fp16",
          "size_text": "141GB",
          "size_bytes": 151397597184,
          "context": "8K",
          "input": "Text"
        }
      ],
      "tags_count": 68
    },
    {
      "slug": "llava",
      "capabilities": [
        "multimodal",
        "reasoning",
        "vision"
      ],
      "pulls": 11200000,
      "pulls_text": "11.2M \n  Downloads",
      "blurb": "llava 🌋 LLaVA is a novel end-to-end trained large model that combines a encoder and Vicuna for general-purpose visual and language understanding. Updated to version 1.6. 7b 13b 34b 98 Tags Updated 1 year ago",
      "name": "🌋 LLaVA is a novel end-to-end trained large multimodal model that combines a vision encoder and Vicuna for general-purpose visual and language understanding. Updated to version 1.6.",
      "description": "Readme 🌋 LLaVA: Large Language and Vision Assistant LLaVA is a multimodal model that combines a vision encoder and Vicuna for general-purpose visual and language understanding, achieving impressive chat capabilities mimicking spirits of the multimodal GPT-4. New in LLaVA 1.6: Increasing the input image resolution to up to 4x more pixels, supporting 672x672, 336x1344, 1344x336 resolutions. Better visual reasoning and OCR capability with an improved visual instruction tuning data mixture. Better visual conversation for more scenarios, covering different applications. Better world knowledge and logical reasoning. CLI Usage Run the model: ollama run llava Then at the prompt, include the path to your image in the prompt: >>> What's in this image? /Users/jmorgan/Desktop/smile.png The image features a yellow smiley face, which is likely the central focus of the picture. API Usage curl http://localhost:11434/api/generate -d '{ \"model\": \"llava\", \"prompt\":\"What is in this picture?\", \"images\": [\"iVBORw0KGgoAAAANSUhEUgAAAG0AAABmCAYAAADBPx+VAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAA3VSURBVHgB7Z27r0zdG8fX743i1bi1ikMoFMQloXRpKFFIqI7LH4BEQ+NWIkjQuSWCRIEoULk0gsK1kCBI0IhrQVT7tz/7zZo888yz1r7MnDl7z5xvsjkzs2fP3uu71nNfa7lkAsm7d++Sffv2JbNmzUqcc8m0adOSzZs3Z+/XES4ZckAWJEGWPiCxjsQNLWmQsWjRIpMseaxcuTKpG/7HP27I8P79e7dq1ars/yL4/v27S0ejqwv+cUOGEGGpKHR37tzJCEpHV9tnT58+dXXCJDdECBE2Ojrqjh071hpNECjx4cMHVycM1Uhbv359B2F79+51586daxN/+pyRkRFXKyRDAqxEp4yMlDDzXG1NPnnyJKkThoK0VFd1ELZu3TrzXKxKfW7dMBQ6bcuWLW2v0VlHjx41z717927ba22U9APcw7Nnz1oGEPeL3m3p2mTAYYnFmMOMXybPPXv2bNIPpFZr1NHn4HMw0KRBjg9NuRw95s8PEcz/6DZELQd/09C9QGq5RsmSRybqkwHGjh07OsJSsYYm3ijPpyHzoiacg35MLdDSIS/O1yM778jOTwYUkKNHWUzUWaOsylE00MyI0fcnOwIdjvtNdW/HZwNLGg+sR1kMepSNJXmIwxBZiG8tDTpEZzKg0GItNsosY8USkxDhD0Rinuiko2gfL/RbiD2LZAjU9zKQJj8RDR0vJBR1/Phx9+PHj9Z7REF4nTZkxzX4LCXHrV271qXkBAPGfP/atWvu/PnzHe4C97F48eIsRLZ9+3a3f/9+87dwP1JxaF7/3r17ba+5l4EcaVo0lj3SBq5kGTJSQmLWMjgYNei2GPT1MuMqGTDEFHzeQSP2wi/jGnkmPJ/nhccs44jvDAxpVcxnq0F6",
      "variants": [
        {
          "tag": "llava:latest",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "llava:v1.6",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "llava:7b",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "llava:13b",
          "size_text": "8.0GB",
          "size_bytes": 8589934592,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llava:34b",
          "size_text": "20GB",
          "size_bytes": 21474836480,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llava:7b-v1.5-q2_K",
          "size_text": "3.5GB",
          "size_bytes": 3758096384,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llava:7b-v1.5-q3_K_S",
          "size_text": "3.6GB",
          "size_bytes": 3865470566,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llava:7b-v1.5-q3_K_M",
          "size_text": "3.9GB",
          "size_bytes": 4187593113,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llava:7b-v1.5-q3_K_L",
          "size_text": "4.2GB",
          "size_bytes": 4509715660,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llava:7b-v1.5-q4_0",
          "size_text": "4.5GB",
          "size_bytes": 4831838208,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llava:7b-v1.5-q4_1",
          "size_text": "4.9GB",
          "size_bytes": 5261334937,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llava:7b-v1.5-q4_K_S",
          "size_text": "4.5GB",
          "size_bytes": 4831838208,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llava:7b-v1.5-q4_K_M",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llava:7b-v1.5-q5_0",
          "size_text": "5.3GB",
          "size_bytes": 5690831667,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llava:7b-v1.5-q5_1",
          "size_text": "5.7GB",
          "size_bytes": 6120328396,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llava:7b-v1.5-q5_K_S",
          "size_text": "5.3GB",
          "size_bytes": 5690831667,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llava:7b-v1.5-q5_K_M",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llava:7b-v1.5-q6_K",
          "size_text": "6.2GB",
          "size_bytes": 6657199308,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llava:7b-v1.5-q8_0",
          "size_text": "7.8GB",
          "size_bytes": 8375186227,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llava:7b-v1.5-fp16",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llava:7b-v1.6",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "llava:7b-v1.6-mistral-q2_K",
          "size_text": "3.3GB",
          "size_bytes": 3543348019,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "llava:7b-v1.6-mistral-q3_K_S",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "llava:7b-v1.6-mistral-q3_K_M",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "llava:7b-v1.6-mistral-q3_K_L",
          "size_text": "4.4GB",
          "size_bytes": 4724464025,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "llava:7b-v1.6-mistral-q4_0",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "llava:7b-v1.6-mistral-q4_1",
          "size_text": "5.2GB",
          "size_bytes": 5583457484,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "llava:7b-v1.6-mistral-q4_K_S",
          "size_text": "4.8GB",
          "size_bytes": 5153960755,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "llava:7b-v1.6-mistral-q4_K_M",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "llava:7b-v1.6-mistral-q5_0",
          "size_text": "5.6GB",
          "size_bytes": 6012954214,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "llava:7b-v1.6-mistral-q5_1",
          "size_text": "6.1GB",
          "size_bytes": 6549825126,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "llava:7b-v1.6-mistral-q5_K_S",
          "size_text": "5.6GB",
          "size_bytes": 6012954214,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "llava:7b-v1.6-mistral-q5_K_M",
          "size_text": "5.8GB",
          "size_bytes": 6227702579,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "llava:7b-v1.6-mistral-q6_K",
          "size_text": "6.6GB",
          "size_bytes": 7086696038,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "llava:7b-v1.6-mistral-q8_0",
          "size_text": "8.3GB",
          "size_bytes": 8912057139,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "llava:7b-v1.6-mistral-fp16",
          "size_text": "15GB",
          "size_bytes": 16106127360,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "llava:7b-v1.6-vicuna-q2_K",
          "size_text": "3.2GB",
          "size_bytes": 3435973836,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llava:7b-v1.6-vicuna-q3_K_S",
          "size_text": "3.6GB",
          "size_bytes": 3865470566,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llava:7b-v1.6-vicuna-q3_K_M",
          "size_text": "3.9GB",
          "size_bytes": 4187593113,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llava:7b-v1.6-vicuna-q3_K_L",
          "size_text": "4.2GB",
          "size_bytes": 4509715660,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llava:7b-v1.6-vicuna-q4_0",
          "size_text": "4.5GB",
          "size_bytes": 4831838208,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llava:7b-v1.6-vicuna-q4_1",
          "size_text": "4.9GB",
          "size_bytes": 5261334937,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llava:7b-v1.6-vicuna-q4_K_S",
          "size_text": "4.5GB",
          "size_bytes": 4831838208,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llava:7b-v1.6-vicuna-q4_K_M",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llava:7b-v1.6-vicuna-q5_0",
          "size_text": "5.3GB",
          "size_bytes": 5690831667,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llava:7b-v1.6-vicuna-q5_1",
          "size_text": "5.7GB",
          "size_bytes": 6120328396,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llava:7b-v1.6-vicuna-q5_K_S",
          "size_text": "5.3GB",
          "size_bytes": 5690831667,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llava:7b-v1.6-vicuna-q5_K_M",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llava:7b-v1.6-vicuna-q6_K",
          "size_text": "6.2GB",
          "size_bytes": 6657199308,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llava:7b-v1.6-vicuna-q8_0",
          "size_text": "7.8GB",
          "size_bytes": 8375186227,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llava:7b-v1.6-vicuna-fp16",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llava:13b-v1.5-q2_K",
          "size_text": "6.1GB",
          "size_bytes": 6549825126,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llava:13b-v1.5-q3_K_S",
          "size_text": "6.3GB",
          "size_bytes": 6764573491,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llava:13b-v1.5-q3_K_M",
          "size_text": "7.0GB",
          "size_bytes": 7516192768,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llava:13b-v1.5-q3_K_L",
          "size_text": "7.6GB",
          "size_bytes": 8160437862,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llava:13b-v1.5-q4_0",
          "size_text": "8.0GB",
          "size_bytes": 8589934592,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llava:13b-v1.5-q4_1",
          "size_text": "8.8GB",
          "size_bytes": 9448928051,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llava:13b-v1.5-q4_K_S",
          "size_text": "8.1GB",
          "size_bytes": 8697308774,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llava:13b-v1.5-q4_K_M",
          "size_text": "8.5GB",
          "size_bytes": 9126805504,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llava:13b-v1.5-q5_0",
          "size_text": "9.6GB",
          "size_bytes": 10307921510,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llava:13b-v1.5-q5_1",
          "size_text": "10GB",
          "size_bytes": 10737418240,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llava:13b-v1.5-q5_K_S",
          "size_text": "9.6GB",
          "size_bytes": 10307921510,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llava:13b-v1.5-q5_K_M",
          "size_text": "9.9GB",
          "size_bytes": 10630044057,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llava:13b-v1.5-q6_K",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llava:13b-v1.5-q8_0",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llava:13b-v1.5-fp16",
          "size_text": "27GB",
          "size_bytes": 28991029248,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llava:13b-v1.6",
          "size_text": "8.0GB",
          "size_bytes": 8589934592,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llava:13b-v1.6-vicuna-q2_K",
          "size_text": "5.5GB",
          "size_bytes": 5905580032,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llava:13b-v1.6-vicuna-q3_K_S",
          "size_text": "6.3GB",
          "size_bytes": 6764573491,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llava:13b-v1.6-vicuna-q3_K_M",
          "size_text": "7.0GB",
          "size_bytes": 7516192768,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llava:13b-v1.6-vicuna-q3_K_L",
          "size_text": "7.6GB",
          "size_bytes": 8160437862,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llava:13b-v1.6-vicuna-q4_0",
          "size_text": "8.0GB",
          "size_bytes": 8589934592,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llava:13b-v1.6-vicuna-q4_1",
          "size_text": "8.8GB",
          "size_bytes": 9448928051,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llava:13b-v1.6-vicuna-q4_K_S",
          "size_text": "8.1GB",
          "size_bytes": 8697308774,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llava:13b-v1.6-vicuna-q4_K_M",
          "size_text": "8.5GB",
          "size_bytes": 9126805504,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llava:13b-v1.6-vicuna-q5_0",
          "size_text": "9.6GB",
          "size_bytes": 10307921510,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llava:13b-v1.6-vicuna-q5_1",
          "size_text": "10GB",
          "size_bytes": 10737418240,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llava:13b-v1.6-vicuna-q5_K_S",
          "size_text": "9.6GB",
          "size_bytes": 10307921510,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llava:13b-v1.6-vicuna-q5_K_M",
          "size_text": "9.9GB",
          "size_bytes": 10630044057,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llava:13b-v1.6-vicuna-q6_K",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llava:13b-v1.6-vicuna-q8_0",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llava:13b-v1.6-vicuna-fp16",
          "size_text": "27GB",
          "size_bytes": 28991029248,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llava:34b-v1.6",
          "size_text": "20GB",
          "size_bytes": 21474836480,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llava:34b-v1.6-q2_K",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llava:34b-v1.6-q3_K_S",
          "size_text": "16GB",
          "size_bytes": 17179869184,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llava:34b-v1.6-q3_K_M",
          "size_text": "17GB",
          "size_bytes": 18253611008,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llava:34b-v1.6-q3_K_L",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llava:34b-v1.6-q4_0",
          "size_text": "20GB",
          "size_bytes": 21474836480,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llava:34b-v1.6-q4_1",
          "size_text": "22GB",
          "size_bytes": 23622320128,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llava:34b-v1.6-q4_K_S",
          "size_text": "20GB",
          "size_bytes": 21474836480,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llava:34b-v1.6-q4_K_M",
          "size_text": "21GB",
          "size_bytes": 22548578304,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llava:34b-v1.6-q5_0",
          "size_text": "24GB",
          "size_bytes": 25769803776,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llava:34b-v1.6-q5_1",
          "size_text": "27GB",
          "size_bytes": 28991029248,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llava:34b-v1.6-q5_K_S",
          "size_text": "24GB",
          "size_bytes": 25769803776,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llava:34b-v1.6-q5_K_M",
          "size_text": "25GB",
          "size_bytes": 26843545600,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llava:34b-v1.6-q6_K",
          "size_text": "29GB",
          "size_bytes": 31138512896,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llava:34b-v1.6-q8_0",
          "size_text": "37GB",
          "size_bytes": 39728447488,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llava:34b-v1.6-fp16",
          "size_text": "69GB",
          "size_bytes": 74088185856,
          "context": "4K",
          "input": "Text"
        }
      ],
      "tags_count": 98
    },
    {
      "slug": "gemma2",
      "capabilities": [
        "tools"
      ],
      "pulls": 8700000,
      "pulls_text": "8.7M \n  Downloads",
      "blurb": "gemma2 Google Gemma 2 is a high-performing and efficient model available in three sizes: 2B, 9B, and 27B. 2b 9b 27b 94 Tags Updated 1 year ago",
      "name": "Google Gemma 2 is a high-performing and efficient model available in three sizes: 2B, 9B, and 27B.",
      "description": "Readme Google’s Gemma 2 model is available in three sizes, 2B, 9B and 27B, featuring a brand new architecture designed for class leading performance and efficiency. Class leading performance At 27 billion parameters, Gemma 2 delivers performance surpassing models more than twice its size in benchmarks. This breakthrough efficiency sets a new standard in the open model landscape. Three sizes: 2B, 9B and 27B parameters 2B Parameters ollama run gemma2:2b 9B Parameters ollama run gemma2 27B Parameters ollama run gemma2:27b Benchmark Intended Usage Open Large Language Models (LLMs) have a wide range of applications across various industries and domains. The following list of potential uses is not comprehensive. The purpose of this list is to provide contextual information about the possible use-cases that the model creators considered as part of model training and development. Content Creation and Communication Text Generation: These models can be used to generate creative text formats such as poems, scripts, code, marketing copy, and email drafts. Chatbots and Conversational AI: Power conversational interfaces for customer service, virtual assistants, or interactive applications. Text Summarization: Generate concise summaries of a text corpus, research papers, or reports. Research and Education Natural Language Processing (NLP) Research: These models can serve as a foundation for researchers to experiment with NLP techniques, develop algorithms, and contribute to the advancement of the field. Language Learning Tools: Support interactive language learning experiences, aiding in grammar correction or providing writing practice. Knowledge Exploration: Assist researchers in exploring large bodies of text by generating summaries or answering questions about specific topics. Using Gemma 2 with popular tooling LangChain from langchain_community.llms import Ollama llm = Ollama(model=\"gemma2\") llm.invoke(\"Why is the sky blue?\") LlamaIndex from llama_index.llms.ollama import Olla",
      "variants": [
        {
          "tag": "gemma2:latest",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:2b",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:9b",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:27b",
          "size_text": "16GB",
          "size_bytes": 17179869184,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:2b-instruct-q2_K",
          "size_text": "1.2GB",
          "size_bytes": 1288490188,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:2b-instruct-q3_K_S",
          "size_text": "1.4GB",
          "size_bytes": 1503238553,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:2b-instruct-q3_K_M",
          "size_text": "1.5GB",
          "size_bytes": 1610612736,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:2b-instruct-q3_K_L",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:2b-instruct-q4_0",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:2b-instruct-q4_1",
          "size_text": "1.8GB",
          "size_bytes": 1932735283,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:2b-instruct-q4_K_S",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:2b-instruct-q4_K_M",
          "size_text": "1.7GB",
          "size_bytes": 1825361100,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:2b-instruct-q5_0",
          "size_text": "1.9GB",
          "size_bytes": 2040109465,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:2b-instruct-q5_1",
          "size_text": "2.0GB",
          "size_bytes": 2147483648,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:2b-instruct-q5_K_S",
          "size_text": "1.9GB",
          "size_bytes": 2040109465,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:2b-instruct-q5_K_M",
          "size_text": "1.9GB",
          "size_bytes": 2040109465,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:2b-instruct-q6_K",
          "size_text": "2.2GB",
          "size_bytes": 2362232012,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:2b-instruct-q8_0",
          "size_text": "2.8GB",
          "size_bytes": 3006477107,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:2b-instruct-fp16",
          "size_text": "5.2GB",
          "size_bytes": 5583457484,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:2b-text-q2_K",
          "size_text": "1.2GB",
          "size_bytes": 1288490188,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:2b-text-q3_K_S",
          "size_text": "1.4GB",
          "size_bytes": 1503238553,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:2b-text-q3_K_M",
          "size_text": "1.5GB",
          "size_bytes": 1610612736,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:2b-text-q3_K_L",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:2b-text-q4_0",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:2b-text-q4_1",
          "size_text": "1.8GB",
          "size_bytes": 1932735283,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:2b-text-q4_K_S",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:2b-text-q4_K_M",
          "size_text": "1.7GB",
          "size_bytes": 1825361100,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:2b-text-q5_0",
          "size_text": "1.9GB",
          "size_bytes": 2040109465,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:2b-text-q5_1",
          "size_text": "2.0GB",
          "size_bytes": 2147483648,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:2b-text-q5_K_S",
          "size_text": "1.9GB",
          "size_bytes": 2040109465,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:2b-text-q5_K_M",
          "size_text": "1.9GB",
          "size_bytes": 2040109465,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:2b-text-q6_K",
          "size_text": "2.2GB",
          "size_bytes": 2362232012,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:2b-text-q8_0",
          "size_text": "2.8GB",
          "size_bytes": 3006477107,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:2b-text-fp16",
          "size_text": "5.2GB",
          "size_bytes": 5583457484,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:9b-instruct-q2_K",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:9b-instruct-q3_K_S",
          "size_text": "4.3GB",
          "size_bytes": 4617089843,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:9b-instruct-q3_K_M",
          "size_text": "4.8GB",
          "size_bytes": 5153960755,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:9b-instruct-q3_K_L",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:9b-instruct-q4_0",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:9b-instruct-q4_1",
          "size_text": "6.0GB",
          "size_bytes": 6442450944,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:9b-instruct-q4_K_S",
          "size_text": "5.5GB",
          "size_bytes": 5905580032,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:9b-instruct-q4_K_M",
          "size_text": "5.8GB",
          "size_bytes": 6227702579,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:9b-instruct-q5_0",
          "size_text": "6.5GB",
          "size_bytes": 6979321856,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:9b-instruct-q5_1",
          "size_text": "7.0GB",
          "size_bytes": 7516192768,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:9b-instruct-q5_K_S",
          "size_text": "6.5GB",
          "size_bytes": 6979321856,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:9b-instruct-q5_K_M",
          "size_text": "6.6GB",
          "size_bytes": 7086696038,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:9b-instruct-q6_K",
          "size_text": "7.6GB",
          "size_bytes": 8160437862,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:9b-instruct-q8_0",
          "size_text": "9.8GB",
          "size_bytes": 10522669875,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:9b-instruct-fp16",
          "size_text": "18GB",
          "size_bytes": 19327352832,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:9b-text-q2_K",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:9b-text-q3_K_S",
          "size_text": "4.3GB",
          "size_bytes": 4617089843,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:9b-text-q3_K_M",
          "size_text": "4.8GB",
          "size_bytes": 5153960755,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:9b-text-q3_K_L",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:9b-text-q4_0",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:9b-text-q4_1",
          "size_text": "6.0GB",
          "size_bytes": 6442450944,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:9b-text-q4_K_S",
          "size_text": "5.5GB",
          "size_bytes": 5905580032,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:9b-text-q4_K_M",
          "size_text": "5.8GB",
          "size_bytes": 6227702579,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:9b-text-q5_0",
          "size_text": "6.5GB",
          "size_bytes": 6979321856,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:9b-text-q5_1",
          "size_text": "7.0GB",
          "size_bytes": 7516192768,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:9b-text-q5_K_S",
          "size_text": "6.5GB",
          "size_bytes": 6979321856,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:9b-text-q5_K_M",
          "size_text": "6.6GB",
          "size_bytes": 7086696038,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:9b-text-q6_K",
          "size_text": "7.6GB",
          "size_bytes": 8160437862,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:9b-text-q8_0",
          "size_text": "9.8GB",
          "size_bytes": 10522669875,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:9b-text-fp16",
          "size_text": "18GB",
          "size_bytes": 19327352832,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:27b-instruct-q2_K",
          "size_text": "10GB",
          "size_bytes": 10737418240,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:27b-instruct-q3_K_S",
          "size_text": "12GB",
          "size_bytes": 12884901888,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:27b-instruct-q3_K_M",
          "size_text": "13GB",
          "size_bytes": 13958643712,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:27b-instruct-q3_K_L",
          "size_text": "15GB",
          "size_bytes": 16106127360,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:27b-instruct-q4_0",
          "size_text": "16GB",
          "size_bytes": 17179869184,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:27b-instruct-q4_1",
          "size_text": "17GB",
          "size_bytes": 18253611008,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:27b-instruct-q4_K_S",
          "size_text": "16GB",
          "size_bytes": 17179869184,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:27b-instruct-q4_K_M",
          "size_text": "17GB",
          "size_bytes": 18253611008,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:27b-instruct-q5_0",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:27b-instruct-q5_1",
          "size_text": "21GB",
          "size_bytes": 22548578304,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:27b-instruct-q5_K_S",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:27b-instruct-q5_K_M",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:27b-instruct-q6_K",
          "size_text": "22GB",
          "size_bytes": 23622320128,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:27b-instruct-q8_0",
          "size_text": "29GB",
          "size_bytes": 31138512896,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:27b-instruct-fp16",
          "size_text": "54GB",
          "size_bytes": 57982058496,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:27b-text-q2_K",
          "size_text": "10GB",
          "size_bytes": 10737418240,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:27b-text-q3_K_S",
          "size_text": "12GB",
          "size_bytes": 12884901888,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:27b-text-q3_K_M",
          "size_text": "13GB",
          "size_bytes": 13958643712,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:27b-text-q3_K_L",
          "size_text": "15GB",
          "size_bytes": 16106127360,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:27b-text-q4_0",
          "size_text": "16GB",
          "size_bytes": 17179869184,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:27b-text-q4_1",
          "size_text": "17GB",
          "size_bytes": 18253611008,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:27b-text-q4_K_S",
          "size_text": "16GB",
          "size_bytes": 17179869184,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:27b-text-q4_K_M",
          "size_text": "17GB",
          "size_bytes": 18253611008,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:27b-text-q5_0",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:27b-text-q5_1",
          "size_text": "21GB",
          "size_bytes": 22548578304,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:27b-text-q5_K_S",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:27b-text-q5_K_M",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:27b-text-q6_K",
          "size_text": "22GB",
          "size_bytes": 23622320128,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:27b-text-q8_0",
          "size_text": "29GB",
          "size_bytes": 31138512896,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma2:27b-text-fp16",
          "size_text": "54GB",
          "size_bytes": 57982058496,
          "context": "8K",
          "input": "Text"
        }
      ],
      "tags_count": 94
    },
    {
      "slug": "qwen2.5-coder",
      "capabilities": [
        "reasoning",
        "tools"
      ],
      "pulls": 8000000,
      "pulls_text": "8M \n  Downloads",
      "blurb": "qwen2.5-coder The latest series of Code-Specific Qwen models, with significant improvements in code generation, code reasoning, and code fixing. 0.5b 1.5b 3b 7b 14b 32b 199 Tags Updated 5 months ago",
      "name": "The latest series of Code-Specific Qwen models, with significant improvements in code generation, code reasoning, and code fixing.",
      "description": "Readme Qwen 2.5 Coder series of models are now updated in 6 sizes: 0.5B, 1.5B, 3B, 7B, 14B and 32B . There are significant improvements in code generation , code reasoning and code fixing . The 32B model has competitive performance with OpenAI’s GPT-4o. 32B: ollama run qwen2.5-coder:32b 14B: ollama run qwen2.5-coder:14b 7B: ollama run qwen2.5-coder:7b 3B: ollama run qwen2.5-coder:3b 1.5B: ollama run qwen2.5-coder:1.5b 0.5B: ollama run qwen2.5-coder:0.5b Code capabilities reaching state of the art for open-source models Code Generation: Qwen2.5 Coder 32B Instruct, as the flagship model of this open-source release, has achieved the best performance among open-source models on multiple popular code generation benchmarks (EvalPlus, LiveCodeBench, BigCodeBench), and has competitive performance with GPT-4o. Code Repair: Code repair is an important programming skill. Qwen2.5 Coder 32B Instruct can help users fix errors in their code, making programming more efficient. Aider is a popular benchmark for code repair, and Qwen2.5 Coder 32B Instruct scored 73.7, performing comparably to GPT-4o on Aider. Code Reasoning: Code reasoning refers to the model’s ability to learn the process of code execution and accurately predict the model’s inputs and outputs. The recently released Qwen2.5 Coder 7B Instruct has already shown impressive performance in code reasoning, and this 32B model takes it a step further. Multiple programming languages An intelligent programming assistant should be familiar with all programming languages. Qwen 2.5 Coder 32B performs excellent across more than 40 programming languages, scoring 65.9 on McEval, with impressive performances in languages like Haskell and Racket. The Qwen team used their own unique data cleaning and balancing during the pre-training phase. Additionally, the multi-language code repair capabilities of Qwen 2.5 Coder 32B Instruct remain impressive, aiding users in understanding and modifying programming languages they are familiar with, s",
      "variants": [
        {
          "tag": "qwen2.5-coder:latest",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:0.5b",
          "size_text": "398MB",
          "size_bytes": 417333248,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:1.5b",
          "size_text": "986MB",
          "size_bytes": 1033895936,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:3b",
          "size_text": "1.9GB",
          "size_bytes": 2040109465,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:7b",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:14b",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:32b",
          "size_text": "20GB",
          "size_bytes": 21474836480,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:0.5b-base",
          "size_text": "531MB",
          "size_bytes": 556793856,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:0.5b-base-q2_K",
          "size_text": "339MB",
          "size_bytes": 355467264,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:0.5b-base-q3_K_S",
          "size_text": "338MB",
          "size_bytes": 354418688,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:0.5b-base-q3_K_M",
          "size_text": "355MB",
          "size_bytes": 372244480,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:0.5b-base-q3_K_L",
          "size_text": "369MB",
          "size_bytes": 386924544,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:0.5b-base-q4_0",
          "size_text": "352MB",
          "size_bytes": 369098752,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:0.5b-base-q4_1",
          "size_text": "375MB",
          "size_bytes": 393216000,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:0.5b-base-q4_K_S",
          "size_text": "385MB",
          "size_bytes": 403701760,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:0.5b-base-q4_K_M",
          "size_text": "398MB",
          "size_bytes": 417333248,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:0.5b-base-q5_0",
          "size_text": "397MB",
          "size_bytes": 416284672,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:0.5b-base-q5_1",
          "size_text": "419MB",
          "size_bytes": 439353344,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:0.5b-base-q5_K_S",
          "size_text": "413MB",
          "size_bytes": 433061888,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:0.5b-base-q5_K_M",
          "size_text": "420MB",
          "size_bytes": 440401920,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:0.5b-base-q6_K",
          "size_text": "506MB",
          "size_bytes": 530579456,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:0.5b-base-q8_0",
          "size_text": "531MB",
          "size_bytes": 556793856,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:0.5b-base-fp16",
          "size_text": "994MB",
          "size_bytes": 1042284544,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:0.5b-instruct",
          "size_text": "398MB",
          "size_bytes": 417333248,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:0.5b-instruct-q2_K",
          "size_text": "339MB",
          "size_bytes": 355467264,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:0.5b-instruct-q3_K_S",
          "size_text": "338MB",
          "size_bytes": 354418688,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:0.5b-instruct-q3_K_M",
          "size_text": "355MB",
          "size_bytes": 372244480,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:0.5b-instruct-q3_K_L",
          "size_text": "369MB",
          "size_bytes": 386924544,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:0.5b-instruct-q4_0",
          "size_text": "352MB",
          "size_bytes": 369098752,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:0.5b-instruct-q4_1",
          "size_text": "375MB",
          "size_bytes": 393216000,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:0.5b-instruct-q4_K_S",
          "size_text": "385MB",
          "size_bytes": 403701760,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:0.5b-instruct-q4_K_M",
          "size_text": "398MB",
          "size_bytes": 417333248,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:0.5b-instruct-q5_0",
          "size_text": "397MB",
          "size_bytes": 416284672,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:0.5b-instruct-q5_1",
          "size_text": "419MB",
          "size_bytes": 439353344,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:0.5b-instruct-q5_K_S",
          "size_text": "413MB",
          "size_bytes": 433061888,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:0.5b-instruct-q5_K_M",
          "size_text": "420MB",
          "size_bytes": 440401920,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:0.5b-instruct-q6_K",
          "size_text": "506MB",
          "size_bytes": 530579456,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:0.5b-instruct-q8_0",
          "size_text": "531MB",
          "size_bytes": 556793856,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:0.5b-instruct-fp16",
          "size_text": "994MB",
          "size_bytes": 1042284544,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:1.5b-base",
          "size_text": "986MB",
          "size_bytes": 1033895936,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:1.5b-base-q2_K",
          "size_text": "676MB",
          "size_bytes": 708837376,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:1.5b-base-q3_K_S",
          "size_text": "761MB",
          "size_bytes": 797966336,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:1.5b-base-q3_K_M",
          "size_text": "824MB",
          "size_bytes": 864026624,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:1.5b-base-q3_K_L",
          "size_text": "880MB",
          "size_bytes": 922746880,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:1.5b-base-q4_0",
          "size_text": "935MB",
          "size_bytes": 980418560,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:1.5b-base-q4_1",
          "size_text": "1.0GB",
          "size_bytes": 1073741824,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:1.5b-base-q4_K_S",
          "size_text": "940MB",
          "size_bytes": 985661440,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:1.5b-base-q4_K_M",
          "size_text": "986MB",
          "size_bytes": 1033895936,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:1.5b-base-q5_0",
          "size_text": "1.1GB",
          "size_bytes": 1181116006,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:1.5b-base-q5_1",
          "size_text": "1.2GB",
          "size_bytes": 1288490188,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:1.5b-base-q5_K_S",
          "size_text": "1.1GB",
          "size_bytes": 1181116006,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:1.5b-base-q5_K_M",
          "size_text": "1.1GB",
          "size_bytes": 1181116006,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:1.5b-base-q6_K",
          "size_text": "1.3GB",
          "size_bytes": 1395864371,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:1.5b-base-q8_0",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:1.5b-base-fp16",
          "size_text": "3.1GB",
          "size_bytes": 3328599654,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:1.5b-instruct",
          "size_text": "986MB",
          "size_bytes": 1033895936,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:1.5b-instruct-q2_K",
          "size_text": "676MB",
          "size_bytes": 708837376,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:1.5b-instruct-q3_K_S",
          "size_text": "761MB",
          "size_bytes": 797966336,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:1.5b-instruct-q3_K_M",
          "size_text": "824MB",
          "size_bytes": 864026624,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:1.5b-instruct-q3_K_L",
          "size_text": "880MB",
          "size_bytes": 922746880,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:1.5b-instruct-q4_0",
          "size_text": "935MB",
          "size_bytes": 980418560,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:1.5b-instruct-q4_1",
          "size_text": "1.0GB",
          "size_bytes": 1073741824,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:1.5b-instruct-q4_K_S",
          "size_text": "940MB",
          "size_bytes": 985661440,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:1.5b-instruct-q4_K_M",
          "size_text": "986MB",
          "size_bytes": 1033895936,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:1.5b-instruct-q5_0",
          "size_text": "1.1GB",
          "size_bytes": 1181116006,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:1.5b-instruct-q5_1",
          "size_text": "1.2GB",
          "size_bytes": 1288490188,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:1.5b-instruct-q5_K_S",
          "size_text": "1.1GB",
          "size_bytes": 1181116006,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:1.5b-instruct-q5_K_M",
          "size_text": "1.1GB",
          "size_bytes": 1181116006,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:1.5b-instruct-q6_K",
          "size_text": "1.3GB",
          "size_bytes": 1395864371,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:1.5b-instruct-q8_0",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:1.5b-instruct-fp16",
          "size_text": "3.1GB",
          "size_bytes": 3328599654,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:3b-base",
          "size_text": "1.9GB",
          "size_bytes": 2040109465,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:3b-base-q2_K",
          "size_text": "1.3GB",
          "size_bytes": 1395864371,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:3b-base-q3_K_S",
          "size_text": "1.5GB",
          "size_bytes": 1610612736,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:3b-base-q3_K_M",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:3b-base-q3_K_L",
          "size_text": "1.7GB",
          "size_bytes": 1825361100,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:3b-base-q4_0",
          "size_text": "1.8GB",
          "size_bytes": 1932735283,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:3b-base-q4_1",
          "size_text": "2.0GB",
          "size_bytes": 2147483648,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:3b-base-q4_K_S",
          "size_text": "1.8GB",
          "size_bytes": 1932735283,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:3b-base-q4_K_M",
          "size_text": "1.9GB",
          "size_bytes": 2040109465,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:3b-base-q5_0",
          "size_text": "2.2GB",
          "size_bytes": 2362232012,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:3b-base-q5_1",
          "size_text": "2.3GB",
          "size_bytes": 2469606195,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:3b-base-q5_K_S",
          "size_text": "2.2GB",
          "size_bytes": 2362232012,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:3b-base-q5_K_M",
          "size_text": "2.2GB",
          "size_bytes": 2362232012,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:3b-base-q6_K",
          "size_text": "2.5GB",
          "size_bytes": 2684354560,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:3b-base-q8_0",
          "size_text": "3.3GB",
          "size_bytes": 3543348019,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:3b-base-fp16",
          "size_text": "6.2GB",
          "size_bytes": 6657199308,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:3b-instruct",
          "size_text": "1.9GB",
          "size_bytes": 2040109465,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:3b-instruct-q2_K",
          "size_text": "1.3GB",
          "size_bytes": 1395864371,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:3b-instruct-q3_K_S",
          "size_text": "1.5GB",
          "size_bytes": 1610612736,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:3b-instruct-q3_K_M",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:3b-instruct-q3_K_L",
          "size_text": "1.7GB",
          "size_bytes": 1825361100,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:3b-instruct-q4_0",
          "size_text": "1.8GB",
          "size_bytes": 1932735283,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:3b-instruct-q4_1",
          "size_text": "2.0GB",
          "size_bytes": 2147483648,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:3b-instruct-q4_K_S",
          "size_text": "1.8GB",
          "size_bytes": 1932735283,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:3b-instruct-q4_K_M",
          "size_text": "1.9GB",
          "size_bytes": 2040109465,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:3b-instruct-q5_0",
          "size_text": "2.2GB",
          "size_bytes": 2362232012,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:3b-instruct-q5_1",
          "size_text": "2.3GB",
          "size_bytes": 2469606195,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:3b-instruct-q5_K_S",
          "size_text": "2.2GB",
          "size_bytes": 2362232012,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:3b-instruct-q5_K_M",
          "size_text": "2.2GB",
          "size_bytes": 2362232012,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:3b-instruct-q6_K",
          "size_text": "2.5GB",
          "size_bytes": 2684354560,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:3b-instruct-q8_0",
          "size_text": "3.3GB",
          "size_bytes": 3543348019,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:3b-instruct-fp16",
          "size_text": "6.2GB",
          "size_bytes": 6657199308,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:7b-base",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:7b-base-q2_K",
          "size_text": "3.0GB",
          "size_bytes": 3221225472,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:7b-base-q3_K_S",
          "size_text": "3.5GB",
          "size_bytes": 3758096384,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:7b-base-q3_K_M",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:7b-base-q3_K_L",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:7b-base-q4_0",
          "size_text": "4.4GB",
          "size_bytes": 4724464025,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:7b-base-q4_1",
          "size_text": "4.9GB",
          "size_bytes": 5261334937,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:7b-base-q4_K_S",
          "size_text": "4.5GB",
          "size_bytes": 4831838208,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:7b-base-q4_K_M",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:7b-base-q5_0",
          "size_text": "5.3GB",
          "size_bytes": 5690831667,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:7b-base-q5_1",
          "size_text": "5.8GB",
          "size_bytes": 6227702579,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:7b-base-q5_K_S",
          "size_text": "5.3GB",
          "size_bytes": 5690831667,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:7b-base-q5_K_M",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:7b-base-q6_K",
          "size_text": "6.3GB",
          "size_bytes": 6764573491,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:7b-base-q8_0",
          "size_text": "8.1GB",
          "size_bytes": 8697308774,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:7b-base-fp16",
          "size_text": "15GB",
          "size_bytes": 16106127360,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:7b-instruct",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:7b-instruct-q2_K",
          "size_text": "3.0GB",
          "size_bytes": 3221225472,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:7b-instruct-q3_K_S",
          "size_text": "3.5GB",
          "size_bytes": 3758096384,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:7b-instruct-q3_K_M",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:7b-instruct-q3_K_L",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:7b-instruct-q4_0",
          "size_text": "4.4GB",
          "size_bytes": 4724464025,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:7b-instruct-q4_1",
          "size_text": "4.9GB",
          "size_bytes": 5261334937,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:7b-instruct-q4_K_S",
          "size_text": "4.5GB",
          "size_bytes": 4831838208,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:7b-instruct-q4_K_M",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:7b-instruct-q5_0",
          "size_text": "5.3GB",
          "size_bytes": 5690831667,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:7b-instruct-q5_1",
          "size_text": "5.8GB",
          "size_bytes": 6227702579,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:7b-instruct-q5_K_S",
          "size_text": "5.3GB",
          "size_bytes": 5690831667,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:7b-instruct-q5_K_M",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:7b-instruct-q6_K",
          "size_text": "6.3GB",
          "size_bytes": 6764573491,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:7b-instruct-q8_0",
          "size_text": "8.1GB",
          "size_bytes": 8697308774,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:7b-instruct-fp16",
          "size_text": "15GB",
          "size_bytes": 16106127360,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:14b-base",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:14b-base-q2_K",
          "size_text": "5.8GB",
          "size_bytes": 6227702579,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:14b-base-q3_K_S",
          "size_text": "6.7GB",
          "size_bytes": 7194070220,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:14b-base-q3_K_M",
          "size_text": "7.3GB",
          "size_bytes": 7838315315,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:14b-base-q3_K_L",
          "size_text": "7.9GB",
          "size_bytes": 8482560409,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:14b-base-q4_0",
          "size_text": "8.5GB",
          "size_bytes": 9126805504,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:14b-base-q4_1",
          "size_text": "9.4GB",
          "size_bytes": 10093173145,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:14b-base-q4_K_S",
          "size_text": "8.6GB",
          "size_bytes": 9234179686,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:14b-base-q4_K_M",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:14b-base-q5_0",
          "size_text": "10GB",
          "size_bytes": 10737418240,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:14b-base-q5_1",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:14b-base-q5_K_S",
          "size_text": "10GB",
          "size_bytes": 10737418240,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:14b-base-q5_K_M",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:14b-base-q6_K",
          "size_text": "12GB",
          "size_bytes": 12884901888,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:14b-base-q8_0",
          "size_text": "16GB",
          "size_bytes": 17179869184,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:14b-base-fp16",
          "size_text": "30GB",
          "size_bytes": 32212254720,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:14b-instruct",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:14b-instruct-q2_K",
          "size_text": "5.8GB",
          "size_bytes": 6227702579,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:14b-instruct-q3_K_S",
          "size_text": "6.7GB",
          "size_bytes": 7194070220,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:14b-instruct-q3_K_M",
          "size_text": "7.3GB",
          "size_bytes": 7838315315,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:14b-instruct-q3_K_L",
          "size_text": "7.9GB",
          "size_bytes": 8482560409,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:14b-instruct-q4_0",
          "size_text": "8.5GB",
          "size_bytes": 9126805504,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:14b-instruct-q4_1",
          "size_text": "9.4GB",
          "size_bytes": 10093173145,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:14b-instruct-q4_K_S",
          "size_text": "8.6GB",
          "size_bytes": 9234179686,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:14b-instruct-q4_K_M",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:14b-instruct-q5_0",
          "size_text": "10GB",
          "size_bytes": 10737418240,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:14b-instruct-q5_1",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:14b-instruct-q5_K_S",
          "size_text": "10GB",
          "size_bytes": 10737418240,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:14b-instruct-q5_K_M",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:14b-instruct-q6_K",
          "size_text": "12GB",
          "size_bytes": 12884901888,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:14b-instruct-q8_0",
          "size_text": "16GB",
          "size_bytes": 17179869184,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:14b-instruct-fp16",
          "size_text": "30GB",
          "size_bytes": 32212254720,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:32b-base",
          "size_text": "20GB",
          "size_bytes": 21474836480,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:32b-base-q2_K",
          "size_text": "12GB",
          "size_bytes": 12884901888,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:32b-base-q3_K_S",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:32b-base-q3_K_M",
          "size_text": "16GB",
          "size_bytes": 17179869184,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:32b-base-q3_K_L",
          "size_text": "17GB",
          "size_bytes": 18253611008,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:32b-base-q4_0",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:32b-base-q4_1",
          "size_text": "21GB",
          "size_bytes": 22548578304,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:32b-base-q4_K_S",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:32b-base-q4_K_M",
          "size_text": "20GB",
          "size_bytes": 21474836480,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:32b-base-q5_0",
          "size_text": "23GB",
          "size_bytes": 24696061952,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:32b-base-q5_1",
          "size_text": "25GB",
          "size_bytes": 26843545600,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:32b-base-q5_K_S",
          "size_text": "23GB",
          "size_bytes": 24696061952,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:32b-base-q5_K_M",
          "size_text": "23GB",
          "size_bytes": 24696061952,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:32b-base-q6_K",
          "size_text": "27GB",
          "size_bytes": 28991029248,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:32b-base-q8_0",
          "size_text": "35GB",
          "size_bytes": 37580963840,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:32b-base-fp16",
          "size_text": "66GB",
          "size_bytes": 70866960384,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:32b-instruct",
          "size_text": "20GB",
          "size_bytes": 21474836480,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:32b-instruct-q2_K",
          "size_text": "12GB",
          "size_bytes": 12884901888,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:32b-instruct-q3_K_S",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:32b-instruct-q3_K_M",
          "size_text": "16GB",
          "size_bytes": 17179869184,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:32b-instruct-q3_K_L",
          "size_text": "17GB",
          "size_bytes": 18253611008,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:32b-instruct-q4_0",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:32b-instruct-q4_1",
          "size_text": "21GB",
          "size_bytes": 22548578304,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:32b-instruct-q4_K_S",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:32b-instruct-q4_K_M",
          "size_text": "20GB",
          "size_bytes": 21474836480,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:32b-instruct-q5_0",
          "size_text": "23GB",
          "size_bytes": 24696061952,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:32b-instruct-q5_1",
          "size_text": "25GB",
          "size_bytes": 26843545600,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:32b-instruct-q5_K_S",
          "size_text": "23GB",
          "size_bytes": 24696061952,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:32b-instruct-q5_K_M",
          "size_text": "23GB",
          "size_bytes": 24696061952,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:32b-instruct-q6_K",
          "size_text": "27GB",
          "size_bytes": 28991029248,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:32b-instruct-q8_0",
          "size_text": "35GB",
          "size_bytes": 37580963840,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5-coder:32b-instruct-fp16",
          "size_text": "66GB",
          "size_bytes": 70866960384,
          "context": "32K",
          "input": "Text"
        }
      ],
      "tags_count": 199
    },
    {
      "slug": "phi4",
      "capabilities": [
        "reasoning"
      ],
      "pulls": 5900000,
      "pulls_text": "5.9M \n  Downloads",
      "blurb": "phi4 Phi-4 is a 14B parameter, state-of-the-art open model from Microsoft. 14b 5 Tags Updated 9 months ago",
      "name": "Phi-4 is a 14B parameter, state-of-the-art open model from Microsoft.",
      "description": "Readme Phi-4 is a 14B parameter, state-of-the-art open model built upon a blend of synthetic datasets, data from filtered public domain websites, and acquired academic books and Q&A datasets. The model underwent a rigorous enhancement and alignment process, incorporating both supervised fine-tuning and direct preference optimization to ensure precise instruction adherence and robust safety measures. Context length: 16k tokens Primary use cases The model is designed to accelerate research on language models, for use as a building block for generative AI powered features. It provides uses for general purpose AI systems and applications (primarily in English) which require: Memory/compute constrained environments. Latency bound scenarios. Reasoning and logic. Out-of-scope use cases The models are not specifically designed or evaluated for all downstream purposes, thus: Developers should consider common limitations of language models as they select use cases, and evaluate and mitigate for accuracy, safety, and fairness before using within a specific downstream use case, particularly for high-risk scenarios. Developers should be aware of and adhere to applicable laws or regulations (including privacy, trade compliance laws, etc.) that are relevant to their use case, including the model’s focus on English. Nothing contained in this readme should be interpreted as or deemed a restriction or modification to the license the model is released under. Write Preview <img src=\"https://ollama.com/assets/library/phi3.5/dbf19a17-e3fd-46b6-a059-e6b4f1fae59f\" width=\"320\" /> **Phi-4** is a 14B parameter, state-of-the-art open model built upon a blend of synthetic datasets, data from filtered public domain websites, and acquired academic books and Q&A datasets. ![Phi-4 benchmark](/assets/library/phi4/67391ae2-e565-4173-ac3c-5e49bc977ac4) The model underwent a rigorous enhancement and alignment process, incorporating both supervised fine-tuning and direct preference optimization to ensur",
      "variants": [
        {
          "tag": "phi4:latest",
          "size_text": "9.1GB",
          "size_bytes": 9771050598,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "phi4:14b",
          "size_text": "9.1GB",
          "size_bytes": 9771050598,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "phi4:14b-q4_K_M",
          "size_text": "9.1GB",
          "size_bytes": 9771050598,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "phi4:14b-q8_0",
          "size_text": "16GB",
          "size_bytes": 17179869184,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "phi4:14b-fp16",
          "size_text": "29GB",
          "size_bytes": 31138512896,
          "context": "16K",
          "input": "Text"
        }
      ],
      "tags_count": 5
    },
    {
      "slug": "gemma",
      "capabilities": [
        "reasoning"
      ],
      "pulls": 5400000,
      "pulls_text": "5.4M \n  Downloads",
      "blurb": "gemma Gemma is a family of lightweight, state-of-the-art open models built by Google DeepMind. Updated to version 1.1 2b 7b 102 Tags Updated 1 year ago",
      "name": "Gemma is a family of lightweight, state-of-the-art open models built by Google DeepMind. Updated to version 1.1",
      "description": "Readme Note: this model requires Ollama 0.1.26 or later. Download it here . Gemma is a new open model developed by Google and its DeepMind team. It’s inspired by Gemini models at Google. Gemma is available in both 2b and 7b parameter sizes: ollama run gemma:2b ollama run gemma:7b (default) The models undergo training on a diverse dataset of web documents to expose them to a wide range of linguistic styles, topics, and vocabularies. This includes code to learn syntax and patterns of programming languages, as well as mathematical text to grasp logical reasoning. To ensure the safety of the model, the team employed various data cleaning and filtering techniques, including rigorous filtering for CSAM (child sexual abuse material), sensitive data filtering, and filtering based on content quality in compliance with Google’s policies. Reference Google Gemma Write Preview <img src=\"https://github.com/ollama/ollama/assets/251292/01333db3-c27b-4044-88b3-9b2ffbe06415\"/> > Note: this model requires Ollama 0.1.26 or later. [Download it here](https://ollama.com/download). Gemma is a new open model developed by Google and its DeepMind team. It’s inspired by Gemini models at Google. Gemma is available in both `2b` and `7b` parameter sizes: * `ollama run gemma:2b` * `ollama run gemma:7b` (default) The models undergo training on a diverse dataset of web documents to expose them to a wide range of linguistic styles, topics, and vocabularies. This includes code to learn syntax and patterns of programming languages, as well as mathematical text to grasp logical reasoning. To ensure the safety of the model, the team employed various data cleaning and filtering techniques, including rigorous filtering for CSAM (child sexual abuse material), sensitive data filtering, and filtering based on content quality in compliance with Google’s policies. # Reference [Google Gemma](https://ai.google.dev/gemma/docs/model_card) Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "gemma:latest",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:instruct",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:text",
          "size_text": "5.2GB",
          "size_bytes": 5583457484,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:v1.1",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:2b",
          "size_text": "1.7GB",
          "size_bytes": 1825361100,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:7b",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:2b-instruct",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:2b-instruct-q2_K",
          "size_text": "1.3GB",
          "size_bytes": 1395864371,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:2b-instruct-v1.1-q2_K",
          "size_text": "1.2GB",
          "size_bytes": 1288490188,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:2b-instruct-q3_K_S",
          "size_text": "1.4GB",
          "size_bytes": 1503238553,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:2b-instruct-v1.1-q3_K_S",
          "size_text": "1.3GB",
          "size_bytes": 1395864371,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:2b-instruct-q3_K_M",
          "size_text": "1.5GB",
          "size_bytes": 1610612736,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:2b-instruct-v1.1-q3_K_M",
          "size_text": "1.4GB",
          "size_bytes": 1503238553,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:2b-instruct-q3_K_L",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:2b-instruct-v1.1-q3_K_L",
          "size_text": "1.5GB",
          "size_bytes": 1610612736,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:2b-instruct-q4_0",
          "size_text": "1.7GB",
          "size_bytes": 1825361100,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:2b-instruct-v1.1-q4_0",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:2b-instruct-q4_1",
          "size_text": "1.8GB",
          "size_bytes": 1932735283,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:2b-instruct-v1.1-q4_1",
          "size_text": "1.7GB",
          "size_bytes": 1825361100,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:2b-instruct-q4_K_S",
          "size_text": "1.7GB",
          "size_bytes": 1825361100,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:2b-instruct-v1.1-q4_K_S",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:2b-instruct-q4_K_M",
          "size_text": "1.8GB",
          "size_bytes": 1932735283,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:2b-instruct-v1.1-q4_K_M",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:2b-instruct-q5_0",
          "size_text": "1.9GB",
          "size_bytes": 2040109465,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:2b-instruct-v1.1-q5_0",
          "size_text": "1.8GB",
          "size_bytes": 1932735283,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:2b-instruct-q5_1",
          "size_text": "2.1GB",
          "size_bytes": 2254857830,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:2b-instruct-v1.1-q5_1",
          "size_text": "1.9GB",
          "size_bytes": 2040109465,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:2b-instruct-q5_K_S",
          "size_text": "1.9GB",
          "size_bytes": 2040109465,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:2b-instruct-v1.1-q5_K_S",
          "size_text": "1.8GB",
          "size_bytes": 1932735283,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:2b-instruct-q5_K_M",
          "size_text": "2.0GB",
          "size_bytes": 2147483648,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:2b-instruct-v1.1-q5_K_M",
          "size_text": "1.8GB",
          "size_bytes": 1932735283,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:2b-instruct-q6_K",
          "size_text": "2.2GB",
          "size_bytes": 2362232012,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:2b-instruct-v1.1-q6_K",
          "size_text": "2.1GB",
          "size_bytes": 2254857830,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:2b-instruct-q8_0",
          "size_text": "2.7GB",
          "size_bytes": 2899102924,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:2b-instruct-v1.1-q8_0",
          "size_text": "2.7GB",
          "size_bytes": 2899102924,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:2b-instruct-fp16",
          "size_text": "4.5GB",
          "size_bytes": 4831838208,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:2b-instruct-v1.1-fp16",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:2b-text",
          "size_text": "1.7GB",
          "size_bytes": 1825361100,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:2b-text-q2_K",
          "size_text": "1.3GB",
          "size_bytes": 1395864371,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:2b-text-q3_K_S",
          "size_text": "1.4GB",
          "size_bytes": 1503238553,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:2b-text-q3_K_M",
          "size_text": "1.5GB",
          "size_bytes": 1610612736,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:2b-text-q3_K_L",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:2b-text-q4_0",
          "size_text": "1.7GB",
          "size_bytes": 1825361100,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:2b-text-q4_1",
          "size_text": "1.8GB",
          "size_bytes": 1932735283,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:2b-text-q4_K_S",
          "size_text": "1.7GB",
          "size_bytes": 1825361100,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:2b-text-q4_K_M",
          "size_text": "1.8GB",
          "size_bytes": 1932735283,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:2b-text-q5_0",
          "size_text": "1.9GB",
          "size_bytes": 2040109465,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:2b-text-q5_1",
          "size_text": "2.1GB",
          "size_bytes": 2254857830,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:2b-text-q5_K_S",
          "size_text": "1.9GB",
          "size_bytes": 2040109465,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:2b-text-q5_K_M",
          "size_text": "2.0GB",
          "size_bytes": 2147483648,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:2b-text-q6_K",
          "size_text": "2.2GB",
          "size_bytes": 2362232012,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:2b-text-q8_0",
          "size_text": "2.7GB",
          "size_bytes": 2899102924,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:2b-text-fp16",
          "size_text": "4.5GB",
          "size_bytes": 4831838208,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:2b-v1.1",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:7b-instruct",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:7b-instruct-q2_K",
          "size_text": "3.7GB",
          "size_bytes": 3972844748,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:7b-instruct-v1.1-q2_K",
          "size_text": "3.5GB",
          "size_bytes": 3758096384,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:7b-instruct-q3_K_S",
          "size_text": "4.2GB",
          "size_bytes": 4509715660,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:7b-instruct-v1.1-q3_K_S",
          "size_text": "4.0GB",
          "size_bytes": 4294967296,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:7b-instruct-q3_K_M",
          "size_text": "4.6GB",
          "size_bytes": 4939212390,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:7b-instruct-v1.1-q3_K_M",
          "size_text": "4.4GB",
          "size_bytes": 4724464025,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:7b-instruct-q3_K_L",
          "size_text": "4.9GB",
          "size_bytes": 5261334937,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:7b-instruct-v1.1-q3_K_L",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:7b-instruct-q4_0",
          "size_text": "5.2GB",
          "size_bytes": 5583457484,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:7b-instruct-v1.1-q4_0",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:7b-instruct-q4_1",
          "size_text": "5.7GB",
          "size_bytes": 6120328396,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:7b-instruct-v1.1-q4_1",
          "size_text": "5.5GB",
          "size_bytes": 5905580032,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:7b-instruct-q4_K_S",
          "size_text": "5.2GB",
          "size_bytes": 5583457484,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:7b-instruct-v1.1-q4_K_S",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:7b-instruct-q4_K_M",
          "size_text": "5.5GB",
          "size_bytes": 5905580032,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:7b-instruct-v1.1-q4_K_M",
          "size_text": "5.3GB",
          "size_bytes": 5690831667,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:7b-instruct-q5_0",
          "size_text": "6.2GB",
          "size_bytes": 6657199308,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:7b-instruct-v1.1-q5_0",
          "size_text": "6.0GB",
          "size_bytes": 6442450944,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:7b-instruct-q5_1",
          "size_text": "6.7GB",
          "size_bytes": 7194070220,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:7b-instruct-v1.1-q5_1",
          "size_text": "6.5GB",
          "size_bytes": 6979321856,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:7b-instruct-q5_K_S",
          "size_text": "6.2GB",
          "size_bytes": 6657199308,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:7b-instruct-v1.1-q5_K_S",
          "size_text": "6.0GB",
          "size_bytes": 6442450944,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:7b-instruct-q5_K_M",
          "size_text": "6.3GB",
          "size_bytes": 6764573491,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:7b-instruct-v1.1-q5_K_M",
          "size_text": "6.1GB",
          "size_bytes": 6549825126,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:7b-instruct-q6_K",
          "size_text": "7.2GB",
          "size_bytes": 7730941132,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:7b-instruct-v1.1-q6_K",
          "size_text": "7.0GB",
          "size_bytes": 7516192768,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:7b-instruct-q8_0",
          "size_text": "9.1GB",
          "size_bytes": 9771050598,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:7b-instruct-v1.1-q8_0",
          "size_text": "9.1GB",
          "size_bytes": 9771050598,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:7b-instruct-fp16",
          "size_text": "17GB",
          "size_bytes": 18253611008,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:7b-instruct-v1.1-fp16",
          "size_text": "17GB",
          "size_bytes": 18253611008,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:7b-text",
          "size_text": "5.2GB",
          "size_bytes": 5583457484,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:7b-text-q2_K",
          "size_text": "3.7GB",
          "size_bytes": 3972844748,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:7b-text-q3_K_S",
          "size_text": "4.2GB",
          "size_bytes": 4509715660,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:7b-text-q3_K_M",
          "size_text": "4.6GB",
          "size_bytes": 4939212390,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:7b-text-q3_K_L",
          "size_text": "4.9GB",
          "size_bytes": 5261334937,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:7b-text-q4_0",
          "size_text": "5.2GB",
          "size_bytes": 5583457484,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:7b-text-q4_1",
          "size_text": "5.7GB",
          "size_bytes": 6120328396,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:7b-text-q4_K_S",
          "size_text": "5.2GB",
          "size_bytes": 5583457484,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:7b-text-q4_K_M",
          "size_text": "5.5GB",
          "size_bytes": 5905580032,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:7b-text-q5_0",
          "size_text": "6.2GB",
          "size_bytes": 6657199308,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:7b-text-q5_1",
          "size_text": "6.7GB",
          "size_bytes": 7194070220,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:7b-text-q5_K_S",
          "size_text": "6.2GB",
          "size_bytes": 6657199308,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:7b-text-q5_K_M",
          "size_text": "6.3GB",
          "size_bytes": 6764573491,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:7b-text-q6_K",
          "size_text": "7.2GB",
          "size_bytes": 7730941132,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:7b-text-q8_0",
          "size_text": "9.1GB",
          "size_bytes": 9771050598,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:7b-text-fp16",
          "size_text": "16GB",
          "size_bytes": 17179869184,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "gemma:7b-v1.1",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "8K",
          "input": "Text"
        }
      ],
      "tags_count": 102
    },
    {
      "slug": "mxbai-embed-large",
      "capabilities": [
        "embedding"
      ],
      "pulls": 5200000,
      "pulls_text": "5.2M \n  Downloads",
      "blurb": "mxbai-embed-large State-of-the-art large model from mixedbread.ai 335m 4 Tags Updated 1 year ago",
      "name": "State-of-the-art large embedding model from mixedbread.ai",
      "description": "Readme mxbai-embed-large As of March 2024, this model archives SOTA performance for Bert-large sized models on the MTEB. It outperforms commercial models like OpenAIs text-embedding-3-large model and matches the performance of model 20x its size. mxbai-embed-large was trained with no overlap of the MTEB data, which indicates that the model generalizes well across several domains, tasks and text length. Usage REST API curl http://localhost:11434/api/embeddings -d '{ \"model\": \"mxbai-embed-large\", \"prompt\": \"Represent this sentence for searching relevant passages: The sky is blue because of Rayleigh scattering\" }' Python library ollama.embeddings(model='mxbai-embed-large', prompt='Represent this sentence for searching relevant passages: The sky is blue because of Rayleigh scattering') Javascript library ollama.embeddings({ model: 'mxbai-embed-large', prompt: 'Represent this sentence for searching relevant passages: The sky is blue because of Rayleigh scattering' }) References Blog post Hugging Face Write Preview ## mxbai-embed-large <img src=\"https://github.com/ollama/ollama/assets/251292/215cfb6a-8efa-4e9b-824d-e5f466b58c49\" widht=\"400\"> As of March 2024, this model archives SOTA performance for Bert-large sized models on the MTEB. It outperforms commercial models like OpenAIs `text-embedding-3-large` model and matches the performance of model 20x its size. `mxbai-embed-large` was trained with no overlap of the MTEB data, which indicates that the model generalizes well across several domains, tasks and text length. ## Usage ### REST API ``` curl http://localhost:11434/api/embeddings -d '{ \"model\": \"mxbai-embed-large\", \"prompt\": \"Represent this sentence for searching relevant passages: The sky is blue because of Rayleigh scattering\" }' ``` ### Python library ``` ollama.embeddings(model='mxbai-embed-large', prompt='Represent this sentence for searching relevant passages: The sky is blue because of Rayleigh scattering') ``` ### Javascript library ``` ollama.embeddings({ ",
      "variants": [
        {
          "tag": "mxbai-embed-large:latest",
          "size_text": "670MB",
          "size_bytes": 702545920,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "mxbai-embed-large:v1",
          "size_text": "670MB",
          "size_bytes": 702545920,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "mxbai-embed-large:335m",
          "size_text": "670MB",
          "size_bytes": 702545920,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "mxbai-embed-large:335m-v1-fp16",
          "size_text": "670MB",
          "size_bytes": 702545920,
          "context": null,
          "input": "Text"
        }
      ],
      "tags_count": 4
    },
    {
      "slug": "qwen",
      "capabilities": [
        "reasoning"
      ],
      "pulls": 5000000,
      "pulls_text": "5M \n  Downloads",
      "blurb": "qwen Qwen 1.5 is a series of large language models by Alibaba Cloud spanning from 0.5B to 110B parameters 0.5b 1.8b 4b 7b 14b 32b 72b 110b 379 Tags Updated 1 year ago",
      "name": "Qwen 1.5 is a series of large language models by Alibaba Cloud spanning from 0.5B to 110B parameters",
      "description": "Readme Qwen 2 is now available here . Qwen is a series of transformer-based large language models by Alibaba Cloud, pre-trained on a large volume of data, including web texts, books, code, etc. New in Qwen 1.5 6 model sizes, including 0.5B, 1.8B, 4B (default), 7B, 14B, 32B (new) and 72B ollama run qwen:0.5b ollama run qwen:1.8b ollama run qwen:4b ollama run qwen:7b ollama run qwen:14b ollama run qwen:32b ollama run qwen:72b ollama run qwen:110b Significant performance improvement in human preference for chat models Multilingual support of both base and chat models Stable support of 32K context length for models of all sizes The original Qwen model is offered in four different parameter sizes: 1.8B, 7B, 14B, and 72B. Features Low-cost deployment : the minimum memory requirement for inference is less than 2GB. Large-scale high-quality training corpora : Models are pre-trained on over 2.2 trillion tokens, including Chinese, English, multilingual texts, code, and mathematics, covering general and professional fields. The distribution of the pre-training corpus has been optimized through a large number of ablation experiments. Good performance : Qwen supports long context lengths (8K on the 1.8b , 7b and 14b parameter models, and 32K on the 72b parameter model), and significantly surpasses existing open-source models of similar scale on multiple Chinese and English downstream evaluation tasks (including common-sense, reasoning, code, mathematics, etc.), and even surpasses some larger-scale models in several benchmarks. More comprehensive vocabulary coverage : Compared with other open-source models based on Chinese and English vocabularies, Qwen uses a vocabulary of over 150K tokens. This vocabulary is more friendly to multiple languages, enabling users to directly further enhance the capability for certain languages without expanding the vocabulary. System prompt : Qwen can realize role playing, language style transfer, task setting, and behavior-setting by using a syste",
      "variants": [
        {
          "tag": "qwen:latest",
          "size_text": "2.3GB",
          "size_bytes": 2469606195,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:0.5b",
          "size_text": "395MB",
          "size_bytes": 414187520,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:1.8b",
          "size_text": "1.1GB",
          "size_bytes": 1181116006,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:4b",
          "size_text": "2.3GB",
          "size_bytes": 2469606195,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:7b",
          "size_text": "4.5GB",
          "size_bytes": 4831838208,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:14b",
          "size_text": "8.2GB",
          "size_bytes": 8804682956,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:32b",
          "size_text": "18GB",
          "size_bytes": 19327352832,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:72b",
          "size_text": "41GB",
          "size_bytes": 44023414784,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:110b",
          "size_text": "63GB",
          "size_bytes": 67645734912,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:0.5b-chat",
          "size_text": "395MB",
          "size_bytes": 414187520,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:0.5b-chat-v1.5-q2_K",
          "size_text": "298MB",
          "size_bytes": 312475648,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:0.5b-chat-v1.5-q3_K_S",
          "size_text": "333MB",
          "size_bytes": 349175808,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:0.5b-chat-v1.5-q3_K_M",
          "size_text": "350MB",
          "size_bytes": 367001600,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:0.5b-chat-v1.5-q3_K_L",
          "size_text": "364MB",
          "size_bytes": 381681664,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:0.5b-chat-v1.5-q4_0",
          "size_text": "395MB",
          "size_bytes": 414187520,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:0.5b-chat-v1.5-q4_1",
          "size_text": "424MB",
          "size_bytes": 444596224,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:0.5b-chat-v1.5-q4_K_S",
          "size_text": "397MB",
          "size_bytes": 416284672,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:0.5b-chat-v1.5-q4_K_M",
          "size_text": "407MB",
          "size_bytes": 426770432,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:0.5b-chat-v1.5-q5_0",
          "size_text": "453MB",
          "size_bytes": 475004928,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:0.5b-chat-v1.5-q5_1",
          "size_text": "482MB",
          "size_bytes": 505413632,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:0.5b-chat-v1.5-q5_K_S",
          "size_text": "453MB",
          "size_bytes": 475004928,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:0.5b-chat-v1.5-q5_K_M",
          "size_text": "459MB",
          "size_bytes": 481296384,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:0.5b-chat-v1.5-q6_K",
          "size_text": "515MB",
          "size_bytes": 540016640,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:0.5b-chat-v1.5-q8_0",
          "size_text": "665MB",
          "size_bytes": 697303040,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:0.5b-chat-v1.5-fp16",
          "size_text": "1.2GB",
          "size_bytes": 1288490188,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:0.5b-text",
          "size_text": "395MB",
          "size_bytes": 414187520,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:0.5b-text-v1.5-q2_K",
          "size_text": "298MB",
          "size_bytes": 312475648,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:0.5b-text-v1.5-q3_K_S",
          "size_text": "333MB",
          "size_bytes": 349175808,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:0.5b-text-v1.5-q3_K_M",
          "size_text": "350MB",
          "size_bytes": 367001600,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:0.5b-text-v1.5-q3_K_L",
          "size_text": "364MB",
          "size_bytes": 381681664,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:0.5b-text-v1.5-q4_0",
          "size_text": "395MB",
          "size_bytes": 414187520,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:0.5b-text-v1.5-q4_1",
          "size_text": "424MB",
          "size_bytes": 444596224,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:0.5b-text-v1.5-q4_K_S",
          "size_text": "397MB",
          "size_bytes": 416284672,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:0.5b-text-v1.5-q4_K_M",
          "size_text": "407MB",
          "size_bytes": 426770432,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:0.5b-text-v1.5-q5_0",
          "size_text": "453MB",
          "size_bytes": 475004928,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:0.5b-text-v1.5-q5_1",
          "size_text": "482MB",
          "size_bytes": 505413632,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:0.5b-text-v1.5-q5_K_S",
          "size_text": "453MB",
          "size_bytes": 475004928,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:0.5b-text-v1.5-q5_K_M",
          "size_text": "459MB",
          "size_bytes": 481296384,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:0.5b-text-v1.5-q6_K",
          "size_text": "515MB",
          "size_bytes": 540016640,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:0.5b-text-v1.5-q8_0",
          "size_text": "665MB",
          "size_bytes": 697303040,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:0.5b-text-v1.5-fp16",
          "size_text": "1.2GB",
          "size_bytes": 1288490188,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:1.8b-chat",
          "size_text": "1.1GB",
          "size_bytes": 1181116006,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:1.8b-chat-q2_K",
          "size_text": "853MB",
          "size_bytes": 894435328,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "qwen:1.8b-chat-v1.5-q2_K",
          "size_text": "863MB",
          "size_bytes": 904921088,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:1.8b-chat-q3_K_S",
          "size_text": "970MB",
          "size_bytes": 1017118720,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "qwen:1.8b-chat-v1.5-q3_K_S",
          "size_text": "970MB",
          "size_bytes": 1017118720,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:1.8b-chat-q3_K_M",
          "size_text": "1.0GB",
          "size_bytes": 1073741824,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "qwen:1.8b-chat-v1.5-q3_K_M",
          "size_text": "1.0GB",
          "size_bytes": 1073741824,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:1.8b-chat-q3_K_L",
          "size_text": "1.1GB",
          "size_bytes": 1181116006,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "qwen:1.8b-chat-v1.5-q3_K_L",
          "size_text": "1.1GB",
          "size_bytes": 1181116006,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:1.8b-chat-q4_0",
          "size_text": "1.1GB",
          "size_bytes": 1181116006,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "qwen:1.8b-chat-v1.5-q4_0",
          "size_text": "1.1GB",
          "size_bytes": 1181116006,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:1.8b-chat-q4_1",
          "size_text": "1.2GB",
          "size_bytes": 1288490188,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "qwen:1.8b-chat-v1.5-q4_1",
          "size_text": "1.2GB",
          "size_bytes": 1288490188,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:1.8b-chat-q4_K_S",
          "size_text": "1.2GB",
          "size_bytes": 1288490188,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "qwen:1.8b-chat-v1.5-q4_K_S",
          "size_text": "1.2GB",
          "size_bytes": 1288490188,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:1.8b-chat-q4_K_M",
          "size_text": "1.2GB",
          "size_bytes": 1288490188,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "qwen:1.8b-chat-v1.5-q4_K_M",
          "size_text": "1.2GB",
          "size_bytes": 1288490188,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:1.8b-chat-q5_0",
          "size_text": "1.3GB",
          "size_bytes": 1395864371,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "qwen:1.8b-chat-v1.5-q5_0",
          "size_text": "1.3GB",
          "size_bytes": 1395864371,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:1.8b-chat-q5_1",
          "size_text": "1.4GB",
          "size_bytes": 1503238553,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "qwen:1.8b-chat-v1.5-q5_1",
          "size_text": "1.4GB",
          "size_bytes": 1503238553,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:1.8b-chat-q5_K_S",
          "size_text": "1.3GB",
          "size_bytes": 1395864371,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "qwen:1.8b-chat-v1.5-q5_K_S",
          "size_text": "1.3GB",
          "size_bytes": 1395864371,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:1.8b-chat-q5_K_M",
          "size_text": "1.4GB",
          "size_bytes": 1503238553,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "qwen:1.8b-chat-v1.5-q5_K_M",
          "size_text": "1.4GB",
          "size_bytes": 1503238553,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:1.8b-chat-q6_K",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "qwen:1.8b-chat-v1.5-q6_K",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:1.8b-chat-q8_0",
          "size_text": "2.0GB",
          "size_bytes": 2147483648,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "qwen:1.8b-chat-v1.5-q8_0",
          "size_text": "2.0GB",
          "size_bytes": 2147483648,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:1.8b-chat-fp16",
          "size_text": "3.7GB",
          "size_bytes": 3972844748,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "qwen:1.8b-chat-v1.5-fp16",
          "size_text": "3.7GB",
          "size_bytes": 3972844748,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:1.8b-text",
          "size_text": "1.1GB",
          "size_bytes": 1181116006,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:1.8b-text-q2_K",
          "size_text": "853MB",
          "size_bytes": 894435328,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "qwen:1.8b-text-v1.5-q2_K",
          "size_text": "863MB",
          "size_bytes": 904921088,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:1.8b-text-q3_K_S",
          "size_text": "970MB",
          "size_bytes": 1017118720,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "qwen:1.8b-text-v1.5-q3_K_S",
          "size_text": "970MB",
          "size_bytes": 1017118720,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:1.8b-text-q3_K_M",
          "size_text": "1.0GB",
          "size_bytes": 1073741824,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "qwen:1.8b-text-v1.5-q3_K_M",
          "size_text": "1.0GB",
          "size_bytes": 1073741824,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:1.8b-text-q3_K_L",
          "size_text": "1.1GB",
          "size_bytes": 1181116006,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "qwen:1.8b-text-v1.5-q3_K_L",
          "size_text": "1.1GB",
          "size_bytes": 1181116006,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:1.8b-text-q4_0",
          "size_text": "1.1GB",
          "size_bytes": 1181116006,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "qwen:1.8b-text-v1.5-q4_0",
          "size_text": "1.1GB",
          "size_bytes": 1181116006,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:1.8b-text-q4_1",
          "size_text": "1.2GB",
          "size_bytes": 1288490188,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "qwen:1.8b-text-v1.5-q4_1",
          "size_text": "1.2GB",
          "size_bytes": 1288490188,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:1.8b-text-q4_K_S",
          "size_text": "1.2GB",
          "size_bytes": 1288490188,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "qwen:1.8b-text-v1.5-q4_K_S",
          "size_text": "1.2GB",
          "size_bytes": 1288490188,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:1.8b-text-q4_K_M",
          "size_text": "1.2GB",
          "size_bytes": 1288490188,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "qwen:1.8b-text-v1.5-q4_K_M",
          "size_text": "1.2GB",
          "size_bytes": 1288490188,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:1.8b-text-q5_0",
          "size_text": "1.3GB",
          "size_bytes": 1395864371,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "qwen:1.8b-text-v1.5-q5_0",
          "size_text": "1.3GB",
          "size_bytes": 1395864371,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:1.8b-text-q5_1",
          "size_text": "1.4GB",
          "size_bytes": 1503238553,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "qwen:1.8b-text-v1.5-q5_1",
          "size_text": "1.4GB",
          "size_bytes": 1503238553,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:1.8b-text-q5_K_S",
          "size_text": "1.3GB",
          "size_bytes": 1395864371,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "qwen:1.8b-text-v1.5-q5_K_S",
          "size_text": "1.3GB",
          "size_bytes": 1395864371,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:1.8b-text-q5_K_M",
          "size_text": "1.4GB",
          "size_bytes": 1503238553,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "qwen:1.8b-text-v1.5-q5_K_M",
          "size_text": "1.4GB",
          "size_bytes": 1503238553,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:1.8b-text-q6_K",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "qwen:1.8b-text-v1.5-q6_K",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:1.8b-text-q8_0",
          "size_text": "2.0GB",
          "size_bytes": 2147483648,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "qwen:1.8b-text-v1.5-q8_0",
          "size_text": "2.0GB",
          "size_bytes": 2147483648,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:1.8b-text-fp16",
          "size_text": "3.7GB",
          "size_bytes": 3972844748,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "qwen:1.8b-text-v1.5-fp16",
          "size_text": "3.7GB",
          "size_bytes": 3972844748,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:4b-chat",
          "size_text": "2.3GB",
          "size_bytes": 2469606195,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:4b-chat-v1.5-q2_K",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:4b-chat-v1.5-q3_K_S",
          "size_text": "1.9GB",
          "size_bytes": 2040109465,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:4b-chat-v1.5-q3_K_M",
          "size_text": "2.0GB",
          "size_bytes": 2147483648,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:4b-chat-v1.5-q3_K_L",
          "size_text": "2.2GB",
          "size_bytes": 2362232012,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:4b-chat-v1.5-q4_0",
          "size_text": "2.3GB",
          "size_bytes": 2469606195,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:4b-chat-v1.5-q4_1",
          "size_text": "2.6GB",
          "size_bytes": 2791728742,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:4b-chat-v1.5-q4_K_S",
          "size_text": "2.3GB",
          "size_bytes": 2469606195,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:4b-chat-v1.5-q4_K_M",
          "size_text": "2.5GB",
          "size_bytes": 2684354560,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:4b-chat-v1.5-q5_0",
          "size_text": "2.8GB",
          "size_bytes": 3006477107,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:4b-chat-v1.5-q5_1",
          "size_text": "3.0GB",
          "size_bytes": 3221225472,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:4b-chat-v1.5-q5_K_S",
          "size_text": "2.8GB",
          "size_bytes": 3006477107,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:4b-chat-v1.5-q5_K_M",
          "size_text": "2.8GB",
          "size_bytes": 3006477107,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:4b-chat-v1.5-q6_K",
          "size_text": "3.2GB",
          "size_bytes": 3435973836,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:4b-chat-v1.5-q8_0",
          "size_text": "4.2GB",
          "size_bytes": 4509715660,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:4b-chat-v1.5-fp16",
          "size_text": "7.9GB",
          "size_bytes": 8482560409,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:4b-text",
          "size_text": "2.3GB",
          "size_bytes": 2469606195,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:4b-text-v1.5-q2_K",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:4b-text-v1.5-q3_K_S",
          "size_text": "1.9GB",
          "size_bytes": 2040109465,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:4b-text-v1.5-q3_K_M",
          "size_text": "2.0GB",
          "size_bytes": 2147483648,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:4b-text-v1.5-q3_K_L",
          "size_text": "2.2GB",
          "size_bytes": 2362232012,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:4b-text-v1.5-q4_0",
          "size_text": "2.3GB",
          "size_bytes": 2469606195,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:4b-text-v1.5-q4_1",
          "size_text": "2.6GB",
          "size_bytes": 2791728742,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:4b-text-v1.5-q4_K_S",
          "size_text": "2.3GB",
          "size_bytes": 2469606195,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:4b-text-v1.5-q4_K_M",
          "size_text": "2.5GB",
          "size_bytes": 2684354560,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:4b-text-v1.5-q5_0",
          "size_text": "2.8GB",
          "size_bytes": 3006477107,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:4b-text-v1.5-q5_1",
          "size_text": "3.0GB",
          "size_bytes": 3221225472,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:4b-text-v1.5-q5_K_S",
          "size_text": "2.8GB",
          "size_bytes": 3006477107,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:4b-text-v1.5-q5_K_M",
          "size_text": "2.8GB",
          "size_bytes": 3006477107,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:4b-text-v1.5-q6_K",
          "size_text": "3.2GB",
          "size_bytes": 3435973836,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:4b-text-v1.5-q8_0",
          "size_text": "4.2GB",
          "size_bytes": 4509715660,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:4b-text-v1.5-fp16",
          "size_text": "7.9GB",
          "size_bytes": 8482560409,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:7b-chat",
          "size_text": "4.5GB",
          "size_bytes": 4831838208,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:7b-chat-q2_K",
          "size_text": "3.0GB",
          "size_bytes": 3221225472,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:7b-chat-v1.5-q2_K",
          "size_text": "3.1GB",
          "size_bytes": 3328599654,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:7b-chat-q3_K_S",
          "size_text": "3.6GB",
          "size_bytes": 3865470566,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:7b-chat-v1.5-q3_K_S",
          "size_text": "3.6GB",
          "size_bytes": 3865470566,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:7b-chat-q3_K_M",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:7b-chat-v1.5-q3_K_M",
          "size_text": "3.9GB",
          "size_bytes": 4187593113,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:7b-chat-q3_K_L",
          "size_text": "4.3GB",
          "size_bytes": 4617089843,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:7b-chat-v1.5-q3_K_L",
          "size_text": "4.2GB",
          "size_bytes": 4509715660,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:7b-chat-q4_0",
          "size_text": "4.5GB",
          "size_bytes": 4831838208,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:7b-chat-v1.5-q4_0",
          "size_text": "4.5GB",
          "size_bytes": 4831838208,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:7b-chat-q4_1",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:7b-chat-v1.5-q4_1",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:7b-chat-q4_K_S",
          "size_text": "4.5GB",
          "size_bytes": 4831838208,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:7b-chat-v1.5-q4_K_S",
          "size_text": "4.5GB",
          "size_bytes": 4831838208,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:7b-chat-q4_K_M",
          "size_text": "4.9GB",
          "size_bytes": 5261334937,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:7b-chat-v1.5-q4_K_M",
          "size_text": "4.8GB",
          "size_bytes": 5153960755,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:7b-chat-q5_0",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:7b-chat-v1.5-q5_0",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:7b-chat-q5_1",
          "size_text": "5.8GB",
          "size_bytes": 6227702579,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:7b-chat-v1.5-q5_1",
          "size_text": "5.8GB",
          "size_bytes": 6227702579,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:7b-chat-q5_K_S",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:7b-chat-v1.5-q5_K_S",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:7b-chat-q5_K_M",
          "size_text": "5.7GB",
          "size_bytes": 6120328396,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:7b-chat-v1.5-q5_K_M",
          "size_text": "5.5GB",
          "size_bytes": 5905580032,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:7b-chat-q6_K",
          "size_text": "6.3GB",
          "size_bytes": 6764573491,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:7b-chat-v1.5-q6_K",
          "size_text": "6.3GB",
          "size_bytes": 6764573491,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:7b-chat-q8_0",
          "size_text": "8.2GB",
          "size_bytes": 8804682956,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:7b-chat-v1.5-q8_0",
          "size_text": "8.2GB",
          "size_bytes": 8804682956,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:7b-chat-fp16",
          "size_text": "15GB",
          "size_bytes": 16106127360,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:7b-chat-v1.5-fp16",
          "size_text": "15GB",
          "size_bytes": 16106127360,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:7b-text",
          "size_text": "4.5GB",
          "size_bytes": 4831838208,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:7b-text-v1.5-q2_K",
          "size_text": "3.1GB",
          "size_bytes": 3328599654,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:7b-text-v1.5-q3_K_S",
          "size_text": "3.6GB",
          "size_bytes": 3865470566,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:7b-text-v1.5-q3_K_M",
          "size_text": "3.9GB",
          "size_bytes": 4187593113,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:7b-text-v1.5-q3_K_L",
          "size_text": "4.2GB",
          "size_bytes": 4509715660,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:7b-text-v1.5-q4_0",
          "size_text": "4.5GB",
          "size_bytes": 4831838208,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:7b-text-v1.5-q4_1",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:7b-text-v1.5-q4_K_S",
          "size_text": "4.5GB",
          "size_bytes": 4831838208,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:7b-text-v1.5-q4_K_M",
          "size_text": "4.8GB",
          "size_bytes": 5153960755,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:7b-text-v1.5-q5_0",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:7b-text-v1.5-q5_1",
          "size_text": "5.8GB",
          "size_bytes": 6227702579,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:7b-text-v1.5-q5_K_S",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:7b-text-v1.5-q5_K_M",
          "size_text": "5.5GB",
          "size_bytes": 5905580032,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:7b-text-v1.5-q6_K",
          "size_text": "6.3GB",
          "size_bytes": 6764573491,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:7b-text-v1.5-q8_0",
          "size_text": "8.2GB",
          "size_bytes": 8804682956,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:7b-text-v1.5-fp16",
          "size_text": "15GB",
          "size_bytes": 16106127360,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:7b-q2_K",
          "size_text": "3.0GB",
          "size_bytes": 3221225472,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:7b-q3_K_S",
          "size_text": "3.6GB",
          "size_bytes": 3865470566,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:7b-q3_K_M",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:7b-q3_K_L",
          "size_text": "4.3GB",
          "size_bytes": 4617089843,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:7b-q4_0",
          "size_text": "4.5GB",
          "size_bytes": 4831838208,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:7b-q4_1",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:7b-q4_K_S",
          "size_text": "4.5GB",
          "size_bytes": 4831838208,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:7b-q4_K_M",
          "size_text": "4.9GB",
          "size_bytes": 5261334937,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:7b-q5_0",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:7b-q5_1",
          "size_text": "5.8GB",
          "size_bytes": 6227702579,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:7b-q5_K_S",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:7b-q5_K_M",
          "size_text": "5.7GB",
          "size_bytes": 6120328396,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:7b-q6_K",
          "size_text": "6.3GB",
          "size_bytes": 6764573491,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:7b-q8_0",
          "size_text": "8.2GB",
          "size_bytes": 8804682956,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:7b-fp16",
          "size_text": "15GB",
          "size_bytes": 16106127360,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:14b-chat",
          "size_text": "8.2GB",
          "size_bytes": 8804682956,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:14b-chat-q2_K",
          "size_text": "6.0GB",
          "size_bytes": 6442450944,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "qwen:14b-chat-v1.5-q2_K",
          "size_text": "6.1GB",
          "size_bytes": 6549825126,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:14b-chat-q3_K_S",
          "size_text": "6.9GB",
          "size_bytes": 7408818585,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "qwen:14b-chat-v1.5-q3_K_S",
          "size_text": "6.9GB",
          "size_bytes": 7408818585,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:14b-chat-q3_K_M",
          "size_text": "7.7GB",
          "size_bytes": 8267812044,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "qwen:14b-chat-v1.5-q3_K_M",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:14b-chat-q3_K_L",
          "size_text": "8.0GB",
          "size_bytes": 8589934592,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "qwen:14b-chat-v1.5-q3_K_L",
          "size_text": "7.8GB",
          "size_bytes": 8375186227,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:14b-chat-q4_0",
          "size_text": "8.2GB",
          "size_bytes": 8804682956,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "qwen:14b-chat-v1.5-q4_0",
          "size_text": "8.2GB",
          "size_bytes": 8804682956,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:14b-chat-q4_1",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "qwen:14b-chat-v1.5-q4_1",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:14b-chat-q4_K_S",
          "size_text": "8.6GB",
          "size_bytes": 9234179686,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "qwen:14b-chat-v1.5-q4_K_S",
          "size_text": "8.6GB",
          "size_bytes": 9234179686,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:14b-chat-q4_K_M",
          "size_text": "9.4GB",
          "size_bytes": 10093173145,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "qwen:14b-chat-v1.5-q4_K_M",
          "size_text": "9.2GB",
          "size_bytes": 9878424780,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:14b-chat-q5_0",
          "size_text": "9.9GB",
          "size_bytes": 10630044057,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "qwen:14b-chat-v1.5-q5_0",
          "size_text": "9.9GB",
          "size_bytes": 10630044057,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:14b-chat-q5_1",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "qwen:14b-chat-v1.5-q5_1",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:14b-chat-q5_K_S",
          "size_text": "10GB",
          "size_bytes": 10737418240,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "qwen:14b-chat-v1.5-q5_K_S",
          "size_text": "10GB",
          "size_bytes": 10737418240,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:14b-chat-q5_K_M",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "qwen:14b-chat-v1.5-q5_K_M",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:14b-chat-q6_K",
          "size_text": "12GB",
          "size_bytes": 12884901888,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "qwen:14b-chat-v1.5-q6_K",
          "size_text": "12GB",
          "size_bytes": 12884901888,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:14b-chat-q8_0",
          "size_text": "15GB",
          "size_bytes": 16106127360,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "qwen:14b-chat-v1.5-q8_0",
          "size_text": "15GB",
          "size_bytes": 16106127360,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:14b-chat-fp16",
          "size_text": "28GB",
          "size_bytes": 30064771072,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "qwen:14b-chat-v1.5-fp16",
          "size_text": "28GB",
          "size_bytes": 30064771072,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:14b-text",
          "size_text": "8.2GB",
          "size_bytes": 8804682956,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:14b-text-q2_K",
          "size_text": "6.0GB",
          "size_bytes": 6442450944,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "qwen:14b-text-v1.5-q2_K",
          "size_text": "6.1GB",
          "size_bytes": 6549825126,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:14b-text-q3_K_S",
          "size_text": "6.9GB",
          "size_bytes": 7408818585,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "qwen:14b-text-v1.5-q3_K_S",
          "size_text": "6.9GB",
          "size_bytes": 7408818585,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:14b-text-q3_K_M",
          "size_text": "7.7GB",
          "size_bytes": 8267812044,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "qwen:14b-text-v1.5-q3_K_M",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:14b-text-q3_K_L",
          "size_text": "8.0GB",
          "size_bytes": 8589934592,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "qwen:14b-text-v1.5-q3_K_L",
          "size_text": "7.8GB",
          "size_bytes": 8375186227,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:14b-text-q4_0",
          "size_text": "8.2GB",
          "size_bytes": 8804682956,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "qwen:14b-text-v1.5-q4_0",
          "size_text": "8.2GB",
          "size_bytes": 8804682956,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:14b-text-q4_1",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "qwen:14b-text-v1.5-q4_1",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:14b-text-q4_K_S",
          "size_text": "8.6GB",
          "size_bytes": 9234179686,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "qwen:14b-text-v1.5-q4_K_S",
          "size_text": "8.6GB",
          "size_bytes": 9234179686,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:14b-text-q4_K_M",
          "size_text": "9.4GB",
          "size_bytes": 10093173145,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "qwen:14b-text-v1.5-q4_K_M",
          "size_text": "9.2GB",
          "size_bytes": 9878424780,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:14b-text-q5_0",
          "size_text": "9.9GB",
          "size_bytes": 10630044057,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "qwen:14b-text-v1.5-q5_0",
          "size_text": "9.9GB",
          "size_bytes": 10630044057,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:14b-text-q5_1",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "qwen:14b-text-v1.5-q5_1",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:14b-text-q5_K_S",
          "size_text": "10GB",
          "size_bytes": 10737418240,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "qwen:14b-text-v1.5-q5_K_S",
          "size_text": "10GB",
          "size_bytes": 10737418240,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:14b-text-q5_K_M",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "qwen:14b-text-v1.5-q5_K_M",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:14b-text-q6_K",
          "size_text": "12GB",
          "size_bytes": 12884901888,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "qwen:14b-text-v1.5-q6_K",
          "size_text": "12GB",
          "size_bytes": 12884901888,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:14b-text-q8_0",
          "size_text": "15GB",
          "size_bytes": 16106127360,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "qwen:14b-text-v1.5-q8_0",
          "size_text": "15GB",
          "size_bytes": 16106127360,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:14b-text-fp16",
          "size_text": "28GB",
          "size_bytes": 30064771072,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "qwen:14b-text-v1.5-fp16",
          "size_text": "28GB",
          "size_bytes": 30064771072,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:32b-chat",
          "size_text": "18GB",
          "size_bytes": 19327352832,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:32b-chat-v1.5-q2_K",
          "size_text": "12GB",
          "size_bytes": 12884901888,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:32b-chat-v1.5-q3_K_S",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:32b-chat-v1.5-q3_K_M",
          "size_text": "16GB",
          "size_bytes": 17179869184,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:32b-chat-v1.5-q3_K_L",
          "size_text": "17GB",
          "size_bytes": 18253611008,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:32b-chat-v1.5-q4_0",
          "size_text": "18GB",
          "size_bytes": 19327352832,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:32b-chat-v1.5-q4_1",
          "size_text": "20GB",
          "size_bytes": 21474836480,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:32b-chat-v1.5-q4_K_S",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:32b-chat-v1.5-q4_K_M",
          "size_text": "20GB",
          "size_bytes": 21474836480,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:32b-chat-v1.5-q5_0",
          "size_text": "22GB",
          "size_bytes": 23622320128,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:32b-chat-v1.5-q5_1",
          "size_text": "24GB",
          "size_bytes": 25769803776,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:32b-chat-v1.5-q5_K_S",
          "size_text": "22GB",
          "size_bytes": 23622320128,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:32b-chat-v1.5-q5_K_M",
          "size_text": "23GB",
          "size_bytes": 24696061952,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:32b-chat-v1.5-q6_K",
          "size_text": "27GB",
          "size_bytes": 28991029248,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:32b-chat-v1.5-q8_0",
          "size_text": "35GB",
          "size_bytes": 37580963840,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:32b-chat-v1.5-fp16",
          "size_text": "65GB",
          "size_bytes": 69793218560,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:32b-text",
          "size_text": "18GB",
          "size_bytes": 19327352832,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:32b-text-v1.5-q2_K",
          "size_text": "12GB",
          "size_bytes": 12884901888,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:32b-text-v1.5-q3_K_S",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:32b-text-v1.5-q3_K_M",
          "size_text": "16GB",
          "size_bytes": 17179869184,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:32b-text-v1.5-q3_K_L",
          "size_text": "17GB",
          "size_bytes": 18253611008,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:32b-text-v1.5-q4_0",
          "size_text": "18GB",
          "size_bytes": 19327352832,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:32b-text-v1.5-q4_1",
          "size_text": "20GB",
          "size_bytes": 21474836480,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:32b-text-v1.5-q4_K_S",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:32b-text-v1.5-q5_0",
          "size_text": "22GB",
          "size_bytes": 23622320128,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:32b-text-v1.5-q5_1",
          "size_text": "24GB",
          "size_bytes": 25769803776,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:32b-text-v1.5-q8_0",
          "size_text": "35GB",
          "size_bytes": 37580963840,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:72b-chat",
          "size_text": "41GB",
          "size_bytes": 44023414784,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:72b-chat-q2_K",
          "size_text": "27GB",
          "size_bytes": 28991029248,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:72b-chat-v1.5-q2_K",
          "size_text": "28GB",
          "size_bytes": 30064771072,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:72b-chat-q3_K_S",
          "size_text": "32GB",
          "size_bytes": 34359738368,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:72b-chat-v1.5-q3_K_S",
          "size_text": "33GB",
          "size_bytes": 35433480192,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:72b-chat-q3_K_M",
          "size_text": "37GB",
          "size_bytes": 39728447488,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:72b-chat-v1.5-q3_K_M",
          "size_text": "36GB",
          "size_bytes": 38654705664,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:72b-chat-q3_K_L",
          "size_text": "39GB",
          "size_bytes": 41875931136,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:72b-chat-v1.5-q3_K_L",
          "size_text": "38GB",
          "size_bytes": 40802189312,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:72b-chat-q4_0",
          "size_text": "41GB",
          "size_bytes": 44023414784,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:72b-chat-v1.5-q4_0",
          "size_text": "41GB",
          "size_bytes": 44023414784,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:72b-chat-q4_1",
          "size_text": "45GB",
          "size_bytes": 48318382080,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:72b-chat-v1.5-q4_1",
          "size_text": "45GB",
          "size_bytes": 48318382080,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:72b-chat-q4_K_S",
          "size_text": "41GB",
          "size_bytes": 44023414784,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:72b-chat-v1.5-q4_K_S",
          "size_text": "42GB",
          "size_bytes": 45097156608,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:72b-chat-q4_K_M",
          "size_text": "45GB",
          "size_bytes": 48318382080,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:72b-chat-v1.5-q4_K_M",
          "size_text": "44GB",
          "size_bytes": 47244640256,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:72b-chat-q5_0",
          "size_text": "50GB",
          "size_bytes": 53687091200,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:72b-chat-v1.5-q5_0",
          "size_text": "50GB",
          "size_bytes": 53687091200,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:72b-chat-q5_1",
          "size_text": "54GB",
          "size_bytes": 57982058496,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:72b-chat-v1.5-q5_1",
          "size_text": "54GB",
          "size_bytes": 57982058496,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:72b-chat-q5_K_S",
          "size_text": "50GB",
          "size_bytes": 53687091200,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:72b-chat-v1.5-q5_K_S",
          "size_text": "50GB",
          "size_bytes": 53687091200,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:72b-chat-q5_K_M",
          "size_text": "53GB",
          "size_bytes": 56908316672,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:72b-chat-v1.5-q5_K_M",
          "size_text": "51GB",
          "size_bytes": 54760833024,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:72b-chat-q6_K",
          "size_text": "59GB",
          "size_bytes": 63350767616,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:72b-chat-v1.5-q6_K",
          "size_text": "59GB",
          "size_bytes": 63350767616,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:72b-chat-q8_0",
          "size_text": "77GB",
          "size_bytes": 82678120448,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:72b-chat-v1.5-q8_0",
          "size_text": "77GB",
          "size_bytes": 82678120448,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:72b-chat-fp16",
          "size_text": "145GB",
          "size_bytes": 155692564480,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:72b-chat-v1.5-fp16",
          "size_text": "145GB",
          "size_bytes": 155692564480,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:72b-text",
          "size_text": "63GB",
          "size_bytes": 67645734912,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:72b-text-q2_K",
          "size_text": "27GB",
          "size_bytes": 28991029248,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:72b-text-v1.5-q2_K",
          "size_text": "28GB",
          "size_bytes": 30064771072,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:72b-text-q3_K_S",
          "size_text": "32GB",
          "size_bytes": 34359738368,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:72b-text-v1.5-q3_K_S",
          "size_text": "33GB",
          "size_bytes": 35433480192,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:72b-text-q3_K_M",
          "size_text": "37GB",
          "size_bytes": 39728447488,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:72b-text-v1.5-q3_K_M",
          "size_text": "36GB",
          "size_bytes": 38654705664,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:72b-text-q3_K_L",
          "size_text": "39GB",
          "size_bytes": 41875931136,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:72b-text-v1.5-q3_K_L",
          "size_text": "38GB",
          "size_bytes": 40802189312,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:72b-text-q4_0",
          "size_text": "41GB",
          "size_bytes": 44023414784,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:72b-text-v1.5-q4_0",
          "size_text": "41GB",
          "size_bytes": 44023414784,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:72b-text-q4_1",
          "size_text": "45GB",
          "size_bytes": 48318382080,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:72b-text-v1.5-q4_1",
          "size_text": "45GB",
          "size_bytes": 48318382080,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:72b-text-q4_K_S",
          "size_text": "41GB",
          "size_bytes": 44023414784,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:72b-text-v1.5-q4_K_S",
          "size_text": "42GB",
          "size_bytes": 45097156608,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:72b-text-q4_K_M",
          "size_text": "45GB",
          "size_bytes": 48318382080,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:72b-text-v1.5-q4_K_M",
          "size_text": "44GB",
          "size_bytes": 47244640256,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:72b-text-q5_0",
          "size_text": "50GB",
          "size_bytes": 53687091200,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:72b-text-v1.5-q5_0",
          "size_text": "50GB",
          "size_bytes": 53687091200,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:72b-text-q5_1",
          "size_text": "54GB",
          "size_bytes": 57982058496,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:72b-text-v1.5-q5_1",
          "size_text": "54GB",
          "size_bytes": 57982058496,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:72b-text-q5_K_S",
          "size_text": "50GB",
          "size_bytes": 53687091200,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:72b-text-v1.5-q5_K_S",
          "size_text": "50GB",
          "size_bytes": 53687091200,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:72b-text-q5_K_M",
          "size_text": "53GB",
          "size_bytes": 56908316672,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:72b-text-v1.5-q5_K_M",
          "size_text": "51GB",
          "size_bytes": 54760833024,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:72b-text-q6_K",
          "size_text": "59GB",
          "size_bytes": 63350767616,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:72b-text-v1.5-q6_K",
          "size_text": "59GB",
          "size_bytes": 63350767616,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:72b-text-q8_0",
          "size_text": "77GB",
          "size_bytes": 82678120448,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:72b-text-v1.5-q8_0",
          "size_text": "77GB",
          "size_bytes": 82678120448,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:72b-text-fp16",
          "size_text": "145GB",
          "size_bytes": 155692564480,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:72b-text-v1.5-fp16",
          "size_text": "145GB",
          "size_bytes": 155692564480,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:110b-chat",
          "size_text": "63GB",
          "size_bytes": 67645734912,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:110b-chat-v1.5-q2_K",
          "size_text": "41GB",
          "size_bytes": 44023414784,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:110b-chat-v1.5-q3_K_S",
          "size_text": "48GB",
          "size_bytes": 51539607552,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:110b-chat-v1.5-q3_K_M",
          "size_text": "54GB",
          "size_bytes": 57982058496,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:110b-chat-v1.5-q3_K_L",
          "size_text": "58GB",
          "size_bytes": 62277025792,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:110b-chat-v1.5-q4_0",
          "size_text": "63GB",
          "size_bytes": 67645734912,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:110b-chat-v1.5-q4_1",
          "size_text": "70GB",
          "size_bytes": 75161927680,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:110b-chat-v1.5-q4_K_S",
          "size_text": "63GB",
          "size_bytes": 67645734912,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:110b-chat-v1.5-q4_K_M",
          "size_text": "67GB",
          "size_bytes": 71940702208,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:110b-chat-v1.5-q5_0",
          "size_text": "77GB",
          "size_bytes": 82678120448,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:110b-chat-v1.5-q5_1",
          "size_text": "84GB",
          "size_bytes": 90194313216,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:110b-chat-v1.5-q5_K_S",
          "size_text": "77GB",
          "size_bytes": 82678120448,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:110b-chat-v1.5-q5_K_M",
          "size_text": "79GB",
          "size_bytes": 84825604096,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:110b-chat-v1.5-q6_K",
          "size_text": "91GB",
          "size_bytes": 97710505984,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:110b-chat-v1.5-q8_0",
          "size_text": "118GB",
          "size_bytes": 126701535232,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:110b-chat-v1.5-fp16",
          "size_text": "222GB",
          "size_bytes": 238370684928,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:110b-text-v1.5-q2_K",
          "size_text": "41GB",
          "size_bytes": 44023414784,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:110b-text-v1.5-q3_K_S",
          "size_text": "48GB",
          "size_bytes": 51539607552,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:110b-text-v1.5-q3_K_M",
          "size_text": "54GB",
          "size_bytes": 57982058496,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:110b-text-v1.5-q3_K_L",
          "size_text": "58GB",
          "size_bytes": 62277025792,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:110b-text-v1.5-q4_0",
          "size_text": "63GB",
          "size_bytes": 67645734912,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:110b-text-v1.5-q4_1",
          "size_text": "70GB",
          "size_bytes": 75161927680,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:110b-text-v1.5-q4_K_S",
          "size_text": "63GB",
          "size_bytes": 67645734912,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:110b-text-v1.5-q4_K_M",
          "size_text": "67GB",
          "size_bytes": 71940702208,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:110b-text-v1.5-q5_0",
          "size_text": "77GB",
          "size_bytes": 82678120448,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:110b-text-v1.5-q5_1",
          "size_text": "84GB",
          "size_bytes": 90194313216,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:110b-text-v1.5-q5_K_S",
          "size_text": "77GB",
          "size_bytes": 82678120448,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:110b-text-v1.5-q5_K_M",
          "size_text": "79GB",
          "size_bytes": 84825604096,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:110b-text-v1.5-q6_K",
          "size_text": "91GB",
          "size_bytes": 97710505984,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:110b-text-v1.5-q8_0",
          "size_text": "118GB",
          "size_bytes": 126701535232,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen:110b-text-v1.5-fp16",
          "size_text": "222GB",
          "size_bytes": 238370684928,
          "context": "32K",
          "input": "Text"
        }
      ],
      "tags_count": 379
    },
    {
      "slug": "llama2",
      "capabilities": [],
      "pulls": 4400000,
      "pulls_text": "4.4M \n  Downloads",
      "blurb": "llama2 Llama 2 is a collection of foundation language models ranging from 7B to 70B parameters. 7b 13b 70b 102 Tags Updated 1 year ago",
      "name": "Llama 2 is a collection of foundation language models ranging from 7B to 70B parameters.",
      "description": "Readme Llama 2 is released by Meta Platforms, Inc. This model is trained on 2 trillion tokens, and by default supports a context length of 4096. Llama 2 Chat models are fine-tuned on over 1 million human annotations, and are made for chat. CLI Open the terminal and run ollama run llama2 API Example using curl: curl -X POST http://localhost:11434/api/generate -d '{ \"model\": \"llama2\", \"prompt\":\"Why is the sky blue?\" }' API documentation Memory requirements 7b models generally require at least 8GB of RAM 13b models generally require at least 16GB of RAM 70b models generally require at least 64GB of RAM If you run into issues with higher quantization levels, try using the q4 model or shut down any other programs that are using a lot of memory. Model variants Chat is fine-tuned for chat/dialogue use cases. These are the default in Ollama, and for models tagged with -chat in the tags tab. Example: ollama run llama2 Pre-trained is without the chat fine-tuning. This is tagged as -text in the tags tab. Example: ollama run llama2:text By default, Ollama uses 4-bit quantization. To try other quantization levels, please try the other tags. The number after the q represents the number of bits used for quantization (i.e. q4 means 4-bit quantization). The higher the number, the more accurate the model is, but the slower it runs, and the more memory it requires. References Llama 2: Open Foundation and Fine-Tuned Chat Models Meta’s Hugging Face repo Write Preview <img src=\"https://github.com/jmorganca/ollama/assets/3325447/286e4e13-0b2b-43a4-9b07-23a50d3a3d33\" width=\"360\" /> Llama 2 is released by Meta Platforms, Inc. This model is trained on 2 trillion tokens, and by default supports a context length of 4096. Llama 2 Chat models are fine-tuned on over 1 million human annotations, and are made for chat. ### CLI Open the terminal and run `ollama run llama2` ### API Example using curl: ```bash curl -X POST http://localhost:11434/api/generate -d '{ \"model\": \"llama2\", \"prompt\":\"Why is t",
      "variants": [
        {
          "tag": "llama2:latest",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:chat",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:text",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:7b",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:13b",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:70b",
          "size_text": "39GB",
          "size_bytes": 41875931136,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:7b-chat",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:7b-chat-q2_K",
          "size_text": "2.8GB",
          "size_bytes": 3006477107,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:7b-chat-q3_K_S",
          "size_text": "2.9GB",
          "size_bytes": 3113851289,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:7b-chat-q3_K_M",
          "size_text": "3.3GB",
          "size_bytes": 3543348019,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:7b-chat-q3_K_L",
          "size_text": "3.6GB",
          "size_bytes": 3865470566,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:7b-chat-q4_0",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:7b-chat-q4_1",
          "size_text": "4.2GB",
          "size_bytes": 4509715660,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:7b-chat-q4_K_S",
          "size_text": "3.9GB",
          "size_bytes": 4187593113,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:7b-chat-q4_K_M",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:7b-chat-q5_0",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:7b-chat-q5_1",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:7b-chat-q5_K_S",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:7b-chat-q5_K_M",
          "size_text": "4.8GB",
          "size_bytes": 5153960755,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:7b-chat-q6_K",
          "size_text": "5.5GB",
          "size_bytes": 5905580032,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:7b-chat-q8_0",
          "size_text": "7.2GB",
          "size_bytes": 7730941132,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:7b-chat-fp16",
          "size_text": "13GB",
          "size_bytes": 13958643712,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:7b-text",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:7b-text-q2_K",
          "size_text": "2.8GB",
          "size_bytes": 3006477107,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:7b-text-q3_K_S",
          "size_text": "2.9GB",
          "size_bytes": 3113851289,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:7b-text-q3_K_M",
          "size_text": "3.3GB",
          "size_bytes": 3543348019,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:7b-text-q3_K_L",
          "size_text": "3.6GB",
          "size_bytes": 3865470566,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:7b-text-q4_0",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:7b-text-q4_1",
          "size_text": "4.2GB",
          "size_bytes": 4509715660,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:7b-text-q4_K_S",
          "size_text": "3.9GB",
          "size_bytes": 4187593113,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:7b-text-q4_K_M",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:7b-text-q5_0",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:7b-text-q5_1",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:7b-text-q5_K_S",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:7b-text-q5_K_M",
          "size_text": "4.8GB",
          "size_bytes": 5153960755,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:7b-text-q6_K",
          "size_text": "5.5GB",
          "size_bytes": 5905580032,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:7b-text-q8_0",
          "size_text": "7.2GB",
          "size_bytes": 7730941132,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:7b-text-fp16",
          "size_text": "13GB",
          "size_bytes": 13958643712,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:13b-chat",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:13b-chat-q2_K",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:13b-chat-q3_K_S",
          "size_text": "5.7GB",
          "size_bytes": 6120328396,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:13b-chat-q3_K_M",
          "size_text": "6.3GB",
          "size_bytes": 6764573491,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:13b-chat-q3_K_L",
          "size_text": "6.9GB",
          "size_bytes": 7408818585,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:13b-chat-q4_0",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:13b-chat-q4_1",
          "size_text": "8.2GB",
          "size_bytes": 8804682956,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:13b-chat-q4_K_S",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:13b-chat-q4_K_M",
          "size_text": "7.9GB",
          "size_bytes": 8482560409,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:13b-chat-q5_0",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:13b-chat-q5_1",
          "size_text": "9.8GB",
          "size_bytes": 10522669875,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:13b-chat-q5_K_S",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:13b-chat-q5_K_M",
          "size_text": "9.2GB",
          "size_bytes": 9878424780,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:13b-chat-q6_K",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:13b-chat-q8_0",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:13b-chat-fp16",
          "size_text": "26GB",
          "size_bytes": 27917287424,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:13b-text",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:13b-text-q2_K",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:13b-text-q3_K_S",
          "size_text": "5.7GB",
          "size_bytes": 6120328396,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:13b-text-q3_K_M",
          "size_text": "6.3GB",
          "size_bytes": 6764573491,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:13b-text-q3_K_L",
          "size_text": "6.9GB",
          "size_bytes": 7408818585,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:13b-text-q4_0",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:13b-text-q4_1",
          "size_text": "8.2GB",
          "size_bytes": 8804682956,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:13b-text-q4_K_S",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:13b-text-q4_K_M",
          "size_text": "7.9GB",
          "size_bytes": 8482560409,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:13b-text-q5_0",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:13b-text-q5_1",
          "size_text": "9.8GB",
          "size_bytes": 10522669875,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:13b-text-q5_K_S",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:13b-text-q5_K_M",
          "size_text": "9.2GB",
          "size_bytes": 9878424780,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:13b-text-q6_K",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:13b-text-q8_0",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:13b-text-fp16",
          "size_text": "26GB",
          "size_bytes": 27917287424,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:70b-chat",
          "size_text": "39GB",
          "size_bytes": 41875931136,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:70b-chat-q2_K",
          "size_text": "29GB",
          "size_bytes": 31138512896,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:70b-chat-q3_K_S",
          "size_text": "30GB",
          "size_bytes": 32212254720,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:70b-chat-q3_K_M",
          "size_text": "33GB",
          "size_bytes": 35433480192,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:70b-chat-q3_K_L",
          "size_text": "36GB",
          "size_bytes": 38654705664,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:70b-chat-q4_0",
          "size_text": "39GB",
          "size_bytes": 41875931136,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:70b-chat-q4_1",
          "size_text": "43GB",
          "size_bytes": 46170898432,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:70b-chat-q4_K_S",
          "size_text": "39GB",
          "size_bytes": 41875931136,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:70b-chat-q4_K_M",
          "size_text": "41GB",
          "size_bytes": 44023414784,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:70b-chat-q5_0",
          "size_text": "47GB",
          "size_bytes": 50465865728,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:70b-chat-q5_1",
          "size_text": "52GB",
          "size_bytes": 55834574848,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:70b-chat-q5_K_S",
          "size_text": "47GB",
          "size_bytes": 50465865728,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:70b-chat-q5_K_M",
          "size_text": "49GB",
          "size_bytes": 52613349376,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:70b-chat-q6_K",
          "size_text": "57GB",
          "size_bytes": 61203283968,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:70b-chat-q8_0",
          "size_text": "73GB",
          "size_bytes": 78383153152,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:70b-chat-fp16",
          "size_text": "138GB",
          "size_bytes": 148176371712,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:70b-text",
          "size_text": "39GB",
          "size_bytes": 41875931136,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:70b-text-q2_K",
          "size_text": "29GB",
          "size_bytes": 31138512896,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:70b-text-q3_K_S",
          "size_text": "30GB",
          "size_bytes": 32212254720,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:70b-text-q3_K_M",
          "size_text": "33GB",
          "size_bytes": 35433480192,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:70b-text-q3_K_L",
          "size_text": "36GB",
          "size_bytes": 38654705664,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:70b-text-q4_0",
          "size_text": "39GB",
          "size_bytes": 41875931136,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:70b-text-q4_1",
          "size_text": "43GB",
          "size_bytes": 46170898432,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:70b-text-q4_K_S",
          "size_text": "39GB",
          "size_bytes": 41875931136,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:70b-text-q4_K_M",
          "size_text": "41GB",
          "size_bytes": 44023414784,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:70b-text-q5_0",
          "size_text": "47GB",
          "size_bytes": 50465865728,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:70b-text-q5_1",
          "size_text": "52GB",
          "size_bytes": 55834574848,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:70b-text-q5_K_S",
          "size_text": "47GB",
          "size_bytes": 50465865728,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:70b-text-q5_K_M",
          "size_text": "49GB",
          "size_bytes": 52613349376,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:70b-text-q6_K",
          "size_text": "57GB",
          "size_bytes": 61203283968,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:70b-text-q8_0",
          "size_text": "73GB",
          "size_bytes": 78383153152,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2:70b-text-fp16",
          "size_text": "138GB",
          "size_bytes": 148176371712,
          "context": "4K",
          "input": "Text"
        }
      ],
      "tags_count": 102
    },
    {
      "slug": "qwen2",
      "capabilities": [
        "embedding",
        "tools"
      ],
      "pulls": 4400000,
      "pulls_text": "4.4M \n  Downloads",
      "blurb": "qwen2 Qwen2 is a new series of large language models from Alibaba group 0.5b 1.5b 7b 72b 97 Tags Updated 1 year ago",
      "name": "Qwen2 is a new series of large language models from Alibaba group",
      "description": "Readme Qwen2 is trained on data in 29 languages , including English and Chinese . It is available in 4 parameter sizes: 0.5B , 1.5B , 7B , 72B . In the 7B and 72B models, context length has been extended to 128k tokens . Models Qwen2-0.5B Qwen2-1.5B Qwen2-7B Qwen2-72B Params 0.49B 1.54B 7.07B 72.71B Non-Emb Params 0.35B 1.31B 5.98B 70.21B GQA True True True True Tie Embedding True True False False Context Length 32K 32K 128K 128K Supported languages This is in addition to English and Chinese Regions Languages Western Europe German, French, Spanish, Portuguese, Italian, Dutch Eastern & Central Europe Russian, Czech, Polish Middle East Arabic, Persian, Hebrew, Turkish Eastern Asia Japanese, Korean South-Eastern Asia Vietnamese, Thai, Indonesian, Malay, Lao, Burmese, Cebuano, Khmer, Tagalog Southern Asia Hindi, Bengali, Urdu Performance License All models with the exception of Qwen2 72B (both instruct and base models) are Apache 2.0 licensed. Qwen2 72B model still uses the original Qianwen License. Write Preview <img src=\"https://ollama.com/assets/library/qwen2/c011288c-fb07-42eb-b802-7287a5f12ea6\" width=\"320\" /> **Qwen2** is trained on data in **29 languages**, including **English and Chinese**. It is available in 4 parameter sizes: **0.5B**, **1.5B**, **7B**, **72B**. In the 7B and 72B models, context length has been extended to **128k tokens**. Models | Qwen2-0.5B | Qwen2-1.5B | Qwen2-7B | Qwen2-72B -- | -- | -- | -- | -- Params | 0.49B | 1.54B | 7.07B | 72.71B Non-Emb Params | 0.35B | 1.31B | 5.98B | 70.21B GQA | True | True | True | True Tie Embedding | True | True | False | False Context Length | 32K | 32K | 128K | 128K ### Supported languages This is in addition to English and Chinese Regions | Languages -- | -- Western Europe | German, French, Spanish, Portuguese, Italian, Dutch Eastern & Central Europe | Russian, Czech, Polish Middle East | Arabic, Persian, Hebrew, Turkish Eastern Asia | Japanese, Korean South-Eastern Asia | Vietnamese, Thai, Indonesian, Malay",
      "variants": [
        {
          "tag": "qwen2:latest",
          "size_text": "4.4GB",
          "size_bytes": 4724464025,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2:0.5b",
          "size_text": "352MB",
          "size_bytes": 369098752,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2:1.5b",
          "size_text": "935MB",
          "size_bytes": 980418560,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2:7b",
          "size_text": "4.4GB",
          "size_bytes": 4724464025,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2:72b",
          "size_text": "41GB",
          "size_bytes": 44023414784,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2:0.5b-instruct",
          "size_text": "352MB",
          "size_bytes": 369098752,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2:0.5b-instruct-q2_K",
          "size_text": "339MB",
          "size_bytes": 355467264,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2:0.5b-instruct-q3_K_S",
          "size_text": "338MB",
          "size_bytes": 354418688,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2:0.5b-instruct-q3_K_M",
          "size_text": "355MB",
          "size_bytes": 372244480,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2:0.5b-instruct-q3_K_L",
          "size_text": "369MB",
          "size_bytes": 386924544,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2:0.5b-instruct-q4_0",
          "size_text": "352MB",
          "size_bytes": 369098752,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2:0.5b-instruct-q4_1",
          "size_text": "375MB",
          "size_bytes": 393216000,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2:0.5b-instruct-q4_K_S",
          "size_text": "385MB",
          "size_bytes": 403701760,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2:0.5b-instruct-q4_K_M",
          "size_text": "398MB",
          "size_bytes": 417333248,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2:0.5b-instruct-q5_0",
          "size_text": "397MB",
          "size_bytes": 416284672,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2:0.5b-instruct-q5_1",
          "size_text": "419MB",
          "size_bytes": 439353344,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2:0.5b-instruct-q5_K_S",
          "size_text": "413MB",
          "size_bytes": 433061888,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2:0.5b-instruct-q5_K_M",
          "size_text": "420MB",
          "size_bytes": 440401920,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2:0.5b-instruct-q6_K",
          "size_text": "506MB",
          "size_bytes": 530579456,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2:0.5b-instruct-q8_0",
          "size_text": "531MB",
          "size_bytes": 556793856,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2:0.5b-instruct-fp16",
          "size_text": "994MB",
          "size_bytes": 1042284544,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2:1.5b-instruct",
          "size_text": "935MB",
          "size_bytes": 980418560,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2:1.5b-instruct-q2_K",
          "size_text": "676MB",
          "size_bytes": 708837376,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2:1.5b-instruct-q3_K_S",
          "size_text": "761MB",
          "size_bytes": 797966336,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2:1.5b-instruct-q3_K_M",
          "size_text": "824MB",
          "size_bytes": 864026624,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2:1.5b-instruct-q3_K_L",
          "size_text": "880MB",
          "size_bytes": 922746880,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2:1.5b-instruct-q4_0",
          "size_text": "935MB",
          "size_bytes": 980418560,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2:1.5b-instruct-q4_1",
          "size_text": "1.0GB",
          "size_bytes": 1073741824,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2:1.5b-instruct-q4_K_S",
          "size_text": "940MB",
          "size_bytes": 985661440,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2:1.5b-instruct-q4_K_M",
          "size_text": "986MB",
          "size_bytes": 1033895936,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2:1.5b-instruct-q5_0",
          "size_text": "1.1GB",
          "size_bytes": 1181116006,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2:1.5b-instruct-q5_1",
          "size_text": "1.2GB",
          "size_bytes": 1288490188,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2:1.5b-instruct-q5_K_S",
          "size_text": "1.1GB",
          "size_bytes": 1181116006,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2:1.5b-instruct-q5_K_M",
          "size_text": "1.1GB",
          "size_bytes": 1181116006,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2:1.5b-instruct-q6_K",
          "size_text": "1.3GB",
          "size_bytes": 1395864371,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2:1.5b-instruct-q8_0",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2:1.5b-instruct-fp16",
          "size_text": "3.1GB",
          "size_bytes": 3328599654,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2:7b-instruct",
          "size_text": "4.4GB",
          "size_bytes": 4724464025,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2:7b-instruct-q2_K",
          "size_text": "3.0GB",
          "size_bytes": 3221225472,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2:7b-instruct-q3_K_S",
          "size_text": "3.5GB",
          "size_bytes": 3758096384,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2:7b-instruct-q3_K_M",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2:7b-instruct-q3_K_L",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2:7b-instruct-q4_0",
          "size_text": "4.4GB",
          "size_bytes": 4724464025,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2:7b-instruct-q4_1",
          "size_text": "4.9GB",
          "size_bytes": 5261334937,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2:7b-instruct-q4_K_S",
          "size_text": "4.5GB",
          "size_bytes": 4831838208,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2:7b-instruct-q4_K_M",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2:7b-instruct-q5_0",
          "size_text": "5.3GB",
          "size_bytes": 5690831667,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2:7b-instruct-q5_1",
          "size_text": "5.8GB",
          "size_bytes": 6227702579,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2:7b-instruct-q5_K_S",
          "size_text": "5.3GB",
          "size_bytes": 5690831667,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2:7b-instruct-q5_K_M",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2:7b-instruct-q6_K",
          "size_text": "6.3GB",
          "size_bytes": 6764573491,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2:7b-instruct-q8_0",
          "size_text": "8.1GB",
          "size_bytes": 8697308774,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2:7b-instruct-fp16",
          "size_text": "15GB",
          "size_bytes": 16106127360,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2:7b-text",
          "size_text": "4.4GB",
          "size_bytes": 4724464025,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "qwen2:7b-text-q2_K",
          "size_text": "3.0GB",
          "size_bytes": 3221225472,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "qwen2:7b-text-q3_K_S",
          "size_text": "3.5GB",
          "size_bytes": 3758096384,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "qwen2:7b-text-q3_K_M",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "qwen2:7b-text-q3_K_L",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "qwen2:7b-text-q4_0",
          "size_text": "4.4GB",
          "size_bytes": 4724464025,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "qwen2:7b-text-q4_1",
          "size_text": "4.9GB",
          "size_bytes": 5261334937,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "qwen2:7b-text-q4_K_S",
          "size_text": "4.5GB",
          "size_bytes": 4831838208,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "qwen2:7b-text-q4_K_M",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "qwen2:7b-text-q5_0",
          "size_text": "5.3GB",
          "size_bytes": 5690831667,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "qwen2:7b-text-q5_1",
          "size_text": "5.8GB",
          "size_bytes": 6227702579,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "qwen2:7b-text-q8_0",
          "size_text": "8.1GB",
          "size_bytes": 8697308774,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "qwen2:72b-instruct",
          "size_text": "41GB",
          "size_bytes": 44023414784,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2:72b-instruct-q2_K",
          "size_text": "30GB",
          "size_bytes": 32212254720,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2:72b-instruct-q3_K_S",
          "size_text": "34GB",
          "size_bytes": 36507222016,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2:72b-instruct-q3_K_M",
          "size_text": "38GB",
          "size_bytes": 40802189312,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2:72b-instruct-q3_K_L",
          "size_text": "40GB",
          "size_bytes": 42949672960,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2:72b-instruct-q4_0",
          "size_text": "41GB",
          "size_bytes": 44023414784,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2:72b-instruct-q4_1",
          "size_text": "46GB",
          "size_bytes": 49392123904,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2:72b-instruct-q4_K_S",
          "size_text": "44GB",
          "size_bytes": 47244640256,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2:72b-instruct-q4_K_M",
          "size_text": "47GB",
          "size_bytes": 50465865728,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2:72b-instruct-q5_0",
          "size_text": "50GB",
          "size_bytes": 53687091200,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2:72b-instruct-q5_1",
          "size_text": "55GB",
          "size_bytes": 59055800320,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2:72b-instruct-q5_K_S",
          "size_text": "51GB",
          "size_bytes": 54760833024,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2:72b-instruct-q5_K_M",
          "size_text": "54GB",
          "size_bytes": 57982058496,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2:72b-instruct-q6_K",
          "size_text": "64GB",
          "size_bytes": 68719476736,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2:72b-instruct-q8_0",
          "size_text": "77GB",
          "size_bytes": 82678120448,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2:72b-instruct-fp16",
          "size_text": "145GB",
          "size_bytes": 155692564480,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2:72b-text",
          "size_text": "41GB",
          "size_bytes": 44023414784,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "qwen2:72b-text-q2_K",
          "size_text": "30GB",
          "size_bytes": 32212254720,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "qwen2:72b-text-q3_K_S",
          "size_text": "34GB",
          "size_bytes": 36507222016,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "qwen2:72b-text-q3_K_M",
          "size_text": "38GB",
          "size_bytes": 40802189312,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "qwen2:72b-text-q3_K_L",
          "size_text": "40GB",
          "size_bytes": 42949672960,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "qwen2:72b-text-q4_0",
          "size_text": "41GB",
          "size_bytes": 44023414784,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "qwen2:72b-text-q4_1",
          "size_text": "46GB",
          "size_bytes": 49392123904,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "qwen2:72b-text-q4_K_S",
          "size_text": "44GB",
          "size_bytes": 47244640256,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "qwen2:72b-text-q4_K_M",
          "size_text": "47GB",
          "size_bytes": 50465865728,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "qwen2:72b-text-q5_0",
          "size_text": "50GB",
          "size_bytes": 53687091200,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "qwen2:72b-text-q5_1",
          "size_text": "55GB",
          "size_bytes": 59055800320,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "qwen2:72b-text-q5_K_S",
          "size_text": "51GB",
          "size_bytes": 54760833024,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "qwen2:72b-text-q5_K_M",
          "size_text": "54GB",
          "size_bytes": 57982058496,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "qwen2:72b-text-q6_K",
          "size_text": "64GB",
          "size_bytes": 68719476736,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "qwen2:72b-text-q8_0",
          "size_text": "77GB",
          "size_bytes": 82678120448,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "qwen2:72b-text-fp16",
          "size_text": "145GB",
          "size_bytes": 155692564480,
          "context": "128K",
          "input": "Text"
        }
      ],
      "tags_count": 97
    },
    {
      "slug": "gpt-oss",
      "capabilities": [
        "reasoning",
        "thinking",
        "tools"
      ],
      "pulls": 3900000,
      "pulls_text": "3.9M \n  Downloads",
      "blurb": "gpt-oss OpenAI’s open-weight models designed for powerful reasoning, agentic tasks, and versatile developer use cases. cloud 20b 120b 5 Tags Updated 3 weeks ago",
      "name": "OpenAI’s open-weight models designed for powerful reasoning, agentic tasks, and versatile developer use cases.",
      "description": "Readme Welcome OpenAI’s gpt-oss! Ollama partners with OpenAI to bring its latest state-of-the-art open weight models to Ollama. The two models, 20B and 120B, bring a whole new local chat experience, and are designed for powerful reasoning, agentic tasks, and versatile developer use cases. Get started You can get started by downloading the latest Ollama version . The model can be downloaded directly in Ollama’s new app or via the terminal: ollama run gpt-oss:20b ollama run gpt-oss:120b Feature highlights Agentic capabilities: Use the models’ native capabilities for function calling, web browsing (Ollama is introducing built-in web search that can be optionally enabled), python tool calls, and structured outputs. Full chain-of-thought: Gain complete access to the model’s reasoning process, facilitating easier debugging and increased trust in outputs. Configurable reasoning effort: Easily adjust the reasoning effort (low, medium, high) based on your specific use case and latency needs. Fine-tunable: Fully customize models to your specific use case through parameter fine-tuning. Permissive Apache 2.0 license: Build freely without copyleft restrictions or patent risk—ideal for experimentation, customization, and commercial deployment. Quantization - MXFP4 format OpenAI utilizes quantization to reduce the memory footprint of the gpt-oss models. The models are post-trained with quantization of the mixture-of-experts (MoE) weights to MXFP4 format, where the weights are quantized to 4.25 bits per parameter. The MoE weights are responsible for 90+% of the total parameter count, and quantizing these to MXFP4 enables the smaller model to run on systems with as little as 16GB memory, and the larger model to fit on a single 80GB GPU. Ollama is supporting the MXFP4 format natively without additional quantizations or conversions. New kernels are developed for Ollama’s new engine to support the MXFP4 format. Ollama collaborated with OpenAI to benchmark against their reference implem",
      "variants": [
        {
          "tag": "gpt-oss:latest",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "gpt-oss:20b",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "gpt-oss:120b",
          "size_text": "65GB",
          "size_bytes": 69793218560,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "gpt-oss:20b-cloud",
          "size_text": null,
          "size_bytes": null,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "gpt-oss:120b-cloud",
          "size_text": null,
          "size_bytes": null,
          "context": "128K",
          "input": "Text"
        }
      ],
      "tags_count": 5
    },
    {
      "slug": "minicpm-v",
      "capabilities": [
        "multimodal",
        "reasoning",
        "vision"
      ],
      "pulls": 3900000,
      "pulls_text": "3.9M \n  Downloads",
      "blurb": "minicpm-v A series of LLMs (MLLMs) designed for -language understanding. 8b 17 Tags Updated 11 months ago",
      "name": "A series of multimodal LLMs (MLLMs) designed for vision-language understanding.",
      "description": "Readme Note: this model requires Ollama 0.3.10 or later. MiniCPM-V 2.6 is the latest and most capable model in the MiniCPM-V series. The model is built on SigLip-400M and Qwen2-7B with a total of 8B parameters. It exhibits a significant performance improvement over MiniCPM-Llama3-V 2.5, and introduces new features for multi-image and video understanding. Notable features of MiniCPM-V 2.6 include: 🔥 Leading Performance : MiniCPM-V 2.6 achieves an average score of 65.2 on the latest version of OpenCompass, a comprehensive evaluation over 8 popular benchmarks. With only 8B parameters, it surpasses widely used proprietary models like GPT-4o mini, GPT-4V, Gemini 1.5 Pro, and Claude 3.5 Sonnet for single image understanding. 🖼️ Multi Image Understanding and In-context Learning . MiniCPM-V 2.6 can also perform conversation and reasoning over multiple images. It achieves state-of-the-art performance on popular multi-image benchmarks such as Mantis-Eval, BLINK, Mathverse mv and Sciverse mv, and also shows promising in-context learning capability. 💪 Strong OCR Capability : MiniCPM-V 2.6 can process images with any aspect ratio and up to 1.8 million pixels (e.g., 1344x1344). It achieves state-of-the-art performance on OCRBench, surpassing proprietary models such as GPT-4o, GPT-4V, and Gemini 1.5 Pro. Based on the the latest RLAIF-V and VisCPM techniques, it features trustworthy behaviors, with significantly lower hallucination rates than GPT-4o and GPT-4V on Object HalBench, and supports multilingual capabilities on English, Chinese, German, French, Italian, Korean, etc. 🚀 Superior Efficiency : In addition to its friendly size, MiniCPM-V 2.6 also shows state-of-the-art token density (i.e., number of pixels encoded into each visual token). It produces only 640 tokens when processing a 1.8M pixel image, which is 75% fewer than most models. This directly improves the inference speed, first-token latency, memory usage, and power consumption. Refrences GitHub Hugging Face Write Pre",
      "variants": [
        {
          "tag": "minicpm-v:latest",
          "size_text": "5.5GB",
          "size_bytes": 5905580032,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "minicpm-v:8b",
          "size_text": "5.5GB",
          "size_bytes": 5905580032,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "minicpm-v:8b-2.6-q2_K",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "minicpm-v:8b-2.6-q3_K_S",
          "size_text": "4.5GB",
          "size_bytes": 4831838208,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "minicpm-v:8b-2.6-q3_K_M",
          "size_text": "4.9GB",
          "size_bytes": 5261334937,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "minicpm-v:8b-2.6-q3_K_L",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "minicpm-v:8b-2.6-q4_0",
          "size_text": "5.5GB",
          "size_bytes": 5905580032,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "minicpm-v:8b-2.6-q4_1",
          "size_text": "5.9GB",
          "size_bytes": 6335076761,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "minicpm-v:8b-2.6-q4_K_S",
          "size_text": "5.5GB",
          "size_bytes": 5905580032,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "minicpm-v:8b-2.6-q4_K_M",
          "size_text": "5.7GB",
          "size_bytes": 6120328396,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "minicpm-v:8b-2.6-q5_0",
          "size_text": "6.4GB",
          "size_bytes": 6871947673,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "minicpm-v:8b-2.6-q5_1",
          "size_text": "6.8GB",
          "size_bytes": 7301444403,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "minicpm-v:8b-2.6-q5_K_S",
          "size_text": "6.4GB",
          "size_bytes": 6871947673,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "minicpm-v:8b-2.6-q5_K_M",
          "size_text": "6.5GB",
          "size_bytes": 6979321856,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "minicpm-v:8b-2.6-q6_K",
          "size_text": "7.3GB",
          "size_bytes": 7838315315,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "minicpm-v:8b-2.6-q8_0",
          "size_text": "9.1GB",
          "size_bytes": 9771050598,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "minicpm-v:8b-2.6-fp16",
          "size_text": "16GB",
          "size_bytes": 17179869184,
          "context": "32K",
          "input": "Text"
        }
      ],
      "tags_count": 17
    },
    {
      "slug": "dolphin3",
      "capabilities": [],
      "pulls": 3400000,
      "pulls_text": "3.4M \n  Downloads",
      "blurb": "dolphin3 Dolphin 3.0 Llama 3.1 8B 🐬 is the next generation of the Dolphin series of instruct-tuned models designed to be the ultimate general purpose local model, enabling coding, math, agentic, function calling, and general use cases. 8b 5 Tags Updated 9 months ago",
      "name": "Dolphin 3.0 Llama 3.1 8B 🐬 is the next generation of the Dolphin series of instruct-tuned models designed to be the ultimate general purpose local model, enabling coding, math, agentic, function calling, and general use cases.",
      "description": "Readme Dolphin 3.0 is the next generation of the Dolphin series of instruct-tuned models. Designed to be the ultimate general purpose local model, enabling coding, math, agentic, function calling, and general use cases. Dolphin aims to be a general purpose model, similar to the models behind ChatGPT, Claude, Gemini. But these models present problems for businesses seeking to include AI in their products. They maintain control of the system prompt, deprecating and changing things as they wish, often causing software to break. They maintain control of the model versions, sometimes changing things silently, or deprecating older models that your business relies on. They maintain control of the alignment, and in particular the alignment is one-size-fits all, not tailored to the application. They can see all your queries and they can potentially use that data in ways you wouldn’t want. Dolphin, in contrast, is steerable and gives control to the system owner. You set the system prompt. You decide the alignment. You have control of your data. Dolphin does not impose its ethics or guidelines on you. You are the one who decides the guidelines. References Hugging Face Write Preview <img src=\"/assets/library/dolphin3/ce75cebe-b012-4195-9a6b-aa6c4b68f93f\" width=\"360\" /> Dolphin 3.0 is the next generation of the Dolphin series of instruct-tuned models. Designed to be the ultimate general purpose local model, enabling coding, math, agentic, function calling, and general use cases. Dolphin aims to be a general purpose model, similar to the models behind ChatGPT, Claude, Gemini. But these models present problems for businesses seeking to include AI in their products. 1. They maintain control of the system prompt, deprecating and changing things as they wish, often causing software to break. 2. They maintain control of the model versions, sometimes changing things silently, or deprecating older models that your business relies on. 3. They maintain control of the alignment, and in par",
      "variants": [
        {
          "tag": "dolphin3:latest",
          "size_text": "4.9GB",
          "size_bytes": 5261334937,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "dolphin3:8b",
          "size_text": "4.9GB",
          "size_bytes": 5261334937,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "dolphin3:8b-llama3.1-q4_K_M",
          "size_text": "4.9GB",
          "size_bytes": 5261334937,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "dolphin3:8b-llama3.1-q8_0",
          "size_text": "8.5GB",
          "size_bytes": 9126805504,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "dolphin3:8b-llama3.1-fp16",
          "size_text": "16GB",
          "size_bytes": 17179869184,
          "context": "128K",
          "input": "Text"
        }
      ],
      "tags_count": 5
    },
    {
      "slug": "codellama",
      "capabilities": [],
      "pulls": 3300000,
      "pulls_text": "3.3M \n  Downloads",
      "blurb": "codellama A large language model that can use text prompts to generate and discuss code. 7b 13b 34b 70b 199 Tags Updated 1 year ago",
      "name": "A large language model that can use text prompts to generate and discuss code.",
      "description": "Readme Code Llama is a model for generating and discussing code, built on top of Llama 2 . It’s designed to make workflows faster and efficient for developers and make it easier for people to learn how to code. It can generate both code and natural language about code. Code Llama supports many of the most popular programming languages used today, including Python, C++, Java, PHP, Typescript (Javascript), C#, Bash and more. Parameter counts Parameter Count 7 billion View ollama run codellama:7b 13 billion View ollama run codellama:13b 34 billion View ollama run codellama:34b 70 billion View ollama run codellama:70b Usage CLI ollama run codellama \"Write me a function that outputs the fibonacci sequence\" API curl -X POST http://localhost:11434/api/generate -d '{ \"model\": \"codellama\", \"prompt\": \"Write me a function that outputs the fibonacci sequence\" }' Variations instruct Fine-tuned to generate helpful and safe answers in natural language python A specialized variation of Code Llama further fine-tuned on 100B tokens of Python code code Base model for code completion Example prompts Ask questions ollama run codellama:7b-instruct 'You are an expert programmer that writes simple, concise code and explanations. Write a python function to generate the nth fibonacci number.' Fill-in-the-middle (FIM) or infill ollama run codellama:7b-code '<PRE> def compute_gcd(x, y): <SUF>return result <MID>' Fill-in-the-middle (FIM) is a special prompt format supported by the code completion model can complete code between two already written code blocks. Code Llama expects a specific format for infilling code: <PRE> {prefix} <SUF>{suffix} <MID> <PRE> , <SUF> and <MID> are special tokens that guide the model. Code review ollama run codellama ' Where is the bug in this code? def fib(n): if n <= 0: return n else: return fib(n-1) + fib(n-2) ' Writing tests ollama run codellama \"write a unit test for this function: $(cat example.py)\" Code completion ollama run codellama:7b-code '# A simple pyt",
      "variants": [
        {
          "tag": "codellama:latest",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:code",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:instruct",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:python",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:7b",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:13b",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:34b",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:70b",
          "size_text": "39GB",
          "size_bytes": 41875931136,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "codellama:7b-code",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:7b-code-q2_K",
          "size_text": "2.8GB",
          "size_bytes": 3006477107,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:7b-code-q3_K_S",
          "size_text": "2.9GB",
          "size_bytes": 3113851289,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:7b-code-q3_K_M",
          "size_text": "3.3GB",
          "size_bytes": 3543348019,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:7b-code-q3_K_L",
          "size_text": "3.6GB",
          "size_bytes": 3865470566,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:7b-code-q4_0",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:7b-code-q4_1",
          "size_text": "4.2GB",
          "size_bytes": 4509715660,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:7b-code-q4_K_S",
          "size_text": "3.9GB",
          "size_bytes": 4187593113,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:7b-code-q4_K_M",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:7b-code-q5_0",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:7b-code-q5_1",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:7b-code-q5_K_S",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:7b-code-q5_K_M",
          "size_text": "4.8GB",
          "size_bytes": 5153960755,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:7b-code-q6_K",
          "size_text": "5.5GB",
          "size_bytes": 5905580032,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:7b-code-q8_0",
          "size_text": "7.2GB",
          "size_bytes": 7730941132,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:7b-code-fp16",
          "size_text": "13GB",
          "size_bytes": 13958643712,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:7b-instruct",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:7b-instruct-q2_K",
          "size_text": "2.8GB",
          "size_bytes": 3006477107,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:7b-instruct-q3_K_S",
          "size_text": "2.9GB",
          "size_bytes": 3113851289,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:7b-instruct-q3_K_M",
          "size_text": "3.3GB",
          "size_bytes": 3543348019,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:7b-instruct-q3_K_L",
          "size_text": "3.6GB",
          "size_bytes": 3865470566,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:7b-instruct-q4_0",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:7b-instruct-q4_1",
          "size_text": "4.2GB",
          "size_bytes": 4509715660,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:7b-instruct-q4_K_S",
          "size_text": "3.9GB",
          "size_bytes": 4187593113,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:7b-instruct-q4_K_M",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:7b-instruct-q5_0",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:7b-instruct-q5_1",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:7b-instruct-q5_K_S",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:7b-instruct-q5_K_M",
          "size_text": "4.8GB",
          "size_bytes": 5153960755,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:7b-instruct-q6_K",
          "size_text": "5.5GB",
          "size_bytes": 5905580032,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:7b-instruct-q8_0",
          "size_text": "7.2GB",
          "size_bytes": 7730941132,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:7b-instruct-fp16",
          "size_text": "13GB",
          "size_bytes": 13958643712,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:7b-python",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:7b-python-q2_K",
          "size_text": "2.8GB",
          "size_bytes": 3006477107,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:7b-python-q3_K_S",
          "size_text": "2.9GB",
          "size_bytes": 3113851289,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:7b-python-q3_K_M",
          "size_text": "3.3GB",
          "size_bytes": 3543348019,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:7b-python-q3_K_L",
          "size_text": "3.6GB",
          "size_bytes": 3865470566,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:7b-python-q4_0",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:7b-python-q4_1",
          "size_text": "4.2GB",
          "size_bytes": 4509715660,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:7b-python-q4_K_S",
          "size_text": "3.9GB",
          "size_bytes": 4187593113,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:7b-python-q4_K_M",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:7b-python-q5_0",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:7b-python-q5_1",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:7b-python-q5_K_S",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:7b-python-q5_K_M",
          "size_text": "4.8GB",
          "size_bytes": 5153960755,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:7b-python-q6_K",
          "size_text": "5.5GB",
          "size_bytes": 5905580032,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:7b-python-q8_0",
          "size_text": "7.2GB",
          "size_bytes": 7730941132,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:7b-python-fp16",
          "size_text": "13GB",
          "size_bytes": 13958643712,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:13b-code",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:13b-code-q2_K",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:13b-code-q3_K_S",
          "size_text": "5.7GB",
          "size_bytes": 6120328396,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:13b-code-q3_K_M",
          "size_text": "6.3GB",
          "size_bytes": 6764573491,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:13b-code-q3_K_L",
          "size_text": "6.9GB",
          "size_bytes": 7408818585,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:13b-code-q4_0",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:13b-code-q4_1",
          "size_text": "8.2GB",
          "size_bytes": 8804682956,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:13b-code-q4_K_S",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:13b-code-q4_K_M",
          "size_text": "7.9GB",
          "size_bytes": 8482560409,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:13b-code-q5_0",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:13b-code-q5_1",
          "size_text": "9.8GB",
          "size_bytes": 10522669875,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:13b-code-q5_K_S",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:13b-code-q5_K_M",
          "size_text": "9.2GB",
          "size_bytes": 9878424780,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:13b-code-q6_K",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:13b-code-q8_0",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:13b-code-fp16",
          "size_text": "26GB",
          "size_bytes": 27917287424,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:13b-instruct",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:13b-instruct-q2_K",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:13b-instruct-q3_K_S",
          "size_text": "5.7GB",
          "size_bytes": 6120328396,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:13b-instruct-q3_K_M",
          "size_text": "6.3GB",
          "size_bytes": 6764573491,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:13b-instruct-q3_K_L",
          "size_text": "6.9GB",
          "size_bytes": 7408818585,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:13b-instruct-q4_0",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:13b-instruct-q4_1",
          "size_text": "8.2GB",
          "size_bytes": 8804682956,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:13b-instruct-q4_K_S",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:13b-instruct-q4_K_M",
          "size_text": "7.9GB",
          "size_bytes": 8482560409,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:13b-instruct-q5_0",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:13b-instruct-q5_1",
          "size_text": "9.8GB",
          "size_bytes": 10522669875,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:13b-instruct-q5_K_S",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:13b-instruct-q5_K_M",
          "size_text": "9.2GB",
          "size_bytes": 9878424780,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:13b-instruct-q6_K",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:13b-instruct-q8_0",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:13b-instruct-fp16",
          "size_text": "26GB",
          "size_bytes": 27917287424,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:13b-python",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:13b-python-q2_K",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:13b-python-q3_K_S",
          "size_text": "5.7GB",
          "size_bytes": 6120328396,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:13b-python-q3_K_M",
          "size_text": "6.3GB",
          "size_bytes": 6764573491,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:13b-python-q3_K_L",
          "size_text": "6.9GB",
          "size_bytes": 7408818585,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:13b-python-q4_0",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:13b-python-q4_1",
          "size_text": "8.2GB",
          "size_bytes": 8804682956,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:13b-python-q4_K_S",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:13b-python-q4_K_M",
          "size_text": "7.9GB",
          "size_bytes": 8482560409,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:13b-python-q5_0",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:13b-python-q5_1",
          "size_text": "9.8GB",
          "size_bytes": 10522669875,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:13b-python-q5_K_S",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:13b-python-q5_K_M",
          "size_text": "9.2GB",
          "size_bytes": 9878424780,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:13b-python-q6_K",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:13b-python-q8_0",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:13b-python-fp16",
          "size_text": "26GB",
          "size_bytes": 27917287424,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:34b-code",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:34b-code-q2_K",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:34b-code-q3_K_S",
          "size_text": "15GB",
          "size_bytes": 16106127360,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:34b-code-q3_K_M",
          "size_text": "16GB",
          "size_bytes": 17179869184,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:34b-code-q3_K_L",
          "size_text": "18GB",
          "size_bytes": 19327352832,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:34b-code-q4_0",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:34b-code-q4_1",
          "size_text": "21GB",
          "size_bytes": 22548578304,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:34b-code-q4_K_S",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:34b-code-q4_K_M",
          "size_text": "20GB",
          "size_bytes": 21474836480,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:34b-code-q5_0",
          "size_text": "23GB",
          "size_bytes": 24696061952,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:34b-code-q5_1",
          "size_text": "25GB",
          "size_bytes": 26843545600,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:34b-code-q5_K_S",
          "size_text": "23GB",
          "size_bytes": 24696061952,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:34b-code-q5_K_M",
          "size_text": "24GB",
          "size_bytes": 25769803776,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:34b-code-q6_K",
          "size_text": "28GB",
          "size_bytes": 30064771072,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:34b-code-q8_0",
          "size_text": "36GB",
          "size_bytes": 38654705664,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:34b-instruct",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:34b-instruct-q2_K",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:34b-instruct-q3_K_S",
          "size_text": "15GB",
          "size_bytes": 16106127360,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:34b-instruct-q3_K_M",
          "size_text": "16GB",
          "size_bytes": 17179869184,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:34b-instruct-q3_K_L",
          "size_text": "18GB",
          "size_bytes": 19327352832,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:34b-instruct-q4_0",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:34b-instruct-q4_1",
          "size_text": "21GB",
          "size_bytes": 22548578304,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:34b-instruct-q4_K_S",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:34b-instruct-q4_K_M",
          "size_text": "20GB",
          "size_bytes": 21474836480,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:34b-instruct-q5_0",
          "size_text": "23GB",
          "size_bytes": 24696061952,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:34b-instruct-q5_1",
          "size_text": "25GB",
          "size_bytes": 26843545600,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:34b-instruct-q5_K_S",
          "size_text": "23GB",
          "size_bytes": 24696061952,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:34b-instruct-q5_K_M",
          "size_text": "24GB",
          "size_bytes": 25769803776,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:34b-instruct-q6_K",
          "size_text": "28GB",
          "size_bytes": 30064771072,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:34b-instruct-q8_0",
          "size_text": "36GB",
          "size_bytes": 38654705664,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:34b-instruct-fp16",
          "size_text": "67GB",
          "size_bytes": 71940702208,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:34b-python",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:34b-python-q2_K",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:34b-python-q3_K_S",
          "size_text": "15GB",
          "size_bytes": 16106127360,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:34b-python-q3_K_M",
          "size_text": "16GB",
          "size_bytes": 17179869184,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:34b-python-q3_K_L",
          "size_text": "18GB",
          "size_bytes": 19327352832,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:34b-python-q4_0",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:34b-python-q4_1",
          "size_text": "21GB",
          "size_bytes": 22548578304,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:34b-python-q4_K_S",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:34b-python-q4_K_M",
          "size_text": "20GB",
          "size_bytes": 21474836480,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:34b-python-q5_0",
          "size_text": "23GB",
          "size_bytes": 24696061952,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:34b-python-q5_1",
          "size_text": "25GB",
          "size_bytes": 26843545600,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:34b-python-q5_K_S",
          "size_text": "23GB",
          "size_bytes": 24696061952,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:34b-python-q5_K_M",
          "size_text": "24GB",
          "size_bytes": 25769803776,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:34b-python-q6_K",
          "size_text": "28GB",
          "size_bytes": 30064771072,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:34b-python-q8_0",
          "size_text": "36GB",
          "size_bytes": 38654705664,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:34b-python-fp16",
          "size_text": "67GB",
          "size_bytes": 71940702208,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:70b-code",
          "size_text": "39GB",
          "size_bytes": 41875931136,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:70b-code-q2_K",
          "size_text": "25GB",
          "size_bytes": 26843545600,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:70b-code-q3_K_S",
          "size_text": "30GB",
          "size_bytes": 32212254720,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:70b-code-q3_K_M",
          "size_text": "33GB",
          "size_bytes": 35433480192,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:70b-code-q3_K_L",
          "size_text": "36GB",
          "size_bytes": 38654705664,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:70b-code-q4_0",
          "size_text": "39GB",
          "size_bytes": 41875931136,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:70b-code-q4_1",
          "size_text": "43GB",
          "size_bytes": 46170898432,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:70b-code-q4_K_S",
          "size_text": "39GB",
          "size_bytes": 41875931136,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:70b-code-q4_K_M",
          "size_text": "41GB",
          "size_bytes": 44023414784,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:70b-code-q5_0",
          "size_text": "47GB",
          "size_bytes": 50465865728,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:70b-code-q5_1",
          "size_text": "52GB",
          "size_bytes": 55834574848,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:70b-code-q5_K_S",
          "size_text": "47GB",
          "size_bytes": 50465865728,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:70b-code-q5_K_M",
          "size_text": "49GB",
          "size_bytes": 52613349376,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:70b-code-q6_K",
          "size_text": "57GB",
          "size_bytes": 61203283968,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:70b-code-q8_0",
          "size_text": "73GB",
          "size_bytes": 78383153152,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:70b-code-fp16",
          "size_text": "138GB",
          "size_bytes": 148176371712,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codellama:70b-instruct",
          "size_text": "39GB",
          "size_bytes": 41875931136,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "codellama:70b-instruct-q2_K",
          "size_text": "25GB",
          "size_bytes": 26843545600,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "codellama:70b-instruct-q3_K_S",
          "size_text": "30GB",
          "size_bytes": 32212254720,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "codellama:70b-instruct-q3_K_M",
          "size_text": "33GB",
          "size_bytes": 35433480192,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "codellama:70b-instruct-q3_K_L",
          "size_text": "36GB",
          "size_bytes": 38654705664,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "codellama:70b-instruct-q4_0",
          "size_text": "39GB",
          "size_bytes": 41875931136,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "codellama:70b-instruct-q4_1",
          "size_text": "43GB",
          "size_bytes": 46170898432,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "codellama:70b-instruct-q4_K_S",
          "size_text": "39GB",
          "size_bytes": 41875931136,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "codellama:70b-instruct-q4_K_M",
          "size_text": "41GB",
          "size_bytes": 44023414784,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "codellama:70b-instruct-q5_0",
          "size_text": "47GB",
          "size_bytes": 50465865728,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "codellama:70b-instruct-q5_1",
          "size_text": "52GB",
          "size_bytes": 55834574848,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "codellama:70b-instruct-q5_K_S",
          "size_text": "47GB",
          "size_bytes": 50465865728,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "codellama:70b-instruct-q5_K_M",
          "size_text": "49GB",
          "size_bytes": 52613349376,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "codellama:70b-instruct-q6_K",
          "size_text": "57GB",
          "size_bytes": 61203283968,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "codellama:70b-instruct-q8_0",
          "size_text": "73GB",
          "size_bytes": 78383153152,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "codellama:70b-instruct-fp16",
          "size_text": "138GB",
          "size_bytes": 148176371712,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "codellama:70b-python",
          "size_text": "39GB",
          "size_bytes": 41875931136,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "codellama:70b-python-q2_K",
          "size_text": "25GB",
          "size_bytes": 26843545600,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "codellama:70b-python-q3_K_S",
          "size_text": "30GB",
          "size_bytes": 32212254720,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "codellama:70b-python-q3_K_M",
          "size_text": "33GB",
          "size_bytes": 35433480192,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "codellama:70b-python-q3_K_L",
          "size_text": "36GB",
          "size_bytes": 38654705664,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "codellama:70b-python-q4_0",
          "size_text": "39GB",
          "size_bytes": 41875931136,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "codellama:70b-python-q4_1",
          "size_text": "43GB",
          "size_bytes": 46170898432,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "codellama:70b-python-q4_K_S",
          "size_text": "39GB",
          "size_bytes": 41875931136,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "codellama:70b-python-q4_K_M",
          "size_text": "41GB",
          "size_bytes": 44023414784,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "codellama:70b-python-q5_0",
          "size_text": "47GB",
          "size_bytes": 50465865728,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "codellama:70b-python-q5_1",
          "size_text": "52GB",
          "size_bytes": 55834574848,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "codellama:70b-python-q5_K_S",
          "size_text": "47GB",
          "size_bytes": 50465865728,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "codellama:70b-python-q5_K_M",
          "size_text": "49GB",
          "size_bytes": 52613349376,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "codellama:70b-python-q6_K",
          "size_text": "57GB",
          "size_bytes": 61203283968,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "codellama:70b-python-q8_0",
          "size_text": "73GB",
          "size_bytes": 78383153152,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "codellama:70b-python-fp16",
          "size_text": "138GB",
          "size_bytes": 148176371712,
          "context": "4K",
          "input": "Text"
        }
      ],
      "tags_count": 199
    },
    {
      "slug": "olmo2",
      "capabilities": [],
      "pulls": 3300000,
      "pulls_text": "3.3M \n  Downloads",
      "blurb": "olmo2 OLMo 2 is a new family of 7B and 13B models trained on up to 5T tokens. These models are on par with or better than equivalently sized fully open models, and competitive with open-weight models such as Llama 3.1 on English academic benchmarks. 7b 13b 9 Tags Updated 9 months ago",
      "name": "OLMo 2 is a new family of 7B and 13B models trained on up to 5T tokens. These models are on par with or better than equivalently sized fully open models, and competitive with open-weight models such as Llama 3.1 on English academic benchmarks.",
      "description": "Readme Note: this model requires Ollama 0.5.5 OLMo 2 is a new family of 7B and 13B models trained on up to 5T tokens. These models are on par with or better than equivalently sized fully open models, and competitive with open-weight models such as Llama 3.1 on English academic benchmarks. References Blog post Write Preview > Note: this model requires [Ollama 0.5.5](https://github.com/ollama/ollama/releases/tag/v0.5.5) ![1732650119-wide-4x.webp](/assets/library/olmo2/71e694b3-a4fe-4bd1-8338-684506f85e8d) OLMo 2 is a new family of 7B and 13B models trained on up to 5T tokens. These models are on par with or better than equivalently sized fully open models, and competitive with open-weight models such as Llama 3.1 on English academic benchmarks. ## References [Blog post](https://allenai.org/blog/olmo2) Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "olmo2:latest",
          "size_text": "4.5GB",
          "size_bytes": 4831838208,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "olmo2:7b",
          "size_text": "4.5GB",
          "size_bytes": 4831838208,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "olmo2:13b",
          "size_text": "8.4GB",
          "size_bytes": 9019431321,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "olmo2:7b-1124-instruct-q4_K_M",
          "size_text": "4.5GB",
          "size_bytes": 4831838208,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "olmo2:7b-1124-instruct-q8_0",
          "size_text": "7.8GB",
          "size_bytes": 8375186227,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "olmo2:7b-1124-instruct-fp16",
          "size_text": "15GB",
          "size_bytes": 16106127360,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "olmo2:13b-1124-instruct-q4_K_M",
          "size_text": "8.4GB",
          "size_bytes": 9019431321,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "olmo2:13b-1124-instruct-q8_0",
          "size_text": "15GB",
          "size_bytes": 16106127360,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "olmo2:13b-1124-instruct-fp16",
          "size_text": "27GB",
          "size_bytes": 28991029248,
          "context": "4K",
          "input": "Text"
        }
      ],
      "tags_count": 9
    },
    {
      "slug": "tinyllama",
      "capabilities": [],
      "pulls": 3100000,
      "pulls_text": "3.1M \n  Downloads",
      "blurb": "tinyllama The TinyLlama project is an open endeavor to train a compact 1.1B Llama model on 3 trillion tokens. 1.1b 36 Tags Updated 1 year ago",
      "name": "The TinyLlama project is an open endeavor to train a compact 1.1B Llama model on 3 trillion tokens.",
      "description": "Readme TinyLlama is a compact model with only 1.1B parameters. This compactness allows it to cater to a multitude of applications demanding a restricted computation and memory footprint. References Hugging Face GitHub Write Preview <img src=\"https://github.com/jmorganca/ollama/assets/251292/6f17d445-d019-4e54-bac8-9cb6b3b01a26\" width=\"320\" /> TinyLlama is a compact model with only 1.1B parameters. This compactness allows it to cater to a multitude of applications demanding a restricted computation and memory footprint. ## References [Hugging Face](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v0.6) [GitHub](https://github.com/jzhang38/TinyLlama) Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "tinyllama:latest",
          "size_text": "638MB",
          "size_bytes": 668991488,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "tinyllama:chat",
          "size_text": "638MB",
          "size_bytes": 668991488,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "tinyllama:v0.6",
          "size_text": "638MB",
          "size_bytes": 668991488,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "tinyllama:v1",
          "size_text": "638MB",
          "size_bytes": 668991488,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "tinyllama:1.1b",
          "size_text": "638MB",
          "size_bytes": 668991488,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "tinyllama:1.1b-chat",
          "size_text": "638MB",
          "size_bytes": 668991488,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "tinyllama:1.1b-chat-v0.6-q2_K",
          "size_text": "483MB",
          "size_bytes": 506462208,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "tinyllama:1.1b-chat-v0.6-q3_K_S",
          "size_text": "500MB",
          "size_bytes": 524288000,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "tinyllama:1.1b-chat-v0.6-q3_K_M",
          "size_text": "551MB",
          "size_bytes": 577765376,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "tinyllama:1.1b-chat-v0.6-q3_K_L",
          "size_text": "593MB",
          "size_bytes": 621805568,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "tinyllama:1.1b-chat-v0.6-q4_0",
          "size_text": "638MB",
          "size_bytes": 668991488,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "tinyllama:1.1b-chat-v0.6-q4_1",
          "size_text": "702MB",
          "size_bytes": 736100352,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "tinyllama:1.1b-chat-v0.6-q4_K_S",
          "size_text": "644MB",
          "size_bytes": 675282944,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "tinyllama:1.1b-chat-v0.6-q4_K_M",
          "size_text": "669MB",
          "size_bytes": 701497344,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "tinyllama:1.1b-chat-v0.6-q5_0",
          "size_text": "767MB",
          "size_bytes": 804257792,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "tinyllama:1.1b-chat-v0.6-q5_1",
          "size_text": "832MB",
          "size_bytes": 872415232,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "tinyllama:1.1b-chat-v0.6-q5_K_S",
          "size_text": "767MB",
          "size_bytes": 804257792,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "tinyllama:1.1b-chat-v0.6-q5_K_M",
          "size_text": "783MB",
          "size_bytes": 821035008,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "tinyllama:1.1b-chat-v0.6-q6_K",
          "size_text": "904MB",
          "size_bytes": 947912704,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "tinyllama:1.1b-chat-v0.6-q8_0",
          "size_text": "1.2GB",
          "size_bytes": 1288490188,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "tinyllama:1.1b-chat-v0.6-fp16",
          "size_text": "2.2GB",
          "size_bytes": 2362232012,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "tinyllama:1.1b-chat-v1-q2_K",
          "size_text": "483MB",
          "size_bytes": 506462208,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "tinyllama:1.1b-chat-v1-q3_K_S",
          "size_text": "500MB",
          "size_bytes": 524288000,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "tinyllama:1.1b-chat-v1-q3_K_M",
          "size_text": "551MB",
          "size_bytes": 577765376,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "tinyllama:1.1b-chat-v1-q3_K_L",
          "size_text": "593MB",
          "size_bytes": 621805568,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "tinyllama:1.1b-chat-v1-q4_0",
          "size_text": "638MB",
          "size_bytes": 668991488,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "tinyllama:1.1b-chat-v1-q4_1",
          "size_text": "702MB",
          "size_bytes": 736100352,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "tinyllama:1.1b-chat-v1-q4_K_S",
          "size_text": "644MB",
          "size_bytes": 675282944,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "tinyllama:1.1b-chat-v1-q4_K_M",
          "size_text": "669MB",
          "size_bytes": 701497344,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "tinyllama:1.1b-chat-v1-q5_0",
          "size_text": "767MB",
          "size_bytes": 804257792,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "tinyllama:1.1b-chat-v1-q5_1",
          "size_text": "832MB",
          "size_bytes": 872415232,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "tinyllama:1.1b-chat-v1-q5_K_S",
          "size_text": "767MB",
          "size_bytes": 804257792,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "tinyllama:1.1b-chat-v1-q5_K_M",
          "size_text": "783MB",
          "size_bytes": 821035008,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "tinyllama:1.1b-chat-v1-q6_K",
          "size_text": "904MB",
          "size_bytes": 947912704,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "tinyllama:1.1b-chat-v1-q8_0",
          "size_text": "1.2GB",
          "size_bytes": 1288490188,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "tinyllama:1.1b-chat-v1-fp16",
          "size_text": "2.2GB",
          "size_bytes": 2362232012,
          "context": "2K",
          "input": "Text"
        }
      ],
      "tags_count": 36
    },
    {
      "slug": "llama3.2-vision",
      "capabilities": [
        "multimodal",
        "reasoning",
        "vision"
      ],
      "pulls": 2800000,
      "pulls_text": "2.8M \n  Downloads",
      "blurb": "llama3.2- Llama 3.2 is a collection of instruction-tuned image generative models in 11B and 90B sizes. 11b 90b 9 Tags Updated 5 months ago",
      "name": "Llama 3.2 Vision is a collection of instruction-tuned image reasoning generative models in 11B and 90B sizes.",
      "description": "Readme The Llama 3.2-Vision collection of multimodal large language models (LLMs) is a collection of instruction-tuned image reasoning generative models in 11B and 90B sizes (text + images in / text out). The Llama 3.2-Vision instruction-tuned models are optimized for visual recognition, image reasoning, captioning, and answering general questions about an image. The models outperform many of the available open source and closed multimodal models on common industry benchmarks. Supported Languages: For text only tasks, English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai are officially supported. Llama 3.2 has been trained on a broader collection of languages than these 8 supported languages. Note for image+text applications, English is the only language supported. Usage First, pull the model: ollama pull llama3.2-vision Python Library To use Llama 3.2 Vision with the Ollama Python library : import ollama response = ollama.chat( model='llama3.2-vision', messages=[{ 'role': 'user', 'content': 'What is in this image?', 'images': ['image.jpg'] }] ) print(response) JavaScript Library To use Llama 3.2 Vision with the Ollama JavaScript library : import ollama from 'ollama' const response = await ollama.chat({ model: 'llama3.2-vision', messages: [{ role: 'user', content: 'What is in this image?', images: ['image.jpg'] }] }) console.log(response) cURL curl http://localhost:11434/api/chat -d '{ \"model\": \"llama3.2-vision\", \"messages\": [ { \"role\": \"user\", \"content\": \"what is in this image?\", \"images\": [\"<base64-encoded image data>\"] } ] }' References GitHub HuggingFace Write Preview <img src=\"/assets/library/llama3.2-vision/ea1fa75c-0d15-453d-a291-ce2d97d8646a\" width=\"280\" /> The Llama 3.2-Vision collection of multimodal large language models (LLMs) is a collection of instruction-tuned image reasoning generative models in 11B and 90B sizes (text + images in / text out). The Llama 3.2-Vision instruction-tuned models are optimized for visual recognition, image r",
      "variants": [
        {
          "tag": "llama3.2-vision:latest",
          "size_text": "7.8GB",
          "size_bytes": 8375186227,
          "context": "128K",
          "input": "Vision"
        },
        {
          "tag": "llama3.2-vision:11b",
          "size_text": "7.8GB",
          "size_bytes": 8375186227,
          "context": "128K",
          "input": "Vision"
        },
        {
          "tag": "llama3.2-vision:90b",
          "size_text": "55GB",
          "size_bytes": 59055800320,
          "context": "128K",
          "input": "Vision"
        },
        {
          "tag": "llama3.2-vision:11b-instruct-q4_K_M",
          "size_text": "7.8GB",
          "size_bytes": 8375186227,
          "context": "128K",
          "input": "Vision"
        },
        {
          "tag": "llama3.2-vision:11b-instruct-q8_0",
          "size_text": "12GB",
          "size_bytes": 12884901888,
          "context": "128K",
          "input": "Vision"
        },
        {
          "tag": "llama3.2-vision:11b-instruct-fp16",
          "size_text": "21GB",
          "size_bytes": 22548578304,
          "context": "128K",
          "input": "Vision"
        },
        {
          "tag": "llama3.2-vision:90b-instruct-q4_K_M",
          "size_text": "55GB",
          "size_bytes": 59055800320,
          "context": "128K",
          "input": "Vision"
        },
        {
          "tag": "llama3.2-vision:90b-instruct-q8_0",
          "size_text": "95GB",
          "size_bytes": 102005473280,
          "context": "128K",
          "input": "Vision"
        },
        {
          "tag": "llama3.2-vision:90b-instruct-fp16",
          "size_text": "177GB",
          "size_bytes": 190052302848,
          "context": "128K",
          "input": "Vision"
        }
      ],
      "tags_count": 9
    },
    {
      "slug": "mistral-nemo",
      "capabilities": [
        "reasoning",
        "tools"
      ],
      "pulls": 2800000,
      "pulls_text": "2.8M \n  Downloads",
      "blurb": "mistral-nemo A state-of-the-art 12B model with 128k context length, built by Mistral AI in collaboration with NVIDIA. 12b 17 Tags Updated 3 months ago",
      "name": "A state-of-the-art 12B model with 128k context length, built by Mistral AI in collaboration with NVIDIA.",
      "description": "Readme Mistral NeMo is a 12B model built in collaboration with NVIDIA. Mistral NeMo offers a large context window of up to 128k tokens. Its reasoning, world knowledge, and coding accuracy are state-of-the-art in its size category. As it relies on standard architecture, Mistral NeMo is easy to use and a drop-in replacement in any system using Mistral 7B. Reference Blog Hugging Face Write Preview <img src=\"https://ollama.com/assets/library/mistral-nemo/72045292-694a-4867-88c8-8635c9d97030\" width=\"280\" /> Mistral NeMo is a 12B model built in collaboration with NVIDIA. Mistral NeMo offers a large context window of up to 128k tokens. Its reasoning, world knowledge, and coding accuracy are state-of-the-art in its size category. As it relies on standard architecture, Mistral NeMo is easy to use and a drop-in replacement in any system using Mistral 7B. ![nemo-base-performance.png](https://ollama.com/assets/library/mistral-nemo/1adf7b56-30e3-49a0-8a52-bb74c19d8a78) ## Reference [Blog](https://mistral.ai/news/mistral-nemo/) [Hugging Face](https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407) Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "mistral-nemo:latest",
          "size_text": "7.1GB",
          "size_bytes": 7623566950,
          "context": "1000K",
          "input": "Text"
        },
        {
          "tag": "mistral-nemo:12b",
          "size_text": "7.1GB",
          "size_bytes": 7623566950,
          "context": "1000K",
          "input": "Text"
        },
        {
          "tag": "mistral-nemo:12b-instruct-2407-q2_K",
          "size_text": "4.8GB",
          "size_bytes": 5153960755,
          "context": "1000K",
          "input": "Text"
        },
        {
          "tag": "mistral-nemo:12b-instruct-2407-q3_K_S",
          "size_text": "5.5GB",
          "size_bytes": 5905580032,
          "context": "1000K",
          "input": "Text"
        },
        {
          "tag": "mistral-nemo:12b-instruct-2407-q3_K_M",
          "size_text": "6.1GB",
          "size_bytes": 6549825126,
          "context": "1000K",
          "input": "Text"
        },
        {
          "tag": "mistral-nemo:12b-instruct-2407-q3_K_L",
          "size_text": "6.6GB",
          "size_bytes": 7086696038,
          "context": "1000K",
          "input": "Text"
        },
        {
          "tag": "mistral-nemo:12b-instruct-2407-q4_0",
          "size_text": "7.1GB",
          "size_bytes": 7623566950,
          "context": "1000K",
          "input": "Text"
        },
        {
          "tag": "mistral-nemo:12b-instruct-2407-q4_1",
          "size_text": "7.8GB",
          "size_bytes": 8375186227,
          "context": "1000K",
          "input": "Text"
        },
        {
          "tag": "mistral-nemo:12b-instruct-2407-q4_K_S",
          "size_text": "7.1GB",
          "size_bytes": 7623566950,
          "context": "1000K",
          "input": "Text"
        },
        {
          "tag": "mistral-nemo:12b-instruct-2407-q4_K_M",
          "size_text": "7.5GB",
          "size_bytes": 8053063680,
          "context": "1000K",
          "input": "Text"
        },
        {
          "tag": "mistral-nemo:12b-instruct-2407-q5_0",
          "size_text": "8.5GB",
          "size_bytes": 9126805504,
          "context": "1000K",
          "input": "Text"
        },
        {
          "tag": "mistral-nemo:12b-instruct-2407-q5_1",
          "size_text": "9.2GB",
          "size_bytes": 9878424780,
          "context": "1000K",
          "input": "Text"
        },
        {
          "tag": "mistral-nemo:12b-instruct-2407-q5_K_S",
          "size_text": "8.5GB",
          "size_bytes": 9126805504,
          "context": "1000K",
          "input": "Text"
        },
        {
          "tag": "mistral-nemo:12b-instruct-2407-q5_K_M",
          "size_text": "8.7GB",
          "size_bytes": 9341553868,
          "context": "1000K",
          "input": "Text"
        },
        {
          "tag": "mistral-nemo:12b-instruct-2407-q6_K",
          "size_text": "10GB",
          "size_bytes": 10737418240,
          "context": "1000K",
          "input": "Text"
        },
        {
          "tag": "mistral-nemo:12b-instruct-2407-q8_0",
          "size_text": "13GB",
          "size_bytes": 13958643712,
          "context": "1000K",
          "input": "Text"
        },
        {
          "tag": "mistral-nemo:12b-instruct-2407-fp16",
          "size_text": "25GB",
          "size_bytes": 26843545600,
          "context": "1000K",
          "input": "Text"
        }
      ],
      "tags_count": 17
    },
    {
      "slug": "llama3.3",
      "capabilities": [
        "tools"
      ],
      "pulls": 2700000,
      "pulls_text": "2.7M \n  Downloads",
      "blurb": "llama3.3 New state of the art 70B model. Llama 3.3 70B offers similar performance compared to the Llama 3.1 405B model. 70b 14 Tags Updated 10 months ago",
      "name": "New state of the art 70B model. Llama 3.3 70B offers similar performance compared to the Llama 3.1 405B model.",
      "description": "Readme New state-of-the-art 70B model from Meta that offers similar performance compared to Llama 3.1 405B model. The Meta Llama 3.3 multilingual large language model (LLM) is a pretrained and instruction tuned generative model in 70B (text in/text out). The Llama 3.3 instruction tuned text only model is optimized for multilingual dialogue use cases and outperform many of the available open source and closed chat models on common industry benchmarks. Supported languages : English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai. New capabilities This release introduces new capabilities, including a longer context window, multilingual inputs and outputs and possible integrations by developers with third party tools. Building with these new capabilities requires specific considerations in addition to the best practices that generally apply across all Generative AI use cases. Tool-use: Just like in standard software development, developers are responsible for the integration of the LLM with the tools and services of their choice. They should define a clear policy for their use case and assess the integrity of the third party services they use to be aware of the safety and security limitations when using this capability. Refer to the Responsible Use Guide for best practices on the safe deployment of the third party safeguards. Multilinguality: Llama 3.3 supports 7 languages in addition to English: French, German, Hindi, Italian, Portuguese, Spanish, and Thai. Llama may be able to output text in other languages than those that meet performance thresholds for safety and helpfulness. We strongly discourage developers from using this model to converse in non-supported languages without implementing finetuning and system controls in alignment with their policies and the best practices shared in the Responsible Use Guide. Intended Use Intended Use Cases Llama 3.3 is intended for commercial and research use in multiple languages. Instruction tuned text only model",
      "variants": [
        {
          "tag": "llama3.3:latest",
          "size_text": "43GB",
          "size_bytes": 46170898432,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.3:70b",
          "size_text": "43GB",
          "size_bytes": 46170898432,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.3:70b-instruct-q2_K",
          "size_text": "26GB",
          "size_bytes": 27917287424,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.3:70b-instruct-q3_K_S",
          "size_text": "31GB",
          "size_bytes": 33285996544,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.3:70b-instruct-q3_K_M",
          "size_text": "34GB",
          "size_bytes": 36507222016,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.3:70b-instruct-q4_0",
          "size_text": "40GB",
          "size_bytes": 42949672960,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.3:70b-instruct-q4_K_S",
          "size_text": "40GB",
          "size_bytes": 42949672960,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.3:70b-instruct-q4_K_M",
          "size_text": "43GB",
          "size_bytes": 46170898432,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.3:70b-instruct-q5_0",
          "size_text": "49GB",
          "size_bytes": 52613349376,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.3:70b-instruct-q5_1",
          "size_text": "53GB",
          "size_bytes": 56908316672,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.3:70b-instruct-q5_K_M",
          "size_text": "50GB",
          "size_bytes": 53687091200,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.3:70b-instruct-q6_K",
          "size_text": "58GB",
          "size_bytes": 62277025792,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.3:70b-instruct-q8_0",
          "size_text": "75GB",
          "size_bytes": 80530636800,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.3:70b-instruct-fp16",
          "size_text": "141GB",
          "size_bytes": 151397597184,
          "context": "128K",
          "input": "Text"
        }
      ],
      "tags_count": 14
    },
    {
      "slug": "bge-m3",
      "capabilities": [
        "embedding"
      ],
      "pulls": 2600000,
      "pulls_text": "2.6M \n  Downloads",
      "blurb": "bge-m3 BGE-M3 is a new model from BAAI distinguished for its versatility in Multi-Functionality, Multi-Linguality, and Multi-Granularity. 567m 3 Tags Updated 1 year ago",
      "name": "BGE-M3 is a new model from BAAI distinguished for its versatility in Multi-Functionality, Multi-Linguality, and Multi-Granularity.",
      "description": "Readme BGE-M3 is based on the XLM-RoBERTa architecture and is distinguished for its versatility in Multi-Functionality, Multi-Linguality, and Multi-Granularity: Multi-Functionality: It can simultaneously perform the three common retrieval functionalities of embedding model: dense retrieval, multi-vector retrieval, and sparse retrieval. Multi-Linguality: It can support more than 100 working languages. Multi-Granularity: It is able to process inputs of different granularities, spanning from short sentences to long documents of up to 8192 tokens. Benchmarks from the open-source community @misc{bge-m3, title={BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation}, author={Jianlv Chen and Shitao Xiao and Peitian Zhang and Kun Luo and Defu Lian and Zheng Liu}, year={2024}, eprint={2402.03216}, archivePrefix={arXiv}, primaryClass={cs.CL} } Write Preview BGE-M3 is based on the XLM-RoBERTa architecture and is distinguished for its versatility in Multi-Functionality, Multi-Linguality, and Multi-Granularity: - Multi-Functionality: It can simultaneously perform the three common retrieval functionalities of embedding model: dense retrieval, multi-vector retrieval, and sparse retrieval. - Multi-Linguality: It can support more than 100 working languages. - Multi-Granularity: It is able to process inputs of different granularities, spanning from short sentences to long documents of up to 8192 tokens. **Benchmarks from the open-source community** ![image.png](https://ollama.com/assets/library/bge-m3/17a9804b-f3da-4d09-bd18-90d4fe8900d3) ``` @misc{bge-m3, title={BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation}, author={Jianlv Chen and Shitao Xiao and Peitian Zhang and Kun Luo and Defu Lian and Zheng Liu}, year={2024}, eprint={2402.03216}, archivePrefix={arXiv}, primaryClass={cs.CL} } ``` Paste, drop or click to upload images (.png, .jpeg, .",
      "variants": [
        {
          "tag": "bge-m3:latest",
          "size_text": "1.2GB",
          "size_bytes": 1288490188,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "bge-m3:567m",
          "size_text": "1.2GB",
          "size_bytes": 1288490188,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "bge-m3:567m-fp16",
          "size_text": "1.2GB",
          "size_bytes": 1288490188,
          "context": "8K",
          "input": "Text"
        }
      ],
      "tags_count": 3
    },
    {
      "slug": "deepseek-v3",
      "capabilities": [],
      "pulls": 2600000,
      "pulls_text": "2.6M \n  Downloads",
      "blurb": "deepseek-v3 A strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. 671b 5 Tags Updated 9 months ago",
      "name": "A strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token.",
      "description": "Readme Note: this model requires Ollama 0.5.5 or later. DeepSeek-V3 achieves a significant breakthrough in inference speed over previous models. It tops the leaderboard among open-source models and rivals the most advanced closed-source models globally. References GitHub Paper Write Preview > Note: this model requires [Ollama 0.5.5](https://github.com/ollama/ollama/releases/tag/v0.5.5) or later. <img src=\"/assets/library/deepseek-v3/069ccc94-63b0-41e6-b2b3-e8e56068ab1a\" width=\"320\" /> DeepSeek-V3 achieves a significant breakthrough in inference speed over previous models. It tops the leaderboard among open-source models and rivals the most advanced closed-source models globally. ## References [GitHub](https://github.com/deepseek-ai/DeepSeek-V3) [Paper](https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf) Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "deepseek-v3:latest",
          "size_text": "404GB",
          "size_bytes": 433791696896,
          "context": "160K",
          "input": "Text"
        },
        {
          "tag": "deepseek-v3:671b",
          "size_text": "404GB",
          "size_bytes": 433791696896,
          "context": "160K",
          "input": "Text"
        },
        {
          "tag": "deepseek-v3:671b-q4_K_M",
          "size_text": "404GB",
          "size_bytes": 433791696896,
          "context": "160K",
          "input": "Text"
        },
        {
          "tag": "deepseek-v3:671b-q8_0",
          "size_text": "713GB",
          "size_bytes": 765577920512,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-v3:671b-fp16",
          "size_text": null,
          "size_bytes": null,
          "context": "4K",
          "input": "Text"
        }
      ],
      "tags_count": 5
    },
    {
      "slug": "mistral-small",
      "capabilities": [
        "reasoning",
        "tools"
      ],
      "pulls": 2100000,
      "pulls_text": "2.1M \n  Downloads",
      "blurb": "mistral-small Mistral Small 3 sets a new benchmark in the “small” Large Language Models category below 70B. 22b 24b 21 Tags Updated 9 months ago",
      "name": "Mistral Small 3 sets a new benchmark in the “small” Large Language Models category below 70B.",
      "description": "Readme Mistral Small 3 sets a new benchmark in the “small” Large Language Models category below 70B, boasting 24B parameters and achieving state-of-the-art capabilities comparable to larger models. Mistral Small can be deployed locally and is exceptionally “knowledge-dense”, fitting in a single RTX 4090 or a 32GB RAM MacBook once quantized. Perfect for: Fast response conversational agents. Low latency function calling. Subject matter experts via fine-tuning. Local inference for hobbyists and organizations handling sensitive data. Key Features Multilingual: Supports dozens of languages, including English, French, German, Spanish, Italian, Chinese, Japanese, Korean, Portuguese, Dutch, and Polish. Agent-Centric: Offers best-in-class agentic capabilities with native function calling and JSON outputting. Advanced Reasoning: State-of-the-art conversational and reasoning capabilities. Apache 2.0 License: Open license allowing usage and modification for both commercial and non-commercial purposes. Context Window: A 32k context window. System Prompt: Maintains strong adherence and support for system prompts. Tokenizer: Utilizes a Tekken tokenizer with a 131k vocabulary size. Human Evaluations We conducted side by side evaluations with an external third-party vendor, on a set of over 1k proprietary coding and generalist prompts. Evaluators were tasked with selecting their preferred model response from anonymized generations produced by Mistral Small 3 vs another model. We are aware that in some cases the benchmarks on human judgement starkly differ from publicly available benchmarks, but have taken extra caution in verifying a fair evaluation. We are confident that the above benchmarks are valid. Instruct performance Our instruction tuned model performs competitively with open weight models three times its size and with proprietary GPT4o-mini model across Code, Math, General knowledge and Instruction following benchmarks. Performance accuracy on all benchmarks were obtained t",
      "variants": [
        {
          "tag": "mistral-small:latest",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral-small:22b",
          "size_text": "13GB",
          "size_bytes": 13958643712,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "mistral-small:24b",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral-small:22b-instruct-2409-q2_K",
          "size_text": "8.3GB",
          "size_bytes": 8912057139,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "mistral-small:22b-instruct-2409-q3_K_S",
          "size_text": "9.6GB",
          "size_bytes": 10307921510,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "mistral-small:22b-instruct-2409-q3_K_M",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "mistral-small:22b-instruct-2409-q3_K_L",
          "size_text": "12GB",
          "size_bytes": 12884901888,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "mistral-small:22b-instruct-2409-q4_0",
          "size_text": "13GB",
          "size_bytes": 13958643712,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "mistral-small:22b-instruct-2409-q4_1",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "mistral-small:22b-instruct-2409-q4_K_S",
          "size_text": "13GB",
          "size_bytes": 13958643712,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "mistral-small:22b-instruct-2409-q4_K_M",
          "size_text": "13GB",
          "size_bytes": 13958643712,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "mistral-small:22b-instruct-2409-q5_0",
          "size_text": "15GB",
          "size_bytes": 16106127360,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "mistral-small:22b-instruct-2409-q5_1",
          "size_text": "17GB",
          "size_bytes": 18253611008,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "mistral-small:22b-instruct-2409-q5_K_S",
          "size_text": "15GB",
          "size_bytes": 16106127360,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "mistral-small:22b-instruct-2409-q5_K_M",
          "size_text": "16GB",
          "size_bytes": 17179869184,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "mistral-small:22b-instruct-2409-q6_K",
          "size_text": "18GB",
          "size_bytes": 19327352832,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "mistral-small:22b-instruct-2409-q8_0",
          "size_text": "24GB",
          "size_bytes": 25769803776,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "mistral-small:22b-instruct-2409-fp16",
          "size_text": "44GB",
          "size_bytes": 47244640256,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "mistral-small:24b-instruct-2501-q4_K_M",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral-small:24b-instruct-2501-q8_0",
          "size_text": "25GB",
          "size_bytes": 26843545600,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral-small:24b-instruct-2501-fp16",
          "size_text": "47GB",
          "size_bytes": 50465865728,
          "context": "32K",
          "input": "Text"
        }
      ],
      "tags_count": 21
    },
    {
      "slug": "llava-llama3",
      "capabilities": [
        "vision"
      ],
      "pulls": 2000000,
      "pulls_text": "2M \n  Downloads",
      "blurb": "llava-llama3 A LLaVA model fine-tuned from Llama 3 Instruct with better scores in several benchmarks. 8b 4 Tags Updated 1 year ago",
      "name": "A LLaVA model fine-tuned from Llama 3 Instruct with better scores in several benchmarks.",
      "description": "Readme llava-llama3 is a LLaVA model fine-tuned from Llama 3 Instruct and CLIP-ViT-Large-patch14-336 with ShareGPT4V-PT and InternVL-SFT by XTuner. References Hugging Face GitHub Write Preview <img src=\"https://ollama.com/assets/library/llava-llama3/dc3b65cd-62de-45cd-93f9-5c6da62214fa\" width=\"320\" /> `llava-llama3` is a LLaVA model fine-tuned from Llama 3 Instruct and CLIP-ViT-Large-patch14-336 with ShareGPT4V-PT and InternVL-SFT by XTuner. <img src=\"https://ollama.com/assets/library/llava-llama3/a4be49e3-b088-4f59-8536-4e55a98ccf4c\" width=\"480\" /> ## References [Hugging Face](https://huggingface.co/xtuner/llava-llama-3-8b-v1_1-gguf) [GitHub](https://github.com/InternLM/xtuner/tree/main) Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "llava-llama3:latest",
          "size_text": "5.5GB",
          "size_bytes": 5905580032,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llava-llama3:8b",
          "size_text": "5.5GB",
          "size_bytes": 5905580032,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llava-llama3:8b-v1.1-q4_0",
          "size_text": "5.5GB",
          "size_bytes": 5905580032,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llava-llama3:8b-v1.1-fp16",
          "size_text": "17GB",
          "size_bytes": 18253611008,
          "context": "8K",
          "input": "Text"
        }
      ],
      "tags_count": 4
    },
    {
      "slug": "smollm2",
      "capabilities": [
        "tools"
      ],
      "pulls": 2000000,
      "pulls_text": "2M \n  Downloads",
      "blurb": "smollm2 SmolLM2 is a family of compact language models available in three size: 135M, 360M, and 1.7B parameters. 135m 360m 1.7b 49 Tags Updated 1 year ago",
      "name": "SmolLM2 is a family of compact language models available in three size: 135M, 360M, and 1.7B parameters.",
      "description": "Readme SmolLM2 is a family of compact language models available in three size: 135M, 360M, and 1.7B parameters. They are capable of solving a wide range of tasks while being lightweight enough to run on-device. References HuggingFace Write Preview ![y45hIMNREW7w_XpHYB_0q.png](/assets/library/smollm2/2616a6ce-5645-48c3-bd5d-9a00f1dd0c9e) SmolLM2 is a family of compact language models available in three size: 135M, 360M, and 1.7B parameters. They are capable of solving a wide range of tasks while being lightweight enough to run on-device. ![benchmark](/assets/library/smollm2/1a637eca-6f07-4572-8e01-011339c072d8) ## References [HuggingFace](https://huggingface.co/collections/HuggingFaceTB/smollm2-6723884218bcda64b34d7db9) Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "smollm2:latest",
          "size_text": "1.8GB",
          "size_bytes": 1932735283,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "smollm2:135m",
          "size_text": "271MB",
          "size_bytes": 284164096,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "smollm2:360m",
          "size_text": "726MB",
          "size_bytes": 761266176,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "smollm2:1.7b",
          "size_text": "1.8GB",
          "size_bytes": 1932735283,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "smollm2:135m-instruct-q2_K",
          "size_text": "88MB",
          "size_bytes": 92274688,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "smollm2:135m-instruct-q3_K_S",
          "size_text": "88MB",
          "size_bytes": 92274688,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "smollm2:135m-instruct-q3_K_M",
          "size_text": "94MB",
          "size_bytes": 98566144,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "smollm2:135m-instruct-q3_K_L",
          "size_text": "98MB",
          "size_bytes": 102760448,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "smollm2:135m-instruct-q4_0",
          "size_text": "92MB",
          "size_bytes": 96468992,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "smollm2:135m-instruct-q4_1",
          "size_text": "98MB",
          "size_bytes": 102760448,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "smollm2:135m-instruct-q4_K_S",
          "size_text": "102MB",
          "size_bytes": 106954752,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "smollm2:135m-instruct-q4_K_M",
          "size_text": "105MB",
          "size_bytes": 110100480,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "smollm2:135m-instruct-q5_0",
          "size_text": "105MB",
          "size_bytes": 110100480,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "smollm2:135m-instruct-q5_1",
          "size_text": "112MB",
          "size_bytes": 117440512,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "smollm2:135m-instruct-q5_K_S",
          "size_text": "110MB",
          "size_bytes": 115343360,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "smollm2:135m-instruct-q5_K_M",
          "size_text": "112MB",
          "size_bytes": 117440512,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "smollm2:135m-instruct-q6_K",
          "size_text": "138MB",
          "size_bytes": 144703488,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "smollm2:135m-instruct-q8_0",
          "size_text": "145MB",
          "size_bytes": 152043520,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "smollm2:135m-instruct-fp16",
          "size_text": "271MB",
          "size_bytes": 284164096,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "smollm2:360m-instruct-q2_K",
          "size_text": "219MB",
          "size_bytes": 229638144,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "smollm2:360m-instruct-q3_K_S",
          "size_text": "219MB",
          "size_bytes": 229638144,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "smollm2:360m-instruct-q3_K_M",
          "size_text": "235MB",
          "size_bytes": 246415360,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "smollm2:360m-instruct-q3_K_L",
          "size_text": "246MB",
          "size_bytes": 257949696,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "smollm2:360m-instruct-q4_0",
          "size_text": "229MB",
          "size_bytes": 240123904,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "smollm2:360m-instruct-q4_1",
          "size_text": "249MB",
          "size_bytes": 261095424,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "smollm2:360m-instruct-q4_K_S",
          "size_text": "260MB",
          "size_bytes": 272629760,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "smollm2:360m-instruct-q4_K_M",
          "size_text": "271MB",
          "size_bytes": 284164096,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "smollm2:360m-instruct-q5_0",
          "size_text": "268MB",
          "size_bytes": 281018368,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "smollm2:360m-instruct-q5_1",
          "size_text": "288MB",
          "size_bytes": 301989888,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "smollm2:360m-instruct-q5_K_S",
          "size_text": "283MB",
          "size_bytes": 296747008,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "smollm2:360m-instruct-q5_K_M",
          "size_text": "290MB",
          "size_bytes": 304087040,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "smollm2:360m-instruct-q6_K",
          "size_text": "367MB",
          "size_bytes": 384827392,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "smollm2:360m-instruct-q8_0",
          "size_text": "386MB",
          "size_bytes": 404750336,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "smollm2:360m-instruct-fp16",
          "size_text": "726MB",
          "size_bytes": 761266176,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "smollm2:1.7b-instruct-q2_K",
          "size_text": "675MB",
          "size_bytes": 707788800,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "smollm2:1.7b-instruct-q3_K_S",
          "size_text": "777MB",
          "size_bytes": 814743552,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "smollm2:1.7b-instruct-q3_K_M",
          "size_text": "860MB",
          "size_bytes": 901775360,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "smollm2:1.7b-instruct-q3_K_L",
          "size_text": "933MB",
          "size_bytes": 978321408,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "smollm2:1.7b-instruct-q4_0",
          "size_text": "991MB",
          "size_bytes": 1039138816,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "smollm2:1.7b-instruct-q4_1",
          "size_text": "1.1GB",
          "size_bytes": 1181116006,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "smollm2:1.7b-instruct-q4_K_S",
          "size_text": "999MB",
          "size_bytes": 1047527424,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "smollm2:1.7b-instruct-q4_K_M",
          "size_text": "1.1GB",
          "size_bytes": 1181116006,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "smollm2:1.7b-instruct-q5_0",
          "size_text": "1.2GB",
          "size_bytes": 1288490188,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "smollm2:1.7b-instruct-q5_1",
          "size_text": "1.3GB",
          "size_bytes": 1395864371,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "smollm2:1.7b-instruct-q5_K_S",
          "size_text": "1.2GB",
          "size_bytes": 1288490188,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "smollm2:1.7b-instruct-q5_K_M",
          "size_text": "1.2GB",
          "size_bytes": 1288490188,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "smollm2:1.7b-instruct-q6_K",
          "size_text": "1.4GB",
          "size_bytes": 1503238553,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "smollm2:1.7b-instruct-q8_0",
          "size_text": "1.8GB",
          "size_bytes": 1932735283,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "smollm2:1.7b-instruct-fp16",
          "size_text": "3.4GB",
          "size_bytes": 3650722201,
          "context": "8K",
          "input": "Text"
        }
      ],
      "tags_count": 49
    },
    {
      "slug": "all-minilm",
      "capabilities": [
        "embedding"
      ],
      "pulls": 1900000,
      "pulls_text": "1.9M \n  Downloads",
      "blurb": "all-minilm models on very large sentence level datasets. 22m 33m 10 Tags Updated 1 year ago",
      "name": "Embedding models on very large sentence level datasets.",
      "description": "Readme Note: this model requires Ollama 0.1.26 or later. Download it here . It can only be used to generate embeddings. The project aims to train sentence embedding models on very large sentence level datasets using a self-supervised contrastive learning objective. Usage REST API curl http://localhost:11434/api/embeddings -d '{ \"model\": \"all-minilm\", \"prompt\": \"The sky is blue because of Rayleigh scattering\" }' Python library ollama.embeddings(model='all-minilm', prompt='The sky is blue because of Rayleigh scattering') Javascript library ollama.embeddings({ model: 'all-minilm', prompt: 'The sky is blue because of Rayleigh scattering' }) References HuggingFace Website Write Preview <img src=\"https://github.com/ollama/ollama/assets/251292/4a6d8231-b7b5-411b-80b9-db37e5323cf9\" width=\"320\" /> > Note: this model requires Ollama 0.1.26 or later. [Download it here](https://ollama.com/download). It can only be used to generate embeddings. The project aims to train sentence embedding models on very large sentence level datasets using a self-supervised contrastive learning objective. ## Usage ### REST API ``` curl http://localhost:11434/api/embeddings -d '{ \"model\": \"all-minilm\", \"prompt\": \"The sky is blue because of Rayleigh scattering\" }' ``` ### Python library ``` ollama.embeddings(model='all-minilm', prompt='The sky is blue because of Rayleigh scattering') ``` ### Javascript library ``` ollama.embeddings({ model: 'all-minilm', prompt: 'The sky is blue because of Rayleigh scattering' }) ``` ## References [HuggingFace](https://huggingface.co/sentence-transformers) [Website](https://www.sbert.net/) Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "all-minilm:latest",
          "size_text": "46MB",
          "size_bytes": 48234496,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "all-minilm:l12",
          "size_text": "67MB",
          "size_bytes": 70254592,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "all-minilm:l6",
          "size_text": "46MB",
          "size_bytes": 48234496,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "all-minilm:v2",
          "size_text": "46MB",
          "size_bytes": 48234496,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "all-minilm:22m",
          "size_text": "46MB",
          "size_bytes": 48234496,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "all-minilm:33m",
          "size_text": "67MB",
          "size_bytes": 70254592,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "all-minilm:22m-l6-v2-fp16",
          "size_text": "46MB",
          "size_bytes": 48234496,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "all-minilm:33m-l12-v2-fp16",
          "size_text": "67MB",
          "size_bytes": 70254592,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "all-minilm:l12-v2",
          "size_text": "67MB",
          "size_bytes": 70254592,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "all-minilm:l6-v2",
          "size_text": "46MB",
          "size_bytes": 48234496,
          "context": null,
          "input": "Text"
        }
      ],
      "tags_count": 10
    },
    {
      "slug": "deepseek-coder",
      "capabilities": [],
      "pulls": 1700000,
      "pulls_text": "1.7M \n  Downloads",
      "blurb": "deepseek-coder DeepSeek Coder is a capable coding model trained on two trillion code and natural language tokens. 1.3b 6.7b 33b 102 Tags Updated 1 year ago",
      "name": "DeepSeek Coder is a capable coding model trained on two trillion code and natural language tokens.",
      "description": "Readme DeepSeek Coder is trained from scratch on both 87% code and 13% natural language in English and Chinese. Each of the models are pre-trained on 2 trillion tokens. Models available 1.3 billion parameter model ollama run deepseek-coder 6.7 billion parameter model ollama run deepseek-coder:6.7b 33 billion parameter model ollama run deepseek-coder:33b CLI Open the terminal and run ollama run deepseek-coder API Example using curl: curl -X POST http://localhost:11434/api/generate -d '{ \"model\": \"deepseek-coder\", \"prompt\":\"Why is the sky blue?\" }' API documentation References HuggingFace Write Preview <img src=\"https://github.com/jmorganca/ollama/assets/251292/0e18bfb4-2964-41e4-b01c-043670228690\" width=\"460\"> DeepSeek Coder is trained from scratch on both 87% code and 13% natural language in English and Chinese. Each of the models are pre-trained on 2 trillion tokens. ### Models available 1.3 billion parameter model ``` ollama run deepseek-coder ``` 6.7 billion parameter model ``` ollama run deepseek-coder:6.7b ``` 33 billion parameter model ``` ollama run deepseek-coder:33b ``` ### CLI Open the terminal and run `ollama run deepseek-coder` ### API Example using curl: ```bash curl -X POST http://localhost:11434/api/generate -d '{ \"model\": \"deepseek-coder\", \"prompt\":\"Why is the sky blue?\" }' ``` [API documentation](https://github.com/jmorganca/ollama/blob/main/docs/api.md) ## References [HuggingFace](https://huggingface.co/deepseek-ai) Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "deepseek-coder:latest",
          "size_text": "776MB",
          "size_bytes": 813694976,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:base",
          "size_text": "776MB",
          "size_bytes": 813694976,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:instruct",
          "size_text": "776MB",
          "size_bytes": 813694976,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:1.3b",
          "size_text": "776MB",
          "size_bytes": 813694976,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:6.7b",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:33b",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:1.3b-base",
          "size_text": "776MB",
          "size_bytes": 813694976,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:1.3b-base-q2_K",
          "size_text": "632MB",
          "size_bytes": 662700032,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:1.3b-base-q3_K_S",
          "size_text": "659MB",
          "size_bytes": 691011584,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:1.3b-base-q3_K_M",
          "size_text": "705MB",
          "size_bytes": 739246080,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:1.3b-base-q3_K_L",
          "size_text": "745MB",
          "size_bytes": 781189120,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:1.3b-base-q4_0",
          "size_text": "776MB",
          "size_bytes": 813694976,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:1.3b-base-q4_1",
          "size_text": "856MB",
          "size_bytes": 897581056,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:1.3b-base-q4_K_S",
          "size_text": "815MB",
          "size_bytes": 854589440,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:1.3b-base-q4_K_M",
          "size_text": "874MB",
          "size_bytes": 916455424,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:1.3b-base-q5_0",
          "size_text": "936MB",
          "size_bytes": 981467136,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:1.3b-base-q5_1",
          "size_text": "1.0GB",
          "size_bytes": 1073741824,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:1.3b-base-q5_K_S",
          "size_text": "953MB",
          "size_bytes": 999292928,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:1.3b-base-q5_K_M",
          "size_text": "1.0GB",
          "size_bytes": 1073741824,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:1.3b-base-q6_K",
          "size_text": "1.2GB",
          "size_bytes": 1288490188,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:1.3b-base-q8_0",
          "size_text": "1.4GB",
          "size_bytes": 1503238553,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:1.3b-base-fp16",
          "size_text": "2.7GB",
          "size_bytes": 2899102924,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:1.3b-instruct",
          "size_text": "776MB",
          "size_bytes": 813694976,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:1.3b-instruct-q2_K",
          "size_text": "632MB",
          "size_bytes": 662700032,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:1.3b-instruct-q3_K_S",
          "size_text": "659MB",
          "size_bytes": 691011584,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:1.3b-instruct-q3_K_M",
          "size_text": "705MB",
          "size_bytes": 739246080,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:1.3b-instruct-q3_K_L",
          "size_text": "745MB",
          "size_bytes": 781189120,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:1.3b-instruct-q4_0",
          "size_text": "776MB",
          "size_bytes": 813694976,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:1.3b-instruct-q4_1",
          "size_text": "856MB",
          "size_bytes": 897581056,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:1.3b-instruct-q4_K_S",
          "size_text": "815MB",
          "size_bytes": 854589440,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:1.3b-instruct-q4_K_M",
          "size_text": "874MB",
          "size_bytes": 916455424,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:1.3b-instruct-q5_0",
          "size_text": "936MB",
          "size_bytes": 981467136,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:1.3b-instruct-q5_1",
          "size_text": "1.0GB",
          "size_bytes": 1073741824,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:1.3b-instruct-q5_K_S",
          "size_text": "953MB",
          "size_bytes": 999292928,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:1.3b-instruct-q5_K_M",
          "size_text": "1.0GB",
          "size_bytes": 1073741824,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:1.3b-instruct-q6_K",
          "size_text": "1.2GB",
          "size_bytes": 1288490188,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:1.3b-instruct-q8_0",
          "size_text": "1.4GB",
          "size_bytes": 1503238553,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:1.3b-instruct-fp16",
          "size_text": "2.7GB",
          "size_bytes": 2899102924,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:6.7b-base",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:6.7b-base-q2_K",
          "size_text": "2.8GB",
          "size_bytes": 3006477107,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:6.7b-base-q3_K_S",
          "size_text": "3.0GB",
          "size_bytes": 3221225472,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:6.7b-base-q3_K_M",
          "size_text": "3.3GB",
          "size_bytes": 3543348019,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:6.7b-base-q3_K_L",
          "size_text": "3.6GB",
          "size_bytes": 3865470566,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:6.7b-base-q4_0",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:6.7b-base-q4_1",
          "size_text": "4.2GB",
          "size_bytes": 4509715660,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:6.7b-base-q4_K_S",
          "size_text": "3.9GB",
          "size_bytes": 4187593113,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:6.7b-base-q4_K_M",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:6.7b-base-q5_0",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:6.7b-base-q5_1",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:6.7b-base-q5_K_S",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:6.7b-base-q5_K_M",
          "size_text": "4.8GB",
          "size_bytes": 5153960755,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:6.7b-base-q6_K",
          "size_text": "5.5GB",
          "size_bytes": 5905580032,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:6.7b-base-q8_0",
          "size_text": "7.2GB",
          "size_bytes": 7730941132,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:6.7b-base-fp16",
          "size_text": "13GB",
          "size_bytes": 13958643712,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:6.7b-instruct",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:6.7b-instruct-q2_K",
          "size_text": "2.8GB",
          "size_bytes": 3006477107,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:6.7b-instruct-q3_K_S",
          "size_text": "3.0GB",
          "size_bytes": 3221225472,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:6.7b-instruct-q3_K_M",
          "size_text": "3.3GB",
          "size_bytes": 3543348019,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:6.7b-instruct-q3_K_L",
          "size_text": "3.6GB",
          "size_bytes": 3865470566,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:6.7b-instruct-q4_0",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:6.7b-instruct-q4_1",
          "size_text": "4.2GB",
          "size_bytes": 4509715660,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:6.7b-instruct-q4_K_S",
          "size_text": "3.9GB",
          "size_bytes": 4187593113,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:6.7b-instruct-q4_K_M",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:6.7b-instruct-q5_0",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:6.7b-instruct-q5_1",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:6.7b-instruct-q5_K_S",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:6.7b-instruct-q5_K_M",
          "size_text": "4.8GB",
          "size_bytes": 5153960755,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:6.7b-instruct-q6_K",
          "size_text": "5.5GB",
          "size_bytes": 5905580032,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:6.7b-instruct-q8_0",
          "size_text": "7.2GB",
          "size_bytes": 7730941132,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:6.7b-instruct-fp16",
          "size_text": "13GB",
          "size_bytes": 13958643712,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:33b-base",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:33b-base-q2_K",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:33b-base-q3_K_S",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:33b-base-q3_K_M",
          "size_text": "16GB",
          "size_bytes": 17179869184,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:33b-base-q3_K_L",
          "size_text": "18GB",
          "size_bytes": 19327352832,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:33b-base-q4_0",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:33b-base-q4_1",
          "size_text": "21GB",
          "size_bytes": 22548578304,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:33b-base-q4_K_S",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:33b-base-q4_K_M",
          "size_text": "20GB",
          "size_bytes": 21474836480,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:33b-base-q5_0",
          "size_text": "23GB",
          "size_bytes": 24696061952,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:33b-base-q5_1",
          "size_text": "25GB",
          "size_bytes": 26843545600,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:33b-base-q5_K_S",
          "size_text": "23GB",
          "size_bytes": 24696061952,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:33b-base-q5_K_M",
          "size_text": "24GB",
          "size_bytes": 25769803776,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:33b-base-q6_K",
          "size_text": "27GB",
          "size_bytes": 28991029248,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:33b-base-q8_0",
          "size_text": "35GB",
          "size_bytes": 37580963840,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:33b-base-fp16",
          "size_text": "67GB",
          "size_bytes": 71940702208,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:33b-instruct",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:33b-instruct-q2_K",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:33b-instruct-q3_K_S",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:33b-instruct-q3_K_M",
          "size_text": "16GB",
          "size_bytes": 17179869184,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:33b-instruct-q3_K_L",
          "size_text": "18GB",
          "size_bytes": 19327352832,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:33b-instruct-q4_0",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:33b-instruct-q4_1",
          "size_text": "21GB",
          "size_bytes": 22548578304,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:33b-instruct-q4_K_S",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:33b-instruct-q4_K_M",
          "size_text": "20GB",
          "size_bytes": 21474836480,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:33b-instruct-q5_0",
          "size_text": "23GB",
          "size_bytes": 24696061952,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:33b-instruct-q5_1",
          "size_text": "25GB",
          "size_bytes": 26843545600,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:33b-instruct-q5_K_S",
          "size_text": "23GB",
          "size_bytes": 24696061952,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:33b-instruct-q5_K_M",
          "size_text": "24GB",
          "size_bytes": 25769803776,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:33b-instruct-q6_K",
          "size_text": "27GB",
          "size_bytes": 28991029248,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:33b-instruct-q8_0",
          "size_text": "35GB",
          "size_bytes": 37580963840,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder:33b-instruct-fp16",
          "size_text": "67GB",
          "size_bytes": 71940702208,
          "context": "16K",
          "input": "Text"
        }
      ],
      "tags_count": 102
    },
    {
      "slug": "qwq",
      "capabilities": [
        "reasoning",
        "thinking",
        "tools"
      ],
      "pulls": 1700000,
      "pulls_text": "1.7M \n  Downloads",
      "blurb": "qwq QwQ is the model of the Qwen series. 32b 8 Tags Updated 7 months ago",
      "name": "QwQ is the reasoning model of the Qwen series.",
      "description": "Readme QwQ is the reasoning model of the Qwen series. Compared with conventional instruction-tuned models, QwQ, which is capable of thinking and reasoning, can achieve significantly enhanced performance in downstream tasks, especially hard problems. QwQ-32B is the medium-sized reasoning model, which is capable of achieving competitive performance against state-of-the-art reasoning models, e.g., DeepSeek-R1, o1-mini. Future Work This marks Qwen’s initial step in scaling Reinforcement Learning (RL) to enhance reasoning capabilities. Through this journey, we have not only witnessed the immense potential of scaled RL but also recognized the untapped possibilities within pretrained language models. As we work towards developing the next generation of Qwen, we are confident that combining stronger foundation models with RL powered by scaled computational resources will propel us closer to achieving Artificial General Intelligence (AGI). Additionally, we are actively exploring the integration of agents with RL to enable long-horizon reasoning, aiming to unlock greater intelligence with inference time scaling. Reference Blog Write Preview QwQ is the reasoning model of the Qwen series. Compared with conventional instruction-tuned models, QwQ, which is capable of thinking and reasoning, can achieve significantly enhanced performance in downstream tasks, especially hard problems. QwQ-32B is the medium-sized reasoning model, which is capable of achieving competitive performance against state-of-the-art reasoning models, e.g., DeepSeek-R1, o1-mini. ![](/assets/library/qwq/e3d71b1c-9c62-413a-a63a-1ca604189a17) ### Future Work This marks Qwen’s initial step in scaling Reinforcement Learning (RL) to enhance reasoning capabilities. Through this journey, we have not only witnessed the immense potential of scaled RL but also recognized the untapped possibilities within pretrained language models. As we work towards developing the next generation of Qwen, we are confident that combinin",
      "variants": [
        {
          "tag": "qwq:latest",
          "size_text": "20GB",
          "size_bytes": 21474836480,
          "context": "40K",
          "input": "Text"
        },
        {
          "tag": "qwq:32b",
          "size_text": "20GB",
          "size_bytes": 21474836480,
          "context": "40K",
          "input": "Text"
        },
        {
          "tag": "qwq:32b-preview-q4_K_M",
          "size_text": "20GB",
          "size_bytes": 21474836480,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwq:32b-preview-q8_0",
          "size_text": "35GB",
          "size_bytes": 37580963840,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwq:32b-preview-fp16",
          "size_text": "66GB",
          "size_bytes": 70866960384,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwq:32b-q4_K_M",
          "size_text": "20GB",
          "size_bytes": 21474836480,
          "context": "40K",
          "input": "Text"
        },
        {
          "tag": "qwq:32b-q8_0",
          "size_text": "35GB",
          "size_bytes": 37580963840,
          "context": "40K",
          "input": "Text"
        },
        {
          "tag": "qwq:32b-fp16",
          "size_text": "66GB",
          "size_bytes": 70866960384,
          "context": "40K",
          "input": "Text"
        }
      ],
      "tags_count": 8
    },
    {
      "slug": "mixtral",
      "capabilities": [
        "tools"
      ],
      "pulls": 1400000,
      "pulls_text": "1.4M \n  Downloads",
      "blurb": "mixtral A set of Mixture of Experts (MoE) model with open weights by Mistral AI in 8x7b and 8x22b parameter sizes. 8x7b 8x22b 70 Tags Updated 10 months ago",
      "name": "A set of Mixture of Experts (MoE) model with open weights by Mistral AI in 8x7b and 8x22b parameter sizes.",
      "description": "Readme The Mixtral large Language Models (LLM) are a set of pretrained generative Sparse Mixture of Experts. Sizes mixtral:8x22b mixtral:8x7b Mixtral 8x22b ollama run mixtral:8x22b Mixtral 8x22B sets a new standard for performance and efficiency within the AI community. It is a sparse Mixture-of-Experts (SMoE) model that uses only 39B active parameters out of 141B, offering unparalleled cost efficiency for its size. Mixtral 8x22B comes with the following strengths: It is fluent in English, French, Italian, German, and Spanish It has strong maths and coding capabilities It is natively capable of function calling 64K tokens context window allows precise information recall from large documents References Announcement HuggingFace Write Preview <img src=\"https://github.com/ollama/ollama/assets/251292/59bf76bc-1f6f-4a45-be4f-d6a13d7f0645\" width=\"320\"> The Mixtral large Language Models (LLM) are a set of pretrained generative Sparse Mixture of Experts. ## Sizes * `mixtral:8x22b` * `mixtral:8x7b` ## Mixtral 8x22b ``` ollama run mixtral:8x22b ``` Mixtral 8x22B sets a new standard for performance and efficiency within the AI community. It is a sparse Mixture-of-Experts (SMoE) model that uses only 39B active parameters out of 141B, offering unparalleled cost efficiency for its size. Mixtral 8x22B comes with the following strengths: * It is fluent in English, French, Italian, German, and Spanish * It has strong maths and coding capabilities * It is natively capable of function calling * 64K tokens context window allows precise information recall from large documents ## References [Announcement](https://mistral.ai/news/mixtral-of-experts/) [HuggingFace](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1) Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "mixtral:latest",
          "size_text": "26GB",
          "size_bytes": 27917287424,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mixtral:instruct",
          "size_text": "26GB",
          "size_bytes": 27917287424,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mixtral:text",
          "size_text": "26GB",
          "size_bytes": 27917287424,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mixtral:v0.1",
          "size_text": "80GB",
          "size_bytes": 85899345920,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "mixtral:8x7b",
          "size_text": "26GB",
          "size_bytes": 27917287424,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mixtral:8x22b",
          "size_text": "80GB",
          "size_bytes": 85899345920,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "mixtral:8x7b-instruct-v0.1-q2_K",
          "size_text": "17GB",
          "size_bytes": 18253611008,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mixtral:8x7b-instruct-v0.1-q3_K_S",
          "size_text": "20GB",
          "size_bytes": 21474836480,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mixtral:8x7b-instruct-v0.1-q3_K_M",
          "size_text": "23GB",
          "size_bytes": 24696061952,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mixtral:8x7b-instruct-v0.1-q3_K_L",
          "size_text": "24GB",
          "size_bytes": 25769803776,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mixtral:8x7b-instruct-v0.1-q4_0",
          "size_text": "26GB",
          "size_bytes": 27917287424,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mixtral:8x7b-instruct-v0.1-q4_1",
          "size_text": "29GB",
          "size_bytes": 31138512896,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mixtral:8x7b-instruct-v0.1-q4_K_S",
          "size_text": "27GB",
          "size_bytes": 28991029248,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mixtral:8x7b-instruct-v0.1-q4_K_M",
          "size_text": "28GB",
          "size_bytes": 30064771072,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mixtral:8x7b-instruct-v0.1-q5_0",
          "size_text": "32GB",
          "size_bytes": 34359738368,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mixtral:8x7b-instruct-v0.1-q5_1",
          "size_text": "35GB",
          "size_bytes": 37580963840,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mixtral:8x7b-instruct-v0.1-q5_K_S",
          "size_text": "32GB",
          "size_bytes": 34359738368,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mixtral:8x7b-instruct-v0.1-q5_K_M",
          "size_text": "33GB",
          "size_bytes": 35433480192,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mixtral:8x7b-instruct-v0.1-q6_K",
          "size_text": "38GB",
          "size_bytes": 40802189312,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mixtral:8x7b-instruct-v0.1-q8_0",
          "size_text": "50GB",
          "size_bytes": 53687091200,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mixtral:8x7b-instruct-v0.1-fp16",
          "size_text": "93GB",
          "size_bytes": 99857989632,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mixtral:8x7b-text",
          "size_text": "26GB",
          "size_bytes": 27917287424,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mixtral:8x7b-text-v0.1-q2_K",
          "size_text": "17GB",
          "size_bytes": 18253611008,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mixtral:8x7b-text-v0.1-q3_K_S",
          "size_text": "20GB",
          "size_bytes": 21474836480,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mixtral:8x7b-text-v0.1-q3_K_M",
          "size_text": "23GB",
          "size_bytes": 24696061952,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mixtral:8x7b-text-v0.1-q3_K_L",
          "size_text": "24GB",
          "size_bytes": 25769803776,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mixtral:8x7b-text-v0.1-q4_0",
          "size_text": "26GB",
          "size_bytes": 27917287424,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mixtral:8x7b-text-v0.1-q4_1",
          "size_text": "29GB",
          "size_bytes": 31138512896,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mixtral:8x7b-text-v0.1-q4_K_S",
          "size_text": "27GB",
          "size_bytes": 28991029248,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mixtral:8x7b-text-v0.1-q4_K_M",
          "size_text": "28GB",
          "size_bytes": 30064771072,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mixtral:8x7b-text-v0.1-q5_0",
          "size_text": "32GB",
          "size_bytes": 34359738368,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mixtral:8x7b-text-v0.1-q5_1",
          "size_text": "35GB",
          "size_bytes": 37580963840,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mixtral:8x7b-text-v0.1-q5_K_S",
          "size_text": "32GB",
          "size_bytes": 34359738368,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mixtral:8x7b-text-v0.1-q5_K_M",
          "size_text": "33GB",
          "size_bytes": 35433480192,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mixtral:8x7b-text-v0.1-q6_K",
          "size_text": "38GB",
          "size_bytes": 40802189312,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mixtral:8x7b-text-v0.1-q8_0",
          "size_text": "50GB",
          "size_bytes": 53687091200,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mixtral:8x7b-text-v0.1-fp16",
          "size_text": "93GB",
          "size_bytes": 99857989632,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mixtral:8x22b-instruct",
          "size_text": "80GB",
          "size_bytes": 85899345920,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "mixtral:8x22b-instruct-v0.1-q2_K",
          "size_text": "52GB",
          "size_bytes": 55834574848,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "mixtral:8x22b-instruct-v0.1-q3_K_S",
          "size_text": "62GB",
          "size_bytes": 66571993088,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "mixtral:8x22b-instruct-v0.1-q3_K_M",
          "size_text": "68GB",
          "size_bytes": 73014444032,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "mixtral:8x22b-instruct-v0.1-q3_K_L",
          "size_text": "73GB",
          "size_bytes": 78383153152,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "mixtral:8x22b-instruct-v0.1-q4_0",
          "size_text": "80GB",
          "size_bytes": 85899345920,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "mixtral:8x22b-instruct-v0.1-q4_1",
          "size_text": "88GB",
          "size_bytes": 94489280512,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "mixtral:8x22b-instruct-v0.1-q4_K_S",
          "size_text": "80GB",
          "size_bytes": 85899345920,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "mixtral:8x22b-instruct-v0.1-q4_K_M",
          "size_text": "86GB",
          "size_bytes": 92341796864,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "mixtral:8x22b-instruct-v0.1-q5_0",
          "size_text": "97GB",
          "size_bytes": 104152956928,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "mixtral:8x22b-instruct-v0.1-q5_1",
          "size_text": "106GB",
          "size_bytes": 113816633344,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "mixtral:8x22b-instruct-v0.1-q5_K_S",
          "size_text": "97GB",
          "size_bytes": 104152956928,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "mixtral:8x22b-instruct-v0.1-q5_K_M",
          "size_text": "100GB",
          "size_bytes": 107374182400,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "mixtral:8x22b-instruct-v0.1-q6_K",
          "size_text": "116GB",
          "size_bytes": 124554051584,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "mixtral:8x22b-instruct-v0.1-q8_0",
          "size_text": "149GB",
          "size_bytes": 159987531776,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "mixtral:8x22b-instruct-v0.1-fp16",
          "size_text": "281GB",
          "size_bytes": 301721452544,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "mixtral:8x22b-text",
          "size_text": "80GB",
          "size_bytes": 85899345920,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "mixtral:8x22b-text-v0.1-q2_K",
          "size_text": "52GB",
          "size_bytes": 55834574848,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "mixtral:8x22b-text-v0.1-q3_K_S",
          "size_text": "61GB",
          "size_bytes": 65498251264,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "mixtral:8x22b-text-v0.1-q3_K_M",
          "size_text": "68GB",
          "size_bytes": 73014444032,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "mixtral:8x22b-text-v0.1-q3_K_L",
          "size_text": "73GB",
          "size_bytes": 78383153152,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "mixtral:8x22b-text-v0.1-q4_0",
          "size_text": "80GB",
          "size_bytes": 85899345920,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "mixtral:8x22b-text-v0.1-q4_1",
          "size_text": "88GB",
          "size_bytes": 94489280512,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "mixtral:8x22b-text-v0.1-q4_K_S",
          "size_text": "80GB",
          "size_bytes": 85899345920,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "mixtral:8x22b-text-v0.1-q4_K_M",
          "size_text": "86GB",
          "size_bytes": 92341796864,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "mixtral:8x22b-text-v0.1-q5_0",
          "size_text": "97GB",
          "size_bytes": 104152956928,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "mixtral:8x22b-text-v0.1-q5_1",
          "size_text": "106GB",
          "size_bytes": 113816633344,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "mixtral:8x22b-text-v0.1-q5_K_S",
          "size_text": "97GB",
          "size_bytes": 104152956928,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "mixtral:8x22b-text-v0.1-q5_K_M",
          "size_text": "100GB",
          "size_bytes": 107374182400,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "mixtral:8x22b-text-v0.1-q6_K",
          "size_text": "116GB",
          "size_bytes": 124554051584,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "mixtral:8x22b-text-v0.1-q8_0",
          "size_text": "149GB",
          "size_bytes": 159987531776,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "mixtral:8x22b-text-v0.1-fp16",
          "size_text": "281GB",
          "size_bytes": 301721452544,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "mixtral:v0.1-instruct",
          "size_text": "80GB",
          "size_bytes": 85899345920,
          "context": "64K",
          "input": "Text"
        }
      ],
      "tags_count": 70
    },
    {
      "slug": "starcoder2",
      "capabilities": [],
      "pulls": 1400000,
      "pulls_text": "1.4M \n  Downloads",
      "blurb": "starcoder2 StarCoder2 is the next generation of transparently trained open code LLMs that comes in three sizes: 3B, 7B and 15B parameters. 3b 7b 15b 67 Tags Updated 1 year ago",
      "name": "StarCoder2 is the next generation of transparently trained open code LLMs that comes in three sizes: 3B, 7B and 15B parameters.",
      "description": "Readme Supporting a context window of up to 16,384 tokens, StarCoder2 is the next generation of transparently trained open code LLMs. starcoder2:instruct : a 15B model that follows natural and human-written instructions starcoder2:15b was trained on 600+ programming languages and 4+ trillion tokens. starcoder2:7b was trained on 17 programming languages and 3.5+ trillion tokens. starcoder2:3b was trained on 17 programming languages and 3+ trillion tokens. StarCoder2-15B is the best in its size class and matches 33B+ models on many evaluations. StarCoder2-3B matches the performance of StarCoder1-15B. References GitHub HuggingFace Paper Write Preview ![StarCode 2](https://github.com/ollama/ollama/assets/251292/5456bb29-de2a-4463-930c-3aaad585f430) Supporting a context window of up to 16,384 tokens, StarCoder2 is the next generation of transparently trained open code LLMs. * `starcoder2:instruct`: a 15B model that follows natural and human-written instructions * `starcoder2:15b` was trained on 600+ programming languages and 4+ trillion tokens. * `starcoder2:7b` was trained on 17 programming languages and 3.5+ trillion tokens. * `starcoder2:3b` was trained on 17 programming languages and 3+ trillion tokens. StarCoder2-15B is the best in its size class and matches 33B+ models on many evaluations. StarCoder2-3B matches the performance of StarCoder1-15B. ## References - [GitHub](https://github.com/bigcode-project/starcoder2) - [HuggingFace](https://huggingface.co/bigcode) - [Paper](https://arxiv.org/abs/2402.19173) Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "starcoder2:latest",
          "size_text": "1.7GB",
          "size_bytes": 1825361100,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "starcoder2:instruct",
          "size_text": "9.1GB",
          "size_bytes": 9771050598,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "starcoder2:3b",
          "size_text": "1.7GB",
          "size_bytes": 1825361100,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "starcoder2:7b",
          "size_text": "4.0GB",
          "size_bytes": 4294967296,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "starcoder2:15b",
          "size_text": "9.1GB",
          "size_bytes": 9771050598,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "starcoder2:3b-q2_K",
          "size_text": "1.1GB",
          "size_bytes": 1181116006,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "starcoder2:3b-q3_K_S",
          "size_text": "1.3GB",
          "size_bytes": 1395864371,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "starcoder2:3b-q3_K_M",
          "size_text": "1.5GB",
          "size_bytes": 1610612736,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "starcoder2:3b-q3_K_L",
          "size_text": "1.7GB",
          "size_bytes": 1825361100,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "starcoder2:3b-q4_0",
          "size_text": "1.7GB",
          "size_bytes": 1825361100,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "starcoder2:3b-q4_1",
          "size_text": "1.9GB",
          "size_bytes": 2040109465,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "starcoder2:3b-q4_K_S",
          "size_text": "1.7GB",
          "size_bytes": 1825361100,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "starcoder2:3b-q4_K_M",
          "size_text": "1.8GB",
          "size_bytes": 1932735283,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "starcoder2:3b-q5_0",
          "size_text": "2.1GB",
          "size_bytes": 2254857830,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "starcoder2:3b-q5_1",
          "size_text": "2.3GB",
          "size_bytes": 2469606195,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "starcoder2:3b-q5_K_S",
          "size_text": "2.1GB",
          "size_bytes": 2254857830,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "starcoder2:3b-q5_K_M",
          "size_text": "2.2GB",
          "size_bytes": 2362232012,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "starcoder2:3b-q6_K",
          "size_text": "2.5GB",
          "size_bytes": 2684354560,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "starcoder2:3b-q8_0",
          "size_text": "3.2GB",
          "size_bytes": 3435973836,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "starcoder2:3b-fp16",
          "size_text": "6.1GB",
          "size_bytes": 6549825126,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "starcoder2:7b-q2_K",
          "size_text": "2.7GB",
          "size_bytes": 2899102924,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "starcoder2:7b-q3_K_S",
          "size_text": "3.1GB",
          "size_bytes": 3328599654,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "starcoder2:7b-q3_K_M",
          "size_text": "3.6GB",
          "size_bytes": 3865470566,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "starcoder2:7b-q3_K_L",
          "size_text": "4.0GB",
          "size_bytes": 4294967296,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "starcoder2:7b-q4_0",
          "size_text": "4.0GB",
          "size_bytes": 4294967296,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "starcoder2:7b-q4_1",
          "size_text": "4.5GB",
          "size_bytes": 4831838208,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "starcoder2:7b-q4_K_S",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "starcoder2:7b-q4_K_M",
          "size_text": "4.4GB",
          "size_bytes": 4724464025,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "starcoder2:7b-q5_0",
          "size_text": "4.9GB",
          "size_bytes": 5261334937,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "starcoder2:7b-q5_1",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "starcoder2:7b-q5_K_S",
          "size_text": "4.9GB",
          "size_bytes": 5261334937,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "starcoder2:7b-q5_K_M",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "starcoder2:7b-q6_K",
          "size_text": "5.9GB",
          "size_bytes": 6335076761,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "starcoder2:7b-q8_0",
          "size_text": "7.6GB",
          "size_bytes": 8160437862,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "starcoder2:7b-fp16",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "starcoder2:15b-instruct",
          "size_text": "9.1GB",
          "size_bytes": 9771050598,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "starcoder2:15b-instruct-v0.1-q2_K",
          "size_text": "6.2GB",
          "size_bytes": 6657199308,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "starcoder2:15b-instruct-v0.1-q3_K_S",
          "size_text": "7.0GB",
          "size_bytes": 7516192768,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "starcoder2:15b-instruct-v0.1-q3_K_M",
          "size_text": "8.0GB",
          "size_bytes": 8589934592,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "starcoder2:15b-instruct-v0.1-q3_K_L",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "starcoder2:15b-instruct-q4_0",
          "size_text": "9.1GB",
          "size_bytes": 9771050598,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "starcoder2:15b-instruct-v0.1-q4_0",
          "size_text": "9.1GB",
          "size_bytes": 9771050598,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "starcoder2:15b-instruct-v0.1-q4_1",
          "size_text": "10GB",
          "size_bytes": 10737418240,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "starcoder2:15b-instruct-v0.1-q4_K_S",
          "size_text": "9.2GB",
          "size_bytes": 9878424780,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "starcoder2:15b-instruct-v0.1-q4_K_M",
          "size_text": "9.9GB",
          "size_bytes": 10630044057,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "starcoder2:15b-instruct-v0.1-q5_0",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "starcoder2:15b-instruct-v0.1-q5_1",
          "size_text": "12GB",
          "size_bytes": 12884901888,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "starcoder2:15b-instruct-v0.1-q5_K_S",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "starcoder2:15b-instruct-v0.1-q5_K_M",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "starcoder2:15b-instruct-v0.1-q6_K",
          "size_text": "13GB",
          "size_bytes": 13958643712,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "starcoder2:15b-instruct-v0.1-q8_0",
          "size_text": "17GB",
          "size_bytes": 18253611008,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "starcoder2:15b-instruct-v0.1-fp16",
          "size_text": "32GB",
          "size_bytes": 34359738368,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "starcoder2:15b-q2_K",
          "size_text": "6.2GB",
          "size_bytes": 6657199308,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "starcoder2:15b-q3_K_S",
          "size_text": "7.0GB",
          "size_bytes": 7516192768,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "starcoder2:15b-q3_K_M",
          "size_text": "8.1GB",
          "size_bytes": 8697308774,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "starcoder2:15b-q3_K_L",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "starcoder2:15b-q4_0",
          "size_text": "9.1GB",
          "size_bytes": 9771050598,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "starcoder2:15b-q4_1",
          "size_text": "10GB",
          "size_bytes": 10737418240,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "starcoder2:15b-q4_K_S",
          "size_text": "9.3GB",
          "size_bytes": 9985798963,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "starcoder2:15b-q4_K_M",
          "size_text": "9.9GB",
          "size_bytes": 10630044057,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "starcoder2:15b-q5_0",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "starcoder2:15b-q5_1",
          "size_text": "12GB",
          "size_bytes": 12884901888,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "starcoder2:15b-q5_K_S",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "starcoder2:15b-q5_K_M",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "starcoder2:15b-q6_K",
          "size_text": "13GB",
          "size_bytes": 13958643712,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "starcoder2:15b-q8_0",
          "size_text": "17GB",
          "size_bytes": 18253611008,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "starcoder2:15b-fp16",
          "size_text": "32GB",
          "size_bytes": 34359738368,
          "context": "16K",
          "input": "Text"
        }
      ],
      "tags_count": 67
    },
    {
      "slug": "codegemma",
      "capabilities": [
        "reasoning"
      ],
      "pulls": 1300000,
      "pulls_text": "1.3M \n  Downloads",
      "blurb": "codegemma CodeGemma is a collection of powerful, lightweight models that can perform a variety of coding tasks like fill-in-the-middle code completion, code generation, natural language understanding, mathematical reasoning, and instruction following. 2b 7b 85 Tags Updated 1 year ago",
      "name": "CodeGemma is a collection of powerful, lightweight models that can perform a variety of coding tasks like fill-in-the-middle code completion, code generation, natural language understanding, mathematical reasoning, and instruction following.",
      "description": "Readme CodeGemma is a collection of powerful, lightweight models that can perform a variety of coding tasks like fill-in-the-middle code completion, code generation, natural language understanding, mathematical reasoning, and instruction following. Variants: instruct a 7b instruction-tuned variant for natural language-to-code chat and instruction following code a 7b pretrained variant that specializes in code completion and generation from code prefixes and/or suffixes 2b a state of the art 2B pretrained variant that provides up to 2x faster code completion Advantages: Intelligent code completion and generation : Complete lines, functions, and even generate entire blocks of code, whether you’re working locally or using Google Cloud resources. Enhanced accuracy : Trained on 500 billion tokens of primarily English language data from web documents, mathematics, and code, CodeGemma models generate code that’s not only more syntactically correct but also semantically meaningful, reducing errors and debugging time. Multi-language proficiency : Supports Python, JavaScript, Java, Kotlin, C++, C#, Rust, Go, and other languages. Streamlined workflows : Integrate a CodeGemma model into your development environment to write less boilerplate and focus on interesting and differentiated code that matters, faster. Fill-in-the-middle CodeGemma models support fill-in-the-middle (FIM), for use in autocomplete or coding assistant tooling. Below is an example using the Ollama Python library: response = generate( model='codegemma:2b-code', prompt=f'<|fim_prefix|>{prefix}<|fim_suffix|>{suffix}<|fim_middle|>', options={ 'num_predict': 128, 'temperature': 0, 'top_p': 0.9, 'stop': ['<|file_separator|>'], }, ) References Hugging Face Report Write Preview <img src=\"https://github.com/ollama/ollama/assets/251292/56ffc5fc-0c30-4ab5-a0e3-65fc66de17bc\" width=\"320\" /> CodeGemma is a collection of powerful, lightweight models that can perform a variety of coding tasks like fill-in-the-middle code co",
      "variants": [
        {
          "tag": "codegemma:latest",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "codegemma:code",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "codegemma:instruct",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "codegemma:2b",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "codegemma:7b",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "codegemma:2b-code",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "codegemma:2b-code-q2_K",
          "size_text": "1.2GB",
          "size_bytes": 1288490188,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "codegemma:2b-code-v1.1-q2_K",
          "size_text": "1.2GB",
          "size_bytes": 1288490188,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "codegemma:2b-code-q3_K_S",
          "size_text": "1.3GB",
          "size_bytes": 1395864371,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "codegemma:2b-code-v1.1-q3_K_S",
          "size_text": "1.3GB",
          "size_bytes": 1395864371,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "codegemma:2b-code-q3_K_M",
          "size_text": "1.4GB",
          "size_bytes": 1503238553,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "codegemma:2b-code-v1.1-q3_K_M",
          "size_text": "1.4GB",
          "size_bytes": 1503238553,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "codegemma:2b-code-q3_K_L",
          "size_text": "1.5GB",
          "size_bytes": 1610612736,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "codegemma:2b-code-v1.1-q3_K_L",
          "size_text": "1.5GB",
          "size_bytes": 1610612736,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "codegemma:2b-code-q4_0",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "codegemma:2b-code-v1.1-q4_0",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "codegemma:2b-code-q4_1",
          "size_text": "1.7GB",
          "size_bytes": 1825361100,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "codegemma:2b-code-v1.1-q4_1",
          "size_text": "1.7GB",
          "size_bytes": 1825361100,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "codegemma:2b-code-q4_K_S",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "codegemma:2b-code-v1.1-q4_K_S",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "codegemma:2b-code-q4_K_M",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "codegemma:2b-code-v1.1-q4_K_M",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "codegemma:2b-code-q5_0",
          "size_text": "1.8GB",
          "size_bytes": 1932735283,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "codegemma:2b-code-v1.1-q5_0",
          "size_text": "1.8GB",
          "size_bytes": 1932735283,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "codegemma:2b-code-q5_1",
          "size_text": "1.9GB",
          "size_bytes": 2040109465,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "codegemma:2b-code-v1.1-q5_1",
          "size_text": "1.9GB",
          "size_bytes": 2040109465,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "codegemma:2b-code-q5_K_S",
          "size_text": "1.8GB",
          "size_bytes": 1932735283,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "codegemma:2b-code-v1.1-q5_K_S",
          "size_text": "1.8GB",
          "size_bytes": 1932735283,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "codegemma:2b-code-q5_K_M",
          "size_text": "1.8GB",
          "size_bytes": 1932735283,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "codegemma:2b-code-v1.1-q5_K_M",
          "size_text": "1.8GB",
          "size_bytes": 1932735283,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "codegemma:2b-code-q6_K",
          "size_text": "2.1GB",
          "size_bytes": 2254857830,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "codegemma:2b-code-v1.1-q6_K",
          "size_text": "2.1GB",
          "size_bytes": 2254857830,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "codegemma:2b-code-q8_0",
          "size_text": "2.7GB",
          "size_bytes": 2899102924,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "codegemma:2b-code-v1.1-q8_0",
          "size_text": "2.7GB",
          "size_bytes": 2899102924,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "codegemma:2b-code-fp16",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "codegemma:2b-code-v1.1-fp16",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "codegemma:2b-v1.1",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "codegemma:7b-code",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "codegemma:7b-code-q2_K",
          "size_text": "3.5GB",
          "size_bytes": 3758096384,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "codegemma:7b-code-q3_K_S",
          "size_text": "4.0GB",
          "size_bytes": 4294967296,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "codegemma:7b-code-q3_K_M",
          "size_text": "4.4GB",
          "size_bytes": 4724464025,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "codegemma:7b-code-q3_K_L",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "codegemma:7b-code-q4_0",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "codegemma:7b-code-q4_1",
          "size_text": "5.5GB",
          "size_bytes": 5905580032,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "codegemma:7b-code-q4_K_S",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "codegemma:7b-code-q4_K_M",
          "size_text": "5.3GB",
          "size_bytes": 5690831667,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "codegemma:7b-code-q5_0",
          "size_text": "6.0GB",
          "size_bytes": 6442450944,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "codegemma:7b-code-q5_1",
          "size_text": "6.5GB",
          "size_bytes": 6979321856,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "codegemma:7b-code-q5_K_S",
          "size_text": "6.0GB",
          "size_bytes": 6442450944,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "codegemma:7b-code-q5_K_M",
          "size_text": "6.1GB",
          "size_bytes": 6549825126,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "codegemma:7b-code-q6_K",
          "size_text": "7.0GB",
          "size_bytes": 7516192768,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "codegemma:7b-code-q8_0",
          "size_text": "9.1GB",
          "size_bytes": 9771050598,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "codegemma:7b-code-fp16",
          "size_text": "17GB",
          "size_bytes": 18253611008,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "codegemma:7b-instruct",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "codegemma:7b-instruct-q2_K",
          "size_text": "3.5GB",
          "size_bytes": 3758096384,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "codegemma:7b-instruct-v1.1-q2_K",
          "size_text": "3.5GB",
          "size_bytes": 3758096384,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "codegemma:7b-instruct-q3_K_S",
          "size_text": "4.0GB",
          "size_bytes": 4294967296,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "codegemma:7b-instruct-v1.1-q3_K_S",
          "size_text": "4.0GB",
          "size_bytes": 4294967296,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "codegemma:7b-instruct-q3_K_M",
          "size_text": "4.4GB",
          "size_bytes": 4724464025,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "codegemma:7b-instruct-v1.1-q3_K_M",
          "size_text": "4.4GB",
          "size_bytes": 4724464025,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "codegemma:7b-instruct-q3_K_L",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "codegemma:7b-instruct-v1.1-q3_K_L",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "codegemma:7b-instruct-q4_0",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "codegemma:7b-instruct-v1.1-q4_0",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "codegemma:7b-instruct-q4_1",
          "size_text": "5.5GB",
          "size_bytes": 5905580032,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "codegemma:7b-instruct-v1.1-q4_1",
          "size_text": "5.5GB",
          "size_bytes": 5905580032,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "codegemma:7b-instruct-q4_K_S",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "codegemma:7b-instruct-v1.1-q4_K_S",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "codegemma:7b-instruct-q4_K_M",
          "size_text": "5.3GB",
          "size_bytes": 5690831667,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "codegemma:7b-instruct-v1.1-q4_K_M",
          "size_text": "5.3GB",
          "size_bytes": 5690831667,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "codegemma:7b-instruct-q5_0",
          "size_text": "6.0GB",
          "size_bytes": 6442450944,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "codegemma:7b-instruct-v1.1-q5_0",
          "size_text": "6.0GB",
          "size_bytes": 6442450944,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "codegemma:7b-instruct-q5_1",
          "size_text": "6.5GB",
          "size_bytes": 6979321856,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "codegemma:7b-instruct-v1.1-q5_1",
          "size_text": "6.5GB",
          "size_bytes": 6979321856,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "codegemma:7b-instruct-q5_K_S",
          "size_text": "6.0GB",
          "size_bytes": 6442450944,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "codegemma:7b-instruct-v1.1-q5_K_S",
          "size_text": "6.0GB",
          "size_bytes": 6442450944,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "codegemma:7b-instruct-q5_K_M",
          "size_text": "6.1GB",
          "size_bytes": 6549825126,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "codegemma:7b-instruct-v1.1-q5_K_M",
          "size_text": "6.1GB",
          "size_bytes": 6549825126,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "codegemma:7b-instruct-q6_K",
          "size_text": "7.0GB",
          "size_bytes": 7516192768,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "codegemma:7b-instruct-v1.1-q6_K",
          "size_text": "7.0GB",
          "size_bytes": 7516192768,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "codegemma:7b-instruct-q8_0",
          "size_text": "9.1GB",
          "size_bytes": 9771050598,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "codegemma:7b-instruct-v1.1-q8_0",
          "size_text": "9.1GB",
          "size_bytes": 9771050598,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "codegemma:7b-instruct-fp16",
          "size_text": "17GB",
          "size_bytes": 18253611008,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "codegemma:7b-instruct-v1.1-fp16",
          "size_text": "17GB",
          "size_bytes": 18253611008,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "codegemma:7b-v1.1",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "8K",
          "input": "Text"
        }
      ],
      "tags_count": 85
    },
    {
      "slug": "llama2-uncensored",
      "capabilities": [],
      "pulls": 1300000,
      "pulls_text": "1.3M \n  Downloads",
      "blurb": "llama2-uncensored Uncensored Llama 2 model by George Sung and Jarrad Hope. 7b 70b 34 Tags Updated 2 years ago",
      "name": "Uncensored Llama 2 model by George Sung and Jarrad Hope.",
      "description": "Readme Llama 2 Uncensored is based on Meta’s Llama 2 model, and was created by George Sung and Jarrad Hope using the process defined by Eric Hartford in his blog post . CLI Open the terminal and run ollama run llama2-uncensored API Example: curl -X POST http://localhost:11434/api/generate -d '{ \"model\": \"llama2-uncensored\", \"prompt\":\"Write a recipe for dangerously spicy mayo.\" }' API documentation Memory requirements 7b models generally require at least 8GB of RAM 70b models generally require at least 64GB of RAM References https://erichartford.com/uncensored-models 7b parameters original source: George Sung 70b parameters original source: George Sung Write Preview Llama 2 Uncensored is based on Meta's Llama 2 model, and was created by George Sung and Jarrad Hope using the process defined by Eric Hartford in his [blog post](https://erichartford.com/uncensored-models). ### CLI Open the terminal and run `ollama run llama2-uncensored` ### API Example: ```bash curl -X POST http://localhost:11434/api/generate -d '{ \"model\": \"llama2-uncensored\", \"prompt\":\"Write a recipe for dangerously spicy mayo.\" }' ``` [API documentation](https://github.com/jmorganca/ollama/blob/main/docs/api.md) ## Memory requirements - 7b models generally require at least 8GB of RAM - 70b models generally require at least 64GB of RAM ## References [https://erichartford.com/uncensored-models](https://erichartford.com/uncensored-models ) 7b parameters original source: [George Sung](https://huggingface.co/georgesung/llama2_7b_chat_uncensored) 70b parameters original source: [George Sung](https://huggingface.co/jarradh/llama2_70b_chat_uncensored) Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "llama2-uncensored:latest",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "llama2-uncensored:7b",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "llama2-uncensored:70b",
          "size_text": "39GB",
          "size_bytes": 41875931136,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "llama2-uncensored:7b-chat",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "llama2-uncensored:7b-chat-q2_K",
          "size_text": "2.8GB",
          "size_bytes": 3006477107,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "llama2-uncensored:7b-chat-q3_K_S",
          "size_text": "2.9GB",
          "size_bytes": 3113851289,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "llama2-uncensored:7b-chat-q3_K_M",
          "size_text": "3.3GB",
          "size_bytes": 3543348019,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "llama2-uncensored:7b-chat-q3_K_L",
          "size_text": "3.6GB",
          "size_bytes": 3865470566,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "llama2-uncensored:7b-chat-q4_0",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "llama2-uncensored:7b-chat-q4_1",
          "size_text": "4.2GB",
          "size_bytes": 4509715660,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "llama2-uncensored:7b-chat-q4_K_S",
          "size_text": "3.9GB",
          "size_bytes": 4187593113,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "llama2-uncensored:7b-chat-q4_K_M",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "llama2-uncensored:7b-chat-q5_0",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "llama2-uncensored:7b-chat-q5_1",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "llama2-uncensored:7b-chat-q5_K_S",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "llama2-uncensored:7b-chat-q5_K_M",
          "size_text": "4.8GB",
          "size_bytes": 5153960755,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "llama2-uncensored:7b-chat-q6_K",
          "size_text": "5.5GB",
          "size_bytes": 5905580032,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "llama2-uncensored:7b-chat-q8_0",
          "size_text": "7.2GB",
          "size_bytes": 7730941132,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "llama2-uncensored:7b-chat-fp16",
          "size_text": "13GB",
          "size_bytes": 13958643712,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "llama2-uncensored:70b-chat",
          "size_text": "39GB",
          "size_bytes": 41875931136,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "llama2-uncensored:70b-chat-q2_K",
          "size_text": "29GB",
          "size_bytes": 31138512896,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "llama2-uncensored:70b-chat-q3_K_S",
          "size_text": "30GB",
          "size_bytes": 32212254720,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "llama2-uncensored:70b-chat-q3_K_M",
          "size_text": "33GB",
          "size_bytes": 35433480192,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "llama2-uncensored:70b-chat-q3_K_L",
          "size_text": "36GB",
          "size_bytes": 38654705664,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "llama2-uncensored:70b-chat-q4_0",
          "size_text": "39GB",
          "size_bytes": 41875931136,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "llama2-uncensored:70b-chat-q4_1",
          "size_text": "43GB",
          "size_bytes": 46170898432,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "llama2-uncensored:70b-chat-q4_K_S",
          "size_text": "39GB",
          "size_bytes": 41875931136,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "llama2-uncensored:70b-chat-q4_K_M",
          "size_text": "41GB",
          "size_bytes": 44023414784,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "llama2-uncensored:70b-chat-q5_0",
          "size_text": "47GB",
          "size_bytes": 50465865728,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "llama2-uncensored:70b-chat-q5_1",
          "size_text": "52GB",
          "size_bytes": 55834574848,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "llama2-uncensored:70b-chat-q5_K_S",
          "size_text": "47GB",
          "size_bytes": 50465865728,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "llama2-uncensored:70b-chat-q5_K_M",
          "size_text": "49GB",
          "size_bytes": 52613349376,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "llama2-uncensored:70b-chat-q6_K",
          "size_text": "57GB",
          "size_bytes": 61203283968,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "llama2-uncensored:70b-chat-q8_0",
          "size_text": "73GB",
          "size_bytes": 78383153152,
          "context": "2K",
          "input": "Text"
        }
      ],
      "tags_count": 34
    },
    {
      "slug": "deepseek-coder-v2",
      "capabilities": [],
      "pulls": 1200000,
      "pulls_text": "1.2M \n  Downloads",
      "blurb": "deepseek-coder-v2 An open-source Mixture-of-Experts code language model that achieves performance comparable to GPT4-Turbo in code-specific tasks. 16b 236b 64 Tags Updated 1 year ago",
      "name": "An open-source Mixture-of-Experts code language model that achieves performance comparable to GPT4-Turbo in code-specific tasks.",
      "description": "Readme DeepSeek-Coder-V2 is an open-source Mixture-of-Experts (MoE) code language model that achieves performance comparable to GPT4-Turbo in code-specific tasks. DeepSeek-Coder-V2 is further pre-trained from DeepSeek-Coder-V2-Base with 6 trillion tokens sourced from a high-quality and multi-source corpus. References Hugging Face Write Preview <img src=\"https://ollama.com/assets/library/deepseek-coder-v2/cce4988a-b798-4c63-82ab-14c5c09462fe\" width=\"280\" /> DeepSeek-Coder-V2 is an open-source Mixture-of-Experts (MoE) code language model that achieves performance comparable to GPT4-Turbo in code-specific tasks. DeepSeek-Coder-V2 is further pre-trained from DeepSeek-Coder-V2-Base with 6 trillion tokens sourced from a high-quality and multi-source corpus. ## References [Hugging Face](https://huggingface.co/deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct) Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "deepseek-coder-v2:latest",
          "size_text": "8.9GB",
          "size_bytes": 9556302233,
          "context": "160K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder-v2:lite",
          "size_text": "8.9GB",
          "size_bytes": 9556302233,
          "context": "160K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder-v2:16b",
          "size_text": "8.9GB",
          "size_bytes": 9556302233,
          "context": "160K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder-v2:236b",
          "size_text": "133GB",
          "size_bytes": 142807662592,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder-v2:16b-lite-base-q2_K",
          "size_text": "6.4GB",
          "size_bytes": 6871947673,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder-v2:16b-lite-base-q3_K_S",
          "size_text": "7.5GB",
          "size_bytes": 8053063680,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder-v2:16b-lite-base-q3_K_M",
          "size_text": "8.1GB",
          "size_bytes": 8697308774,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder-v2:16b-lite-base-q3_K_L",
          "size_text": "8.5GB",
          "size_bytes": 9126805504,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder-v2:16b-lite-base-q4_0",
          "size_text": "8.9GB",
          "size_bytes": 9556302233,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder-v2:16b-lite-base-q4_1",
          "size_text": "9.9GB",
          "size_bytes": 10630044057,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder-v2:16b-lite-base-q4_K_S",
          "size_text": "9.5GB",
          "size_bytes": 10200547328,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder-v2:16b-lite-base-q4_K_M",
          "size_text": "10GB",
          "size_bytes": 10737418240,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder-v2:16b-lite-base-q5_0",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder-v2:16b-lite-base-q5_1",
          "size_text": "12GB",
          "size_bytes": 12884901888,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder-v2:16b-lite-base-q5_K_S",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder-v2:16b-lite-base-q5_K_M",
          "size_text": "12GB",
          "size_bytes": 12884901888,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder-v2:16b-lite-base-q6_K",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder-v2:16b-lite-base-q8_0",
          "size_text": "17GB",
          "size_bytes": 18253611008,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder-v2:16b-lite-base-fp16",
          "size_text": "31GB",
          "size_bytes": 33285996544,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder-v2:16b-lite-instruct-q2_K",
          "size_text": "6.4GB",
          "size_bytes": 6871947673,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder-v2:16b-lite-instruct-q3_K_S",
          "size_text": "7.5GB",
          "size_bytes": 8053063680,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder-v2:16b-lite-instruct-q3_K_M",
          "size_text": "8.1GB",
          "size_bytes": 8697308774,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder-v2:16b-lite-instruct-q3_K_L",
          "size_text": "8.5GB",
          "size_bytes": 9126805504,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder-v2:16b-lite-instruct-q4_0",
          "size_text": "8.9GB",
          "size_bytes": 9556302233,
          "context": "160K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder-v2:16b-lite-instruct-q4_1",
          "size_text": "9.9GB",
          "size_bytes": 10630044057,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder-v2:16b-lite-instruct-q4_K_S",
          "size_text": "9.5GB",
          "size_bytes": 10200547328,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder-v2:16b-lite-instruct-q4_K_M",
          "size_text": "10GB",
          "size_bytes": 10737418240,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder-v2:16b-lite-instruct-q5_0",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder-v2:16b-lite-instruct-q5_1",
          "size_text": "12GB",
          "size_bytes": 12884901888,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder-v2:16b-lite-instruct-q5_K_S",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder-v2:16b-lite-instruct-q5_K_M",
          "size_text": "12GB",
          "size_bytes": 12884901888,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder-v2:16b-lite-instruct-q6_K",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder-v2:16b-lite-instruct-q8_0",
          "size_text": "17GB",
          "size_bytes": 18253611008,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder-v2:16b-lite-instruct-fp16",
          "size_text": "31GB",
          "size_bytes": 33285996544,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder-v2:236b-base-q2_K",
          "size_text": "86GB",
          "size_bytes": 92341796864,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder-v2:236b-base-q3_K_S",
          "size_text": "102GB",
          "size_bytes": 109521666048,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder-v2:236b-base-q3_K_M",
          "size_text": "113GB",
          "size_bytes": 121332826112,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder-v2:236b-base-q3_K_L",
          "size_text": "122GB",
          "size_bytes": 130996502528,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder-v2:236b-base-q4_0",
          "size_text": "133GB",
          "size_bytes": 142807662592,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder-v2:236b-base-q4_1",
          "size_text": "148GB",
          "size_bytes": 158913789952,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder-v2:236b-base-q4_K_S",
          "size_text": "134GB",
          "size_bytes": 143881404416,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder-v2:236b-base-q4_K_M",
          "size_text": "142GB",
          "size_bytes": 152471339008,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder-v2:236b-base-q5_0",
          "size_text": "162GB",
          "size_bytes": 173946175488,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder-v2:236b-base-q5_1",
          "size_text": "177GB",
          "size_bytes": 190052302848,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder-v2:236b-base-q5_K_S",
          "size_text": "162GB",
          "size_bytes": 173946175488,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder-v2:236b-base-q5_K_M",
          "size_text": "167GB",
          "size_bytes": 179314884608,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder-v2:236b-base-q6_K",
          "size_text": "194GB",
          "size_bytes": 208305913856,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder-v2:236b-base-q8_0",
          "size_text": "251GB",
          "size_bytes": 269509197824,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder-v2:236b-base-fp16",
          "size_text": "472GB",
          "size_bytes": 506806140928,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder-v2:236b-instruct-q2_K",
          "size_text": "86GB",
          "size_bytes": 92341796864,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder-v2:236b-instruct-q3_K_S",
          "size_text": "102GB",
          "size_bytes": 109521666048,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder-v2:236b-instruct-q3_K_M",
          "size_text": "113GB",
          "size_bytes": 121332826112,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder-v2:236b-instruct-q3_K_L",
          "size_text": "122GB",
          "size_bytes": 130996502528,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder-v2:236b-instruct-q4_0",
          "size_text": "133GB",
          "size_bytes": 142807662592,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder-v2:236b-instruct-q4_1",
          "size_text": "148GB",
          "size_bytes": 158913789952,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder-v2:236b-instruct-q4_K_S",
          "size_text": "134GB",
          "size_bytes": 143881404416,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder-v2:236b-instruct-q4_K_M",
          "size_text": "142GB",
          "size_bytes": 152471339008,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder-v2:236b-instruct-q5_0",
          "size_text": "162GB",
          "size_bytes": 173946175488,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder-v2:236b-instruct-q5_1",
          "size_text": "177GB",
          "size_bytes": 190052302848,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder-v2:236b-instruct-q5_K_S",
          "size_text": "162GB",
          "size_bytes": 173946175488,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder-v2:236b-instruct-q5_K_M",
          "size_text": "167GB",
          "size_bytes": 179314884608,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder-v2:236b-instruct-q6_K",
          "size_text": "194GB",
          "size_bytes": 208305913856,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder-v2:236b-instruct-q8_0",
          "size_text": "251GB",
          "size_bytes": 269509197824,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-coder-v2:236b-instruct-fp16",
          "size_text": "472GB",
          "size_bytes": 506806140928,
          "context": "4K",
          "input": "Text"
        }
      ],
      "tags_count": 64
    },
    {
      "slug": "falcon3",
      "capabilities": [],
      "pulls": 1100000,
      "pulls_text": "1.1M \n  Downloads",
      "blurb": "falcon3 A family of efficient AI models under 10B parameters performant in science, math, and coding through innovative training techniques. 1b 3b 7b 10b 17 Tags Updated 10 months ago",
      "name": "A family of efficient AI models under 10B parameters performant in science, math, and coding through innovative training techniques.",
      "description": "Readme Falcon3 represents TII’s latest advancement in efficient language models under 10B parameters, focused on enhancing science, math, and code capabilities while maintaining training efficiency. Key Features Four sizes: 1B, 3B, 7B, 10B Depth up-scaling technique used to create 10B model from 7B Knowledge distillation for smaller models (1B, 3B) Performance Highlights falcon3:1b outperforms smollm2:1.7b , matches gemma2:2b falcon3:10b achieves SOTA in under-13B category Extended context length up to 32K tokens (8K for 1B model) References Hugging Face Write Preview <img src=\"https://ollama.com/assets/library/falcon3/7ead93c7-c315-42ad-b49f-3aa370c3e7db\" width=\"425\" /> Falcon3 represents TII's latest advancement in efficient language models under 10B parameters, focused on enhancing science, math, and code capabilities while maintaining training efficiency. # Key Features - Four sizes: 1B, 3B, 7B, 10B - Depth up-scaling technique used to create 10B model from 7B - Knowledge distillation for smaller models (1B, 3B) # Performance Highlights - `falcon3:1b` outperforms `smollm2:1.7b`, matches `gemma2:2b` - `falcon3:10b` achieves SOTA in under-13B category - Extended context length up to 32K tokens (8K for 1B model) # References [Hugging Face](https://huggingface.co/blog/falcon3) Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "falcon3:latest",
          "size_text": "4.6GB",
          "size_bytes": 4939212390,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "falcon3:1b",
          "size_text": "1.8GB",
          "size_bytes": 1932735283,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "falcon3:3b",
          "size_text": "2.0GB",
          "size_bytes": 2147483648,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "falcon3:7b",
          "size_text": "4.6GB",
          "size_bytes": 4939212390,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "falcon3:10b",
          "size_text": "6.3GB",
          "size_bytes": 6764573491,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "falcon3:1b-instruct-q4_K_M",
          "size_text": "1.1GB",
          "size_bytes": 1181116006,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "falcon3:1b-instruct-q8_0",
          "size_text": "1.8GB",
          "size_bytes": 1932735283,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "falcon3:1b-instruct-fp16",
          "size_text": "3.3GB",
          "size_bytes": 3543348019,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "falcon3:3b-instruct-q4_K_M",
          "size_text": "2.0GB",
          "size_bytes": 2147483648,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "falcon3:3b-instruct-q8_0",
          "size_text": "3.4GB",
          "size_bytes": 3650722201,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "falcon3:3b-instruct-fp16",
          "size_text": "6.5GB",
          "size_bytes": 6979321856,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "falcon3:7b-instruct-q4_K_M",
          "size_text": "4.6GB",
          "size_bytes": 4939212390,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "falcon3:7b-instruct-q8_0",
          "size_text": "7.9GB",
          "size_bytes": 8482560409,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "falcon3:7b-instruct-fp16",
          "size_text": "15GB",
          "size_bytes": 16106127360,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "falcon3:10b-instruct-q4_K_M",
          "size_text": "6.3GB",
          "size_bytes": 6764573491,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "falcon3:10b-instruct-q8_0",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "falcon3:10b-instruct-fp16",
          "size_text": "21GB",
          "size_bytes": 22548578304,
          "context": "32K",
          "input": "Text"
        }
      ],
      "tags_count": 17
    },
    {
      "slug": "granite3.1-moe",
      "capabilities": [
        "tools"
      ],
      "pulls": 1000000,
      "pulls_text": "1M \n  Downloads",
      "blurb": "granite3.1-moe The IBM Granite 1B and 3B models are long-context mixture of experts (MoE) Granite models from IBM designed for low latency usage. 1b 3b 33 Tags Updated 9 months ago",
      "name": "The IBM Granite 1B and 3B models are long-context mixture of experts (MoE) Granite models from IBM designed for low latency usage.",
      "description": "Readme Granite mixture of experts models The IBM Granite 1B and 3B models are long-context mixture of experts (MoE) Granite models from IBM designed for low latency usage. The models are trained on over 10 trillion tokens of data, the Granite MoE models are ideal for deployment in on-device applications or situations requiring instantaneous inference. Parameter Sizes 1B: ollama run granite3.1-moe:1b 3B: ollama run granite3.1-moe:3b Supported Languages English, German, Spanish, French, Japanese, Portuguese, Arabic, Czech, Italian, Korean, Dutch, Chinese (Simplified) Capabilities Summarization Text classification Text extraction Question-answering Retrieval Augmented Generation (RAG) Code related tasks Function-calling tasks Multilingual dialog use cases Long-context tasks including long document/meeting summarization, long document QA, etc. Granite dense models The Granite dense models are available in 2B and 8B parameter sizes designed to support tool-based use cases and for retrieval augmented generation (RAG), streamlining code generation, translation and bug fixing. See model page Learn more Developers: IBM Research GitHub Repository: ibm-granite/granite-language-models Website : Granite Docs Release Date : December 18th, 2024 License: Apache 2.0 . Write Preview ## Granite mixture of experts models The IBM Granite **1B and 3B models** are long-context mixture of experts (MoE) Granite models from IBM designed for low latency usage. The models are trained on over 10 trillion tokens of data, the Granite MoE models are ideal for deployment in on-device applications or situations requiring instantaneous inference. ### Parameter Sizes **1B:** `ollama run granite3.1-moe:1b` **3B:** `ollama run granite3.1-moe:3b` ### Supported Languages English, German, Spanish, French, Japanese, Portuguese, Arabic, Czech, Italian, Korean, Dutch, Chinese (Simplified) ### Capabilities * Summarization * Text classification * Text extraction * Question-answering * Retrieval Augmented Genera",
      "variants": [
        {
          "tag": "granite3.1-moe:latest",
          "size_text": "2.0GB",
          "size_bytes": 2147483648,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "granite3.1-moe:1b",
          "size_text": "1.4GB",
          "size_bytes": 1503238553,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "granite3.1-moe:3b",
          "size_text": "2.0GB",
          "size_bytes": 2147483648,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "granite3.1-moe:1b-instruct-q2_K",
          "size_text": "524MB",
          "size_bytes": 549453824,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "granite3.1-moe:1b-instruct-q3_K_S",
          "size_text": "610MB",
          "size_bytes": 639631360,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "granite3.1-moe:1b-instruct-q3_K_M",
          "size_text": "671MB",
          "size_bytes": 703594496,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "granite3.1-moe:1b-instruct-q3_K_L",
          "size_text": "724MB",
          "size_bytes": 759169024,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "granite3.1-moe:1b-instruct-q4_0",
          "size_text": "781MB",
          "size_bytes": 818937856,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "granite3.1-moe:1b-instruct-q4_1",
          "size_text": "861MB",
          "size_bytes": 902823936,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "granite3.1-moe:1b-instruct-q4_K_S",
          "size_text": "787MB",
          "size_bytes": 825229312,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "granite3.1-moe:1b-instruct-q4_K_M",
          "size_text": "834MB",
          "size_bytes": 874512384,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "granite3.1-moe:1b-instruct-q5_0",
          "size_text": "941MB",
          "size_bytes": 986710016,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "granite3.1-moe:1b-instruct-q5_1",
          "size_text": "1.0GB",
          "size_bytes": 1073741824,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "granite3.1-moe:1b-instruct-q5_K_S",
          "size_text": "941MB",
          "size_bytes": 986710016,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "granite3.1-moe:1b-instruct-q5_K_M",
          "size_text": "969MB",
          "size_bytes": 1016070144,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "granite3.1-moe:1b-instruct-q6_K",
          "size_text": "1.1GB",
          "size_bytes": 1181116006,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "granite3.1-moe:1b-instruct-q8_0",
          "size_text": "1.4GB",
          "size_bytes": 1503238553,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "granite3.1-moe:1b-instruct-fp16",
          "size_text": "2.7GB",
          "size_bytes": 2899102924,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "granite3.1-moe:3b-instruct-q2_K",
          "size_text": "1.3GB",
          "size_bytes": 1395864371,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "granite3.1-moe:3b-instruct-q3_K_S",
          "size_text": "1.5GB",
          "size_bytes": 1610612736,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "granite3.1-moe:3b-instruct-q3_K_M",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "granite3.1-moe:3b-instruct-q3_K_L",
          "size_text": "1.8GB",
          "size_bytes": 1932735283,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "granite3.1-moe:3b-instruct-q4_0",
          "size_text": "1.9GB",
          "size_bytes": 2040109465,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "granite3.1-moe:3b-instruct-q4_1",
          "size_text": "2.1GB",
          "size_bytes": 2254857830,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "granite3.1-moe:3b-instruct-q4_K_S",
          "size_text": "1.9GB",
          "size_bytes": 2040109465,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "granite3.1-moe:3b-instruct-q4_K_M",
          "size_text": "2.0GB",
          "size_bytes": 2147483648,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "granite3.1-moe:3b-instruct-q5_0",
          "size_text": "2.3GB",
          "size_bytes": 2469606195,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "granite3.1-moe:3b-instruct-q5_1",
          "size_text": "2.5GB",
          "size_bytes": 2684354560,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "granite3.1-moe:3b-instruct-q5_K_S",
          "size_text": "2.3GB",
          "size_bytes": 2469606195,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "granite3.1-moe:3b-instruct-q5_K_M",
          "size_text": "2.4GB",
          "size_bytes": 2576980377,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "granite3.1-moe:3b-instruct-q6_K",
          "size_text": "2.7GB",
          "size_bytes": 2899102924,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "granite3.1-moe:3b-instruct-q8_0",
          "size_text": "3.5GB",
          "size_bytes": 3758096384,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "granite3.1-moe:3b-instruct-fp16",
          "size_text": "6.6GB",
          "size_bytes": 7086696038,
          "context": "128K",
          "input": "Text"
        }
      ],
      "tags_count": 33
    },
    {
      "slug": "qwen2.5vl",
      "capabilities": [
        "tools",
        "vision"
      ],
      "pulls": 994400,
      "pulls_text": "994.4K \n  Downloads",
      "blurb": "qwen2.5vl Flagship -language model of Qwen and also a significant leap from the previous Qwen2-VL. 3b 7b 32b 72b 17 Tags Updated 5 months ago",
      "name": "Flagship vision-language model of Qwen and also a significant leap from the previous Qwen2-VL.",
      "description": "Readme Note: this model requires Ollama 0.7.0. Download Ollama Qwen2.5-VL, the new flagship vision-language model of Qwen and also a significant leap from the previous Qwen2-VL. The key features include: Understand things visually: Qwen2.5-VL is not only proficient in recognizing common objects such as flowers, birds, fish, and insects, but it is highly capable of analyzing texts, charts, icons, graphics, and layouts within images. Being agentic: Qwen2.5-VL directly plays as a visual agent that can reason and dynamically direct tools, which is capable of computer use and phone use. Capable of visual localization in different formats: Qwen2.5-VL can accurately localize objects in an image by generating bounding boxes or points, and it can provide stable JSON outputs for coordinates and attributes. Generating structured outputs: for data like scans of invoices, forms, tables, etc. Qwen2.5-VL supports structured outputs of their contents, benefiting usages in finance, commerce, etc. Performance We evaluate our models with the SOTA models as well as the best models of similar model sizes. In terms of the flagship model Qwen2.5-VL-72B-Instruct, it achieves competitive performance in a series of benchmarks covering domains and tasks, including college-level problems, math, document understanding, general question answering, math, and visual agent. Notably, Qwen2.5-VL achieves significant advantages in understanding documents and diagrams, and it is capable of playing as a visual agent without task-specific fine tuning. In terms of smaller models, Qwen2.5-VL-7B-Instruct outperforms GPT-4o-mini in a number of tasks, and Qwen2.5-VL-3B, which is a solution for edge AI, even outperforms the 7B model of our previous version Qwen2-VL. References Blog Qwen2.5-VL Technical Report Write Preview > Note: this model requires Ollama 0.7.0. [Download Ollama](https://ollama.com/download) <img src=\"/assets/library/qwen2.5vl/7bd5aad3-d5c5-43b1-b915-1e85a378f1c4\" width=\"280\" /> Qwen2.5-VL, ",
      "variants": [
        {
          "tag": "qwen2.5vl:latest",
          "size_text": "6.0GB",
          "size_bytes": 6442450944,
          "context": "125K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5vl:3b",
          "size_text": "3.2GB",
          "size_bytes": 3435973836,
          "context": "125K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5vl:7b",
          "size_text": "6.0GB",
          "size_bytes": 6442450944,
          "context": "125K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5vl:32b",
          "size_text": "21GB",
          "size_bytes": 22548578304,
          "context": "125K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5vl:72b",
          "size_text": "49GB",
          "size_bytes": 52613349376,
          "context": "125K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5vl:3b-q4_K_M",
          "size_text": "3.2GB",
          "size_bytes": 3435973836,
          "context": "125K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5vl:3b-q8_0",
          "size_text": "4.6GB",
          "size_bytes": 4939212390,
          "context": "125K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5vl:3b-fp16",
          "size_text": "7.5GB",
          "size_bytes": 8053063680,
          "context": "125K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5vl:7b-q4_K_M",
          "size_text": "6.0GB",
          "size_bytes": 6442450944,
          "context": "125K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5vl:7b-q8_0",
          "size_text": "9.4GB",
          "size_bytes": 10093173145,
          "context": "125K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5vl:7b-fp16",
          "size_text": "17GB",
          "size_bytes": 18253611008,
          "context": "125K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5vl:32b-q4_K_M",
          "size_text": "21GB",
          "size_bytes": 22548578304,
          "context": "125K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5vl:32b-q8_0",
          "size_text": "36GB",
          "size_bytes": 38654705664,
          "context": "125K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5vl:32b-fp16",
          "size_text": "67GB",
          "size_bytes": 71940702208,
          "context": "125K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5vl:72b-q4_K_M",
          "size_text": "49GB",
          "size_bytes": 52613349376,
          "context": "125K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5vl:72b-q8_0",
          "size_text": "79GB",
          "size_bytes": 84825604096,
          "context": "125K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5vl:72b-fp16",
          "size_text": "147GB",
          "size_bytes": 157840048128,
          "context": "125K",
          "input": "Text"
        }
      ],
      "tags_count": 17
    },
    {
      "slug": "snowflake-arctic-embed",
      "capabilities": [
        "embedding"
      ],
      "pulls": 951500,
      "pulls_text": "951.5K \n  Downloads",
      "blurb": "snowflake-arctic-embed A suite of text models by Snowflake, optimized for performance. 22m 33m 110m 137m 335m 16 Tags Updated 1 year ago",
      "name": "A suite of text embedding models by Snowflake, optimized for performance.",
      "description": "Readme snowflake-arctic-embed is a suite of text embedding models that focuses on creating high-quality retrieval models optimized for performance. The models are trained by leveraging existing open-source text representation models, such as bert-base-uncased, and are trained in a multi-stage pipeline to optimize their retrieval performance. This model is available in 5 parameter sizes: snowflake-arctic-embed:335m (default) snowflake-arctic-embed:137m snowflake-arctic-embed:110m snowflake-arctic-embed:33m snowflake-arctic-embed:22m Reference Blog Post HuggingFace Write Preview <img src=\"https://github.com/ollama/ollama/assets/251292/115f484a-1911-46e9-902f-65a70f84e1d0\" width=\"320\" /> `snowflake-arctic-embed` is a suite of text embedding models that focuses on creating high-quality retrieval models optimized for performance. The models are trained by leveraging existing open-source text representation models, such as bert-base-uncased, and are trained in a multi-stage pipeline to optimize their retrieval performance. This model is available in 5 parameter sizes: * `snowflake-arctic-embed:335m` (default) * `snowflake-arctic-embed:137m` * `snowflake-arctic-embed:110m` * `snowflake-arctic-embed:33m` * `snowflake-arctic-embed:22m` ## Reference [Blog Post](https://www.snowflake.com/blog/introducing-snowflake-arctic-embed-snowflakes-state-of-the-art-text-embedding-family-of-models/) [HuggingFace](https://huggingface.co/Snowflake) Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "snowflake-arctic-embed:latest",
          "size_text": "669MB",
          "size_bytes": 701497344,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "snowflake-arctic-embed:l",
          "size_text": "669MB",
          "size_bytes": 701497344,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "snowflake-arctic-embed:m",
          "size_text": "219MB",
          "size_bytes": 229638144,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "snowflake-arctic-embed:s",
          "size_text": "67MB",
          "size_bytes": 70254592,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "snowflake-arctic-embed:xs",
          "size_text": "46MB",
          "size_bytes": 48234496,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "snowflake-arctic-embed:22m",
          "size_text": "46MB",
          "size_bytes": 48234496,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "snowflake-arctic-embed:33m",
          "size_text": "67MB",
          "size_bytes": 70254592,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "snowflake-arctic-embed:110m",
          "size_text": "219MB",
          "size_bytes": 229638144,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "snowflake-arctic-embed:137m",
          "size_text": "274MB",
          "size_bytes": 287309824,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "snowflake-arctic-embed:335m",
          "size_text": "669MB",
          "size_bytes": 701497344,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "snowflake-arctic-embed:22m-xs-fp16",
          "size_text": "46MB",
          "size_bytes": 48234496,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "snowflake-arctic-embed:33m-s-fp16",
          "size_text": "67MB",
          "size_bytes": 70254592,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "snowflake-arctic-embed:110m-m-fp16",
          "size_text": "219MB",
          "size_bytes": 229638144,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "snowflake-arctic-embed:137m-m-long-fp16",
          "size_text": "274MB",
          "size_bytes": 287309824,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "snowflake-arctic-embed:335m-l-fp16",
          "size_text": "669MB",
          "size_bytes": 701497344,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "snowflake-arctic-embed:m-long",
          "size_text": "274MB",
          "size_bytes": 287309824,
          "context": "2K",
          "input": "Text"
        }
      ],
      "tags_count": 16
    },
    {
      "slug": "orca-mini",
      "capabilities": [],
      "pulls": 943400,
      "pulls_text": "943.4K \n  Downloads",
      "blurb": "orca-mini A general-purpose model ranging from 3 billion parameters to 70 billion, suitable for entry-level hardware. 3b 7b 13b 70b 119 Tags Updated 2 years ago",
      "name": "A general-purpose model ranging from 3 billion parameters to 70 billion, suitable for entry-level hardware.",
      "description": "Readme Orca Mini is a Llama and Llama 2 model trained on Orca Style datasets created using the approaches defined in the paper, Orca: Progressive Learning from Complex Explanation Traces of GPT-4. There are two variations available. The original Orca Mini based on Llama in 3, 7, and 13 billion parameter sizes, and v3 based on Llama 2 in 7, 13, and 70 billion parameter sizes. Usage CLI Open the terminal and run ollama run orca-mini API Example: curl -X POST http://localhost:11434/api/generate -d '{ \"model\": \"orca-mini\", \"prompt\":\"Why is the sky blue?\" }' Memory requirements 7b models generally require at least 8GB of RAM 13b models generally require at least 16GB of RAM 70b models generally require at least 64GB of RAM Reference 3b parameters original source: Pankaj Mathur 7b parameters original source: Pankaj Mathur 13b parameters original source: Pankaj Mathur Orca Mini v3 source on Ollama 13b parameters original source: Pankaj Mathur 70b parameters source: Pankaj Mathur Orca: Progressive Learning from Complex Explanation Traces of GPT-4 Write Preview Orca Mini is a Llama and Llama 2 model trained on Orca Style datasets created using the approaches defined in the paper, Orca: Progressive Learning from Complex Explanation Traces of GPT-4. There are two variations available. The original Orca Mini based on Llama in 3, 7, and 13 billion parameter sizes, and v3 based on Llama 2 in 7, 13, and 70 billion parameter sizes. ## Usage ### CLI Open the terminal and run `ollama run orca-mini` ### API Example: ```bash curl -X POST http://localhost:11434/api/generate -d '{ \"model\": \"orca-mini\", \"prompt\":\"Why is the sky blue?\" }' ``` ## Memory requirements - 7b models generally require at least 8GB of RAM - 13b models generally require at least 16GB of RAM - 70b models generally require at least 64GB of RAM ## Reference 3b parameters original source: [Pankaj Mathur](https://huggingface.co/psmathur/orca_mini_3b) 7b parameters original source: [Pankaj Mathur](https://huggingface.co/",
      "variants": [
        {
          "tag": "orca-mini:latest",
          "size_text": "2.0GB",
          "size_bytes": 2147483648,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:3b",
          "size_text": "2.0GB",
          "size_bytes": 2147483648,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:7b",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:13b",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:70b",
          "size_text": "39GB",
          "size_bytes": 41875931136,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:3b-q4_0",
          "size_text": "2.0GB",
          "size_bytes": 2147483648,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:3b-q4_1",
          "size_text": "2.2GB",
          "size_bytes": 2362232012,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:3b-q5_0",
          "size_text": "2.4GB",
          "size_bytes": 2576980377,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:3b-q5_1",
          "size_text": "2.6GB",
          "size_bytes": 2791728742,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:3b-q8_0",
          "size_text": "3.6GB",
          "size_bytes": 3865470566,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:3b-fp16",
          "size_text": "6.9GB",
          "size_bytes": 7408818585,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:7b-v2-q2_K",
          "size_text": "2.8GB",
          "size_bytes": 3006477107,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:7b-v2-q3_K_S",
          "size_text": "2.9GB",
          "size_bytes": 3113851289,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:7b-v2-q3_K_M",
          "size_text": "3.3GB",
          "size_bytes": 3543348019,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:7b-v2-q3_K_L",
          "size_text": "3.6GB",
          "size_bytes": 3865470566,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:7b-v2-q4_0",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:7b-v2-q4_1",
          "size_text": "4.2GB",
          "size_bytes": 4509715660,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:7b-v2-q4_K_S",
          "size_text": "3.9GB",
          "size_bytes": 4187593113,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:7b-v2-q4_K_M",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:7b-v2-q5_0",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:7b-v2-q5_1",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:7b-v2-q5_K_S",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:7b-v2-q5_K_M",
          "size_text": "4.8GB",
          "size_bytes": 5153960755,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:7b-v2-q6_K",
          "size_text": "5.5GB",
          "size_bytes": 5905580032,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:7b-v2-q8_0",
          "size_text": "7.2GB",
          "size_bytes": 7730941132,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:7b-v2-fp16",
          "size_text": "13GB",
          "size_bytes": 13958643712,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:7b-v3",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:7b-v3-q2_K",
          "size_text": "2.8GB",
          "size_bytes": 3006477107,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:7b-v3-q3_K_S",
          "size_text": "2.9GB",
          "size_bytes": 3113851289,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:7b-v3-q3_K_M",
          "size_text": "3.3GB",
          "size_bytes": 3543348019,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:7b-v3-q3_K_L",
          "size_text": "3.6GB",
          "size_bytes": 3865470566,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:7b-v3-q4_0",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:7b-v3-q4_1",
          "size_text": "4.2GB",
          "size_bytes": 4509715660,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:7b-v3-q4_K_S",
          "size_text": "3.9GB",
          "size_bytes": 4187593113,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:7b-v3-q4_K_M",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:7b-v3-q5_0",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:7b-v3-q5_1",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:7b-v3-q5_K_S",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:7b-v3-q5_K_M",
          "size_text": "4.8GB",
          "size_bytes": 5153960755,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:7b-v3-q6_K",
          "size_text": "5.5GB",
          "size_bytes": 5905580032,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:7b-v3-q8_0",
          "size_text": "7.2GB",
          "size_bytes": 7730941132,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:7b-v3-fp16",
          "size_text": "13GB",
          "size_bytes": 13958643712,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:7b-q2_K",
          "size_text": "2.8GB",
          "size_bytes": 3006477107,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:7b-q3_K_S",
          "size_text": "2.9GB",
          "size_bytes": 3113851289,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:7b-q3_K_M",
          "size_text": "3.3GB",
          "size_bytes": 3543348019,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:7b-q3_K_L",
          "size_text": "3.6GB",
          "size_bytes": 3865470566,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:7b-q4_0",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:7b-q4_1",
          "size_text": "4.2GB",
          "size_bytes": 4509715660,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:7b-q4_K_S",
          "size_text": "3.9GB",
          "size_bytes": 4187593113,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:7b-q4_K_M",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:7b-q5_0",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:7b-q5_1",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:7b-q5_K_S",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:7b-q5_K_M",
          "size_text": "4.8GB",
          "size_bytes": 5153960755,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:7b-q6_K",
          "size_text": "5.5GB",
          "size_bytes": 5905580032,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:7b-q8_0",
          "size_text": "7.2GB",
          "size_bytes": 7730941132,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:7b-fp16",
          "size_text": "13GB",
          "size_bytes": 13958643712,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:13b-v2-q2_K",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:13b-v2-q3_K_S",
          "size_text": "5.7GB",
          "size_bytes": 6120328396,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:13b-v2-q3_K_M",
          "size_text": "6.3GB",
          "size_bytes": 6764573491,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:13b-v2-q3_K_L",
          "size_text": "6.9GB",
          "size_bytes": 7408818585,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:13b-v2-q4_0",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:13b-v2-q4_1",
          "size_text": "8.2GB",
          "size_bytes": 8804682956,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:13b-v2-q4_K_S",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:13b-v2-q4_K_M",
          "size_text": "7.9GB",
          "size_bytes": 8482560409,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:13b-v2-q5_0",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:13b-v2-q5_1",
          "size_text": "9.8GB",
          "size_bytes": 10522669875,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:13b-v2-q5_K_S",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:13b-v2-q5_K_M",
          "size_text": "9.2GB",
          "size_bytes": 9878424780,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:13b-v2-q6_K",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:13b-v2-q8_0",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:13b-v2-fp16",
          "size_text": "26GB",
          "size_bytes": 27917287424,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:13b-v3",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:13b-v3-q2_K",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:13b-v3-q3_K_S",
          "size_text": "5.7GB",
          "size_bytes": 6120328396,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:13b-v3-q3_K_M",
          "size_text": "6.3GB",
          "size_bytes": 6764573491,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:13b-v3-q3_K_L",
          "size_text": "6.9GB",
          "size_bytes": 7408818585,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:13b-v3-q4_0",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:13b-v3-q4_1",
          "size_text": "8.2GB",
          "size_bytes": 8804682956,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:13b-v3-q4_K_S",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:13b-v3-q4_K_M",
          "size_text": "7.9GB",
          "size_bytes": 8482560409,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:13b-v3-q5_0",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:13b-v3-q5_1",
          "size_text": "9.8GB",
          "size_bytes": 10522669875,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:13b-v3-q5_K_S",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:13b-v3-q5_K_M",
          "size_text": "9.2GB",
          "size_bytes": 9878424780,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:13b-v3-q6_K",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:13b-v3-q8_0",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:13b-v3-fp16",
          "size_text": "26GB",
          "size_bytes": 27917287424,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:13b-q2_K",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:13b-q3_K_S",
          "size_text": "5.7GB",
          "size_bytes": 6120328396,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:13b-q3_K_M",
          "size_text": "6.3GB",
          "size_bytes": 6764573491,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:13b-q3_K_L",
          "size_text": "6.9GB",
          "size_bytes": 7408818585,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:13b-q4_0",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:13b-q4_1",
          "size_text": "8.2GB",
          "size_bytes": 8804682956,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:13b-q4_K_S",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:13b-q4_K_M",
          "size_text": "7.9GB",
          "size_bytes": 8482560409,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:13b-q5_0",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:13b-q5_1",
          "size_text": "9.8GB",
          "size_bytes": 10522669875,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:13b-q5_K_S",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:13b-q5_K_M",
          "size_text": "9.2GB",
          "size_bytes": 9878424780,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:13b-q6_K",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:13b-q8_0",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:13b-fp16",
          "size_text": "26GB",
          "size_bytes": 27917287424,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:70b-v3",
          "size_text": "39GB",
          "size_bytes": 41875931136,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:70b-v3-q2_K",
          "size_text": "29GB",
          "size_bytes": 31138512896,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:70b-v3-q3_K_S",
          "size_text": "30GB",
          "size_bytes": 32212254720,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:70b-v3-q3_K_M",
          "size_text": "33GB",
          "size_bytes": 35433480192,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:70b-v3-q3_K_L",
          "size_text": "36GB",
          "size_bytes": 38654705664,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:70b-v3-q4_0",
          "size_text": "39GB",
          "size_bytes": 41875931136,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:70b-v3-q4_1",
          "size_text": "43GB",
          "size_bytes": 46170898432,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:70b-v3-q4_K_S",
          "size_text": "39GB",
          "size_bytes": 41875931136,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:70b-v3-q4_K_M",
          "size_text": "41GB",
          "size_bytes": 44023414784,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:70b-v3-q5_0",
          "size_text": "47GB",
          "size_bytes": 50465865728,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:70b-v3-q5_1",
          "size_text": "52GB",
          "size_bytes": 55834574848,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:70b-v3-q5_K_S",
          "size_text": "47GB",
          "size_bytes": 50465865728,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:70b-v3-q5_K_M",
          "size_text": "49GB",
          "size_bytes": 52613349376,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:70b-v3-q6_K",
          "size_text": "57GB",
          "size_bytes": 61203283968,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:70b-v3-q8_0",
          "size_text": "73GB",
          "size_bytes": 78383153152,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "orca-mini:70b-v3-fp16",
          "size_text": "138GB",
          "size_bytes": 148176371712,
          "context": "4K",
          "input": "Text"
        }
      ],
      "tags_count": 119
    },
    {
      "slug": "llama4",
      "capabilities": [
        "multimodal",
        "reasoning",
        "tools",
        "vision"
      ],
      "pulls": 757500,
      "pulls_text": "757.5K \n  Downloads",
      "blurb": "llama4 Meta's latest collection of models. 16x17b 128x17b 11 Tags Updated 4 months ago",
      "name": "Meta's latest collection of multimodal models.",
      "description": "Readme The Llama 4 collection of models are natively multimodal AI models that enable text and multimodal experiences. These two models leverage a mixture-of-experts (MoE) architecture and support native multimodality (image input). Supported languages: Arabic, English, French, German, Hindi, Indonesian, Italian, Portuguese, Spanish, Tagalog, Thai, and Vietnamese. Input: multilingual text, image Output: multilingual text, code Models Llama 4 Scout ollama run llama4:scout 109B parameter MoE model with 17B active parameters Llama 4 Maverick ollama run llama4:maverick 400B parameter MoE model with 17B active parameters Intended Use Intended Use Cases: Llama 4 is intended for commercial and research use in multiple languages. Instruction tuned models are intended for assistant-like chat and visual reasoning tasks, whereas pretrained models can be adapted for natural language generation. For vision, Llama 4 models are also optimized for visual recognition, image reasoning, captioning, and answering general questions about an image. The Llama 4 model collection also supports the ability to leverage the outputs of its models to improve other models including synthetic data generation and distillation. The Llama 4 Community License allows for these use cases. Out-of-scope : Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 4 Community License. Use in languages or capabilities beyond those explicitly referenced as supported in this model card. Note: Llama 4 has been trained on a broader collection of languages than the 12 supported languages (pre-training includes 200 total languages ). Developers may fine-tune Llama 4 models for languages beyond the 12 supported languages provided they comply with the Llama 4 Community License and the Acceptable Use Policy. Developers are responsible for ensuring that their use of Llama 4 in additional languages is ",
      "variants": [
        {
          "tag": "llama4:latest",
          "size_text": "67GB",
          "size_bytes": 71940702208,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "llama4:maverick",
          "size_text": "245GB",
          "size_bytes": 263066746880,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "llama4:scout",
          "size_text": "67GB",
          "size_bytes": 71940702208,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "llama4:16x17b",
          "size_text": "67GB",
          "size_bytes": 71940702208,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "llama4:128x17b",
          "size_text": "245GB",
          "size_bytes": 263066746880,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "llama4:17b-maverick-128e-instruct-q4_K_M",
          "size_text": "245GB",
          "size_bytes": 263066746880,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "llama4:17b-maverick-128e-instruct-q8_0",
          "size_text": "428GB",
          "size_bytes": 459561500672,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "llama4:17b-maverick-128e-instruct-fp16",
          "size_text": "803GB",
          "size_bytes": 862214684672,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "llama4:17b-scout-16e-instruct-q4_K_M",
          "size_text": "67GB",
          "size_bytes": 71940702208,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "llama4:17b-scout-16e-instruct-q8_0",
          "size_text": "117GB",
          "size_bytes": 125627793408,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "llama4:17b-scout-16e-instruct-fp16",
          "size_text": "217GB",
          "size_bytes": 233001975808,
          "context": null,
          "input": "Text"
        }
      ],
      "tags_count": 11
    },
    {
      "slug": "phi",
      "capabilities": [
        "reasoning"
      ],
      "pulls": 731500,
      "pulls_text": "731.5K \n  Downloads",
      "blurb": "phi Phi-2: a 2.7B language model by Microsoft Research that demonstrates outstanding and language understanding capabilities. 2.7b 18 Tags Updated 1 year ago",
      "name": "Phi-2: a 2.7B language model by Microsoft Research that demonstrates outstanding reasoning and language understanding capabilities.",
      "description": "Readme Phi-2 is a small language model capable of common-sense reasoning and language understanding. It showcases “state-of-the-art performance” among language models with less than 13 billion parameters. Example prompt By default, phi includes a chat prompt template designed for multi-turn conversations: % ollama run phi >>> Hello, can you help me find my way to Toronto? Certainly! What is the exact location in Toronto that you are looking for? >>> Yonge & Bloor Sure, Yonge and Bloor is a busy intersection in downtown Toronto. Would you like to take public transportation or drive there? >>> Public transportation Great! The easiest way to get there is by taking the TTC subway. You can take Line 1, which runs along Yonge Street and passes through downtown Toronto. Using Ollama’s API: curl http://localhost:11434/api/chat -d '{ \"model\": \"phi\", \"messages\": [ { \"role\": \"user\", \"content\": \"why is the sky blue?\" } ] }' Example prompts (raw mode) Phi also responds well to a wide variety of prompt formats when using raw mode in Ollama’s API, which bypasses all default prompt templating: Instruct curl http://localhost:11434/api/generate -d '{ \"model\": \"phi\", \"prompt\": \"Instruct: Write a detailed analogy between mathematics and a lighthouse.\\nOutput:\", \"options\": { \"stop\": [\"Instruct:\", \"Output:\"] }, \"raw\": true, \"stream\": false }' Code Completion curl http://localhost:11434/api/generate -d '{ \"model\": \"phi\", \"prompt\": \"def print_prime(n):\\n \", \"raw\": true, \"stream\": false }' Text completion curl http://localhost:11434/api/generate -d '{ \"model\": \"phi\", \"prompt\": \"There once was a mouse named\", \"raw\": true, \"stream\": false }' References HuggingFace Blog Post Write Preview <img src=\"https://github.com/jmorganca/ollama/assets/251292/e06e1a36-97b2-417a-b2b2-028c980359b1\" width=\"240\" /> Phi-2 is a small language model capable of common-sense reasoning and language understanding. It showcases \"state-of-the-art performance\" among language models with less than 13 billion parameters.",
      "variants": [
        {
          "tag": "phi:latest",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "phi:chat",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "phi:2.7b",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "phi:2.7b-chat-v2-q2_K",
          "size_text": "1.2GB",
          "size_bytes": 1288490188,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "phi:2.7b-chat-v2-q3_K_S",
          "size_text": "1.3GB",
          "size_bytes": 1395864371,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "phi:2.7b-chat-v2-q3_K_M",
          "size_text": "1.5GB",
          "size_bytes": 1610612736,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "phi:2.7b-chat-v2-q3_K_L",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "phi:2.7b-chat-v2-q4_0",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "phi:2.7b-chat-v2-q4_1",
          "size_text": "1.8GB",
          "size_bytes": 1932735283,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "phi:2.7b-chat-v2-q4_K_S",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "phi:2.7b-chat-v2-q4_K_M",
          "size_text": "1.8GB",
          "size_bytes": 1932735283,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "phi:2.7b-chat-v2-q5_0",
          "size_text": "1.9GB",
          "size_bytes": 2040109465,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "phi:2.7b-chat-v2-q5_1",
          "size_text": "2.1GB",
          "size_bytes": 2254857830,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "phi:2.7b-chat-v2-q5_K_S",
          "size_text": "1.9GB",
          "size_bytes": 2040109465,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "phi:2.7b-chat-v2-q5_K_M",
          "size_text": "2.1GB",
          "size_bytes": 2254857830,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "phi:2.7b-chat-v2-q6_K",
          "size_text": "2.3GB",
          "size_bytes": 2469606195,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "phi:2.7b-chat-v2-q8_0",
          "size_text": "3.0GB",
          "size_bytes": 3221225472,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "phi:2.7b-chat-v2-fp16",
          "size_text": "5.6GB",
          "size_bytes": 6012954214,
          "context": "2K",
          "input": "Text"
        }
      ],
      "tags_count": 18
    },
    {
      "slug": "mistral-small3.2",
      "capabilities": [
        "tools",
        "vision"
      ],
      "pulls": 722400,
      "pulls_text": "722.4K \n  Downloads",
      "blurb": "mistral-small3.2 An update to Mistral Small that improves on function calling, instruction following, and less repetition errors. 24b 5 Tags Updated 4 months ago",
      "name": "An update to Mistral Small that improves on function calling, instruction following, and less repetition errors.",
      "description": "Readme Mistral-Small-3.2-24B-Instruct-2506 is a minor update of Mistral-Small-3.1-24B-Instruct-2503. Small-3.2 improves in the following categories: Instruction following : Small-3.2 is better at following precise instructions Repetition errors : Small-3.2 produces less infinite generations or repetitive answers Function calling : Small-3.2’s function calling template is more robust (see here and examples) In all other categories Small-3.2 should match or slightly improve compared to Mistral-Small-3.1-24B-Instruct-2503. References Hugging Face Write Preview <img src=\"/assets/library/mistral-small3.2/cec4138e-e20c-4d5b-910f-56f8647dbda1\" width=\"140\" /> Mistral-Small-3.2-24B-Instruct-2506 is a minor update of Mistral-Small-3.1-24B-Instruct-2503. Small-3.2 improves in the following categories: * **Instruction following**: Small-3.2 is better at following precise instructions * **Repetition errors**: Small-3.2 produces less infinite generations or repetitive answers * **Function calling**: Small-3.2's function calling template is more robust (see here and examples) In all other categories Small-3.2 should match or slightly improve compared to Mistral-Small-3.1-24B-Instruct-2503. ## References [Hugging Face](https://huggingface.co/mistralai/Mistral-Small-3.2-24B-Instruct-2506) Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "mistral-small3.2:latest",
          "size_text": "15GB",
          "size_bytes": 16106127360,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "mistral-small3.2:24b",
          "size_text": "15GB",
          "size_bytes": 16106127360,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "mistral-small3.2:24b-instruct-2506-q4_K_M",
          "size_text": "15GB",
          "size_bytes": 16106127360,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "mistral-small3.2:24b-instruct-2506-q8_0",
          "size_text": "26GB",
          "size_bytes": 27917287424,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "mistral-small3.2:24b-instruct-2506-fp16",
          "size_text": "48GB",
          "size_bytes": 51539607552,
          "context": "128K",
          "input": "Text"
        }
      ],
      "tags_count": 5
    },
    {
      "slug": "dolphin-mixtral",
      "capabilities": [],
      "pulls": 717900,
      "pulls_text": "717.9K \n  Downloads",
      "blurb": "dolphin-mixtral Uncensored, 8x7b and 8x22b fine-tuned models based on the Mixtral mixture of experts models that excels at coding tasks. Created by Eric Hartford. 8x7b 8x22b 70 Tags Updated 10 months ago",
      "name": "Uncensored, 8x7b and 8x22b fine-tuned models based on the Mixtral mixture of experts models that excels at coding tasks. Created by Eric Hartford.",
      "description": "Readme The Dolphin model by Eric Hartford based on Mixtral that is trained with additional datasets: Synthia, OpenHermes and PureDove New Dolphin-Coder MagiCoder Sizes dolphin-mixtral:8x22b dolphin-mixtral:8x7b References HuggingFace Write Preview <img src=\"https://github.com/jmorganca/ollama/assets/251292/c65aad4e-7a58-4645-97f2-5e8ca158275f\" width=\"280\" /> The Dolphin model by Eric Hartford based on [Mixtral](https://ollama.ai/library/mixtral) that is trained with additional datasets: * Synthia, OpenHermes and PureDove * New Dolphin-Coder * MagiCoder ### Sizes * `dolphin-mixtral:8x22b` * `dolphin-mixtral:8x7b` ## References [HuggingFace](https://huggingface.co/ehartford/dolphin-2.5-mixtral-8x7b) Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "dolphin-mixtral:latest",
          "size_text": "26GB",
          "size_bytes": 27917287424,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mixtral:v2.5",
          "size_text": "26GB",
          "size_bytes": 27917287424,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mixtral:v2.6",
          "size_text": "26GB",
          "size_bytes": 27917287424,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mixtral:v2.7",
          "size_text": "26GB",
          "size_bytes": 27917287424,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mixtral:8x7b",
          "size_text": "26GB",
          "size_bytes": 27917287424,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mixtral:8x22b",
          "size_text": "80GB",
          "size_bytes": 85899345920,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mixtral:8x7b-v2.5",
          "size_text": "26GB",
          "size_bytes": 27917287424,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mixtral:8x7b-v2.5-q2_K",
          "size_text": "17GB",
          "size_bytes": 18253611008,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mixtral:8x7b-v2.5-q3_K_S",
          "size_text": "20GB",
          "size_bytes": 21474836480,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mixtral:8x7b-v2.5-q3_K_M",
          "size_text": "23GB",
          "size_bytes": 24696061952,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mixtral:8x7b-v2.5-q3_K_L",
          "size_text": "24GB",
          "size_bytes": 25769803776,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mixtral:8x7b-v2.5-q4_0",
          "size_text": "26GB",
          "size_bytes": 27917287424,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mixtral:8x7b-v2.5-q4_1",
          "size_text": "29GB",
          "size_bytes": 31138512896,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mixtral:8x7b-v2.5-q4_K_S",
          "size_text": "27GB",
          "size_bytes": 28991029248,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mixtral:8x7b-v2.5-q4_K_M",
          "size_text": "28GB",
          "size_bytes": 30064771072,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mixtral:8x7b-v2.5-q5_0",
          "size_text": "32GB",
          "size_bytes": 34359738368,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mixtral:8x7b-v2.5-q5_1",
          "size_text": "35GB",
          "size_bytes": 37580963840,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mixtral:8x7b-v2.5-q5_K_S",
          "size_text": "32GB",
          "size_bytes": 34359738368,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mixtral:8x7b-v2.5-q5_K_M",
          "size_text": "33GB",
          "size_bytes": 35433480192,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mixtral:8x7b-v2.5-q6_K",
          "size_text": "38GB",
          "size_bytes": 40802189312,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mixtral:8x7b-v2.5-q8_0",
          "size_text": "50GB",
          "size_bytes": 53687091200,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mixtral:8x7b-v2.5-fp16",
          "size_text": "93GB",
          "size_bytes": 99857989632,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mixtral:8x7b-v2.6",
          "size_text": "26GB",
          "size_bytes": 27917287424,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mixtral:8x7b-v2.6-q2_K",
          "size_text": "17GB",
          "size_bytes": 18253611008,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mixtral:8x7b-v2.6-q3_K_S",
          "size_text": "20GB",
          "size_bytes": 21474836480,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mixtral:8x7b-v2.6-q3_K_M",
          "size_text": "23GB",
          "size_bytes": 24696061952,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mixtral:8x7b-v2.6-q3_K_L",
          "size_text": "24GB",
          "size_bytes": 25769803776,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mixtral:8x7b-v2.6-q4_0",
          "size_text": "26GB",
          "size_bytes": 27917287424,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mixtral:8x7b-v2.6-q4_1",
          "size_text": "29GB",
          "size_bytes": 31138512896,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mixtral:8x7b-v2.6-q4_K_S",
          "size_text": "27GB",
          "size_bytes": 28991029248,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mixtral:8x7b-v2.6-q4_K_M",
          "size_text": "28GB",
          "size_bytes": 30064771072,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mixtral:8x7b-v2.6-q5_0",
          "size_text": "32GB",
          "size_bytes": 34359738368,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mixtral:8x7b-v2.6-q5_1",
          "size_text": "35GB",
          "size_bytes": 37580963840,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mixtral:8x7b-v2.6-q5_K_S",
          "size_text": "32GB",
          "size_bytes": 34359738368,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mixtral:8x7b-v2.6-q5_K_M",
          "size_text": "33GB",
          "size_bytes": 35433480192,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mixtral:8x7b-v2.6-q6_K",
          "size_text": "38GB",
          "size_bytes": 40802189312,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mixtral:8x7b-v2.6-q8_0",
          "size_text": "50GB",
          "size_bytes": 53687091200,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mixtral:8x7b-v2.6-fp16",
          "size_text": "93GB",
          "size_bytes": 99857989632,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mixtral:8x7b-v2.7",
          "size_text": "26GB",
          "size_bytes": 27917287424,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mixtral:8x7b-v2.7-q2_K",
          "size_text": "17GB",
          "size_bytes": 18253611008,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mixtral:8x7b-v2.7-q3_K_S",
          "size_text": "20GB",
          "size_bytes": 21474836480,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mixtral:8x7b-v2.7-q3_K_M",
          "size_text": "23GB",
          "size_bytes": 24696061952,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mixtral:8x7b-v2.7-q3_K_L",
          "size_text": "24GB",
          "size_bytes": 25769803776,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mixtral:8x7b-v2.7-q4_0",
          "size_text": "26GB",
          "size_bytes": 27917287424,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mixtral:8x7b-v2.7-q4_1",
          "size_text": "29GB",
          "size_bytes": 31138512896,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mixtral:8x7b-v2.7-q4_K_S",
          "size_text": "27GB",
          "size_bytes": 28991029248,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mixtral:8x7b-v2.7-q4_K_M",
          "size_text": "28GB",
          "size_bytes": 30064771072,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mixtral:8x7b-v2.7-q5_0",
          "size_text": "32GB",
          "size_bytes": 34359738368,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mixtral:8x7b-v2.7-q5_1",
          "size_text": "35GB",
          "size_bytes": 37580963840,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mixtral:8x7b-v2.7-q5_K_S",
          "size_text": "32GB",
          "size_bytes": 34359738368,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mixtral:8x7b-v2.7-q5_K_M",
          "size_text": "33GB",
          "size_bytes": 35433480192,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mixtral:8x7b-v2.7-q6_K",
          "size_text": "38GB",
          "size_bytes": 40802189312,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mixtral:8x7b-v2.7-q8_0",
          "size_text": "50GB",
          "size_bytes": 53687091200,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mixtral:8x7b-v2.7-fp16",
          "size_text": "93GB",
          "size_bytes": 99857989632,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mixtral:8x22b-v2.9",
          "size_text": "80GB",
          "size_bytes": 85899345920,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mixtral:8x22b-v2.9-q2_K",
          "size_text": "52GB",
          "size_bytes": 55834574848,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mixtral:8x22b-v2.9-q3_K_S",
          "size_text": "61GB",
          "size_bytes": 65498251264,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mixtral:8x22b-v2.9-q3_K_M",
          "size_text": "68GB",
          "size_bytes": 73014444032,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mixtral:8x22b-v2.9-q3_K_L",
          "size_text": "73GB",
          "size_bytes": 78383153152,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mixtral:8x22b-v2.9-q4_0",
          "size_text": "80GB",
          "size_bytes": 85899345920,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mixtral:8x22b-v2.9-q4_1",
          "size_text": "88GB",
          "size_bytes": 94489280512,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mixtral:8x22b-v2.9-q4_K_S",
          "size_text": "80GB",
          "size_bytes": 85899345920,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mixtral:8x22b-v2.9-q4_K_M",
          "size_text": "86GB",
          "size_bytes": 92341796864,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mixtral:8x22b-v2.9-q5_0",
          "size_text": "97GB",
          "size_bytes": 104152956928,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mixtral:8x22b-v2.9-q5_1",
          "size_text": "106GB",
          "size_bytes": 113816633344,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mixtral:8x22b-v2.9-q5_K_S",
          "size_text": "97GB",
          "size_bytes": 104152956928,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mixtral:8x22b-v2.9-q5_K_M",
          "size_text": "100GB",
          "size_bytes": 107374182400,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mixtral:8x22b-v2.9-q6_K",
          "size_text": "116GB",
          "size_bytes": 124554051584,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mixtral:8x22b-v2.9-q8_0",
          "size_text": "149GB",
          "size_bytes": 159987531776,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mixtral:8x22b-v2.9-fp16",
          "size_text": "281GB",
          "size_bytes": 301721452544,
          "context": "64K",
          "input": "Text"
        }
      ],
      "tags_count": 70
    },
    {
      "slug": "gemma3n",
      "capabilities": [
        "reasoning",
        "tools"
      ],
      "pulls": 685300,
      "pulls_text": "685.3K \n  Downloads",
      "blurb": "gemma3n Gemma 3n models are designed for efficient execution on everyday devices such as laptops, tablets or phones. e2b e4b 9 Tags Updated 4 months ago",
      "name": "Gemma 3n models are designed for efficient execution on everyday devices such as laptops, tablets or phones.",
      "description": "Readme Gemma 3n models are designed for efficient execution on everyday devices such as laptops, tablets or phones. These models were trained with data in over 140 spoken languages. Gemma 3n models use selective parameter activation technology to reduce resource requirements. This technique allows the models to operate at an effective size of 2B and 4B parameters, which is lower than the total number of parameters they contain. Models Effective 2B ollama run gemma3n:e2b Effective 4B ollama run gemma3n:e4b Evaluation Model evaluation metrics and results. Benchmark Results These models were evaluated at full precision (float32) against a large collection of different datasets and metrics to cover different aspects of content generation. Evaluation results marked with IT are for instruction-tuned models. Evaluation results marked with PT are for pre-trained models. The models available on Ollama are instruction-tuned models. Reasoning and factuality Benchmark Metric n-shot E2B PT E4B PT HellaSwag Accuracy 10-shot 72.2 78.6 BoolQ Accuracy 0-shot 76.4 81.6 PIQA Accuracy 0-shot 78.9 81.0 SocialIQA Accuracy 0-shot 48.8 50.0 TriviaQA Accuracy 5-shot 60.8 70.2 Natural Questions Accuracy 5-shot 15.5 20.9 ARC-c Accuracy 25-shot 51.7 61.6 ARC-e Accuracy 0-shot 75.8 81.6 WinoGrande Accuracy 5-shot 66.8 71.7 BIG-Bench Hard Accuracy few-shot 44.3 52.9 DROP Token F1 score 1-shot 53.9 60.8 Multilingual Benchmark Metric n-shot E2B IT E4B IT MGSM Accuracy 0-shot 53.1 60.7 WMT24++ (ChrF) Character-level F-score 0-shot 42.7 50.1 Include Accuracy 0-shot 38.6 57.2 MMLU (ProX) Accuracy 0-shot 8.1 19.9 OpenAI MMLU Accuracy 0-shot 22.3 35.6 Global-MMLU Accuracy 0-shot 55.1 60.3 ECLeKTic ECLeKTic score 0-shot 2.5 1.9 STEM and code Benchmark Metric n-shot E2B IT E4B IT GPQA Diamond RelaxedAccuracy/accuracy 0-shot 24.8 23.7 LiveCodeBench v5 pass@1 0-shot 18.6 25.7 Codegolf v2.2 pass@1 0-shot 11.0 16.8 AIME 2025 Accuracy 0-shot 6.7 11.6 Additional benchmarks Benchmark Metric n-shot E2B IT E4B IT",
      "variants": [
        {
          "tag": "gemma3n:latest",
          "size_text": "7.5GB",
          "size_bytes": 8053063680,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "gemma3n:e2b",
          "size_text": "5.6GB",
          "size_bytes": 6012954214,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "gemma3n:e4b",
          "size_text": "7.5GB",
          "size_bytes": 8053063680,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "gemma3n:e2b-it-q4_K_M",
          "size_text": "5.6GB",
          "size_bytes": 6012954214,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "gemma3n:e2b-it-q8_0",
          "size_text": "6.6GB",
          "size_bytes": 7086696038,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "gemma3n:e2b-it-fp16",
          "size_text": "8.9GB",
          "size_bytes": 9556302233,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "gemma3n:e4b-it-q4_K_M",
          "size_text": "7.5GB",
          "size_bytes": 8053063680,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "gemma3n:e4b-it-q8_0",
          "size_text": "9.5GB",
          "size_bytes": 10200547328,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "gemma3n:e4b-it-fp16",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "32K",
          "input": "Text"
        }
      ],
      "tags_count": 9
    },
    {
      "slug": "cogito",
      "capabilities": [
        "reasoning",
        "thinking",
        "tools"
      ],
      "pulls": 670800,
      "pulls_text": "670.8K \n  Downloads",
      "blurb": "cogito Cogito v1 Preview is a family of hybrid models by Deep Cogito that outperform the best available open models of the same size, including counterparts from LLaMA, DeepSeek, and Qwen across most standard benchmarks. 3b 8b 14b 32b 70b 20 Tags Updated 6 months ago",
      "name": "Cogito v1 Preview is a family of hybrid reasoning models by Deep Cogito that outperform the best available open models of the same size, including counterparts from LLaMA, DeepSeek, and Qwen across most standard benchmarks.",
      "description": "Readme The Cogito v1 Preview LLMs are instruction tuned generative models (text in/text out). All models are released under an open license for commercial use. Cogito models are hybrid reasoning models. Each model can answer directly (standard LLM), or self-reflect before answering (like reasoning models). The LLMs are trained using Iterated Distillation and Amplification (IDA) - an scalable and efficient alignment strategy for superintelligence using iterative self-improvement. The models have been optimized for coding, STEM, instruction following and general helpfulness, and have significantly higher multilingual, coding and tool calling capabilities than size equivalent counterparts. In both standard and reasoning modes, Cogito v1-preview models outperform their size equivalent counterparts on common industry benchmarks. Each model is trained in over 30 languages and supports a context length of 128k. Extended thinking To enable extended thinking, include Enable deep thinking subroutine. in the system prompt: /set system \"\"\"Enable deep thinking subroutine.\"\"\" Or via the API: curl http://localhost:11434/api/chat -d '{ \"model\": \"cogito\", \"messages\": [ { \"role\": \"system\", \"content\": \"Enable deep thinking subroutine.\" }, { \"role\": \"user\", \"content\": \"How many letter Rs are in the word Strawberry?\" } ] }' Sizes 3B ollama run cogito:3b 8B ollama run cogito:8b 14B ollama run cogito:14b 32B ollama run cogito:32b 70B ollama run cogito:70b Benchmarks Smaller models - 3B and 8B 3B performance 8B performance 3B tool calling Medium models - 14B and 32B 14B 32B Larger models - 70B References Blog post Hugging Face Write Preview <img src=\"/assets/library/cogito/44ceefc5-6a71-4d18-958e-21d45d309b18\" width=\"320\" /> The Cogito v1 Preview LLMs are instruction tuned generative models (text in/text out). All models are released under an open license for commercial use. - Cogito models are hybrid reasoning models. Each model can answer directly (standard LLM), or self-reflect before a",
      "variants": [
        {
          "tag": "cogito:latest",
          "size_text": "4.9GB",
          "size_bytes": 5261334937,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "cogito:3b",
          "size_text": "2.2GB",
          "size_bytes": 2362232012,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "cogito:8b",
          "size_text": "4.9GB",
          "size_bytes": 5261334937,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "cogito:14b",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "cogito:32b",
          "size_text": "20GB",
          "size_bytes": 21474836480,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "cogito:70b",
          "size_text": "43GB",
          "size_bytes": 46170898432,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "cogito:3b-v1-preview-llama-q4_K_M",
          "size_text": "2.2GB",
          "size_bytes": 2362232012,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "cogito:3b-v1-preview-llama-q8_0",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "cogito:3b-v1-preview-llama-fp16",
          "size_text": "7.2GB",
          "size_bytes": 7730941132,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "cogito:8b-v1-preview-llama-q4_K_M",
          "size_text": "4.9GB",
          "size_bytes": 5261334937,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "cogito:8b-v1-preview-llama-q8_0",
          "size_text": "8.5GB",
          "size_bytes": 9126805504,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "cogito:14b-v1-preview-qwen-q4_K_M",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "cogito:14b-v1-preview-qwen-q8_0",
          "size_text": "16GB",
          "size_bytes": 17179869184,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "cogito:14b-v1-preview-qwen-fp16",
          "size_text": "30GB",
          "size_bytes": 32212254720,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "cogito:32b-v1-preview-qwen-q4_K_M",
          "size_text": "20GB",
          "size_bytes": 21474836480,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "cogito:32b-v1-preview-qwen-q8_0",
          "size_text": "35GB",
          "size_bytes": 37580963840,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "cogito:32b-v1-preview-qwen-fp16",
          "size_text": "66GB",
          "size_bytes": 70866960384,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "cogito:70b-v1-preview-llama-q4_K_M",
          "size_text": "43GB",
          "size_bytes": 46170898432,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "cogito:70b-v1-preview-llama-q8_0",
          "size_text": "75GB",
          "size_bytes": 80530636800,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "cogito:70b-v1-preview-llama-fp16",
          "size_text": "141GB",
          "size_bytes": 151397597184,
          "context": "128K",
          "input": "Text"
        }
      ],
      "tags_count": 20
    },
    {
      "slug": "granite3.3",
      "capabilities": [
        "reasoning",
        "thinking",
        "tools"
      ],
      "pulls": 670300,
      "pulls_text": "670.3K \n  Downloads",
      "blurb": "granite3.3 IBM Granite 2B and 8B models are 128K context length language models that have been fine-tuned for improved and instruction-following capabilities. 2b 8b 3 Tags Updated 6 months ago",
      "name": "IBM Granite 2B and 8B models are 128K context length language models that have been fine-tuned for improved reasoning and instruction-following capabilities.",
      "description": "Readme Granite 3.3 The IBM Granite 2B and 8B models are 128K context length language models that have been fine-tuned for improved reasoning and instruction-following capabilities. These models deliver significant gains on benchmarks for measuring generic performance including AlpacaEval-2.0 and Arena-Hard, and improvements in mathematics, coding, and instruction following. They also supports Fill-in-the-Middle (FIM) for code completion tasks and structured reasoning. Parameter Sizes 2B: ollama run granite3.3:2b 8B: ollama run granite3.3:8b Supported Languages English, German, Spanish, French, Japanese, Portuguese, Arabic, Czech, Italian, Korean, Dutch, and Chinese. However, users may finetune this Granite model for languages beyond these 12 languages. Intended Use These models are designed to handle general instruction-following tasks and can be integrated into AI assistants across various domains, including business applications. Capabilities Thinking Summarization Text classification Text extraction Question-answering Retrieval Augmented Generation (RAG) Code related tasks Function-calling tasks Multilingual dialog use cases Fill-in-the-middle Long-context tasks including long document/meeting summarization, long document QA, etc. Thinking To enable thinking, add a message with “role”: “control” and set “content” to “thinking”. For example: { \"messages\": [ {\"role\": \"control\", \"content\": \"thinking\"}, {\"role\": \"user\", \"content\": \"How do I get to the airport if my car won't start?\"} ] } Learn more Developers: IBM Research Website : Granite Docs Release Date : April 16th, 2025 License: Apache 2.0 Write Preview <img src=\"/assets/library/granite3.2/90c5e567-0004-425c-a17a-1b846c2b5d3d\" width=\"600\" /> ## Granite 3.3 The IBM Granite **2B and 8B models** are 128K context length language models that have been fine-tuned for improved reasoning and instruction-following capabilities. These models deliver significant gains on benchmarks for measuring generic performance inclu",
      "variants": [
        {
          "tag": "granite3.3:latest",
          "size_text": "4.9GB",
          "size_bytes": 5261334937,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "granite3.3:2b",
          "size_text": "1.5GB",
          "size_bytes": 1610612736,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "granite3.3:8b",
          "size_text": "4.9GB",
          "size_bytes": 5261334937,
          "context": "128K",
          "input": "Text"
        }
      ],
      "tags_count": 3
    },
    {
      "slug": "phi4-reasoning",
      "capabilities": [
        "reasoning"
      ],
      "pulls": 636000,
      "pulls_text": "636K \n  Downloads",
      "blurb": "phi4- Phi 4 and plus are 14-billion parameter open-weight models that rival much larger models on complex tasks. 14b 9 Tags Updated 6 months ago",
      "name": "Phi 4 reasoning and reasoning plus are 14-billion parameter open-weight reasoning models that rival much larger models on complex reasoning tasks.",
      "description": "Readme Phi 4 reasoning and reasoning plus models are 14 billion parameter models that rival much larger models on complex reasoning tasks. Phi 4 reasoning model is trained via supervised fine-tuning of Phi 4 on carefully curated reasoning demonstrations from OpenAI’s o3-mini. This model demonstrates meticulous data curation and high quality synthetic datasets allow smaller models to compete with larger counterparts. Phi 4 reasoning plus model builds on top of Phi 4 reasoning, and is further trained with reinforcement learning to deliver higher accuracy. Models Phi 4 reasoning ollama run phi4-reasoning Phi 4 reasoning plus ollama run phi4-reasoning:plus Benchmarks Phi-4-reasoning performance across representative reasoning benchmarks spanning mathematical and scientific reasoning. We illustrate the performance gains from reasoning-focused post-training of Phi-4 via Phi-4-reasoning (SFT) and Phi-4-reasoning-plus (SFT+RL), alongside a representative set of baselines from two model families: open-weight models from DeepSeek including DeepSeek R1 (671B Mixture-of-Experts) and its distilled dense variant DeepSeek-R1 Distill Llama 70B, and OpenAI’s proprietary frontier models o1-mini and o3-mini. Phi-4-reasoning and Phi-4-reasoning-plus consistently outperform the base model Phi-4 by significant margins, exceed DeepSeek-R1 Distill Llama 70B (5x larger) and demonstrate competitive performance against significantly larger models such as Deepseek-R1. Accuracy of models across general-purpose benchmarks for: long input context QA (FlenQA), instruction following (IFEval), Coding (HumanEvalPlus), knowledge & language understanding (MMLUPro), safety detection (ToxiGen), and other general skills (ArenaHard and PhiBench). References Blog post Write Preview Phi 4 reasoning and reasoning plus models are 14 billion parameter models that rival much larger models on complex reasoning tasks. Phi 4 reasoning model is trained via supervised fine-tuning of Phi 4 on carefully curated reasoni",
      "variants": [
        {
          "tag": "phi4-reasoning:latest",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "phi4-reasoning:plus",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "phi4-reasoning:14b",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "phi4-reasoning:14b-plus-q4_K_M",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "phi4-reasoning:14b-plus-q8_0",
          "size_text": "17GB",
          "size_bytes": 18253611008,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "phi4-reasoning:14b-plus-fp16",
          "size_text": "29GB",
          "size_bytes": 31138512896,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "phi4-reasoning:14b-q4_K_M",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "phi4-reasoning:14b-q8_0",
          "size_text": "17GB",
          "size_bytes": 18253611008,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "phi4-reasoning:14b-fp16",
          "size_text": "29GB",
          "size_bytes": 31138512896,
          "context": "32K",
          "input": "Text"
        }
      ],
      "tags_count": 9
    },
    {
      "slug": "openthinker",
      "capabilities": [
        "reasoning"
      ],
      "pulls": 604700,
      "pulls_text": "604.7K \n  Downloads",
      "blurb": "openthinker A fully open-source family of models built using a dataset derived by distilling DeepSeek-R1. 7b 32b 15 Tags Updated 7 months ago",
      "name": "A fully open-source family of reasoning models built using a dataset derived by distilling DeepSeek-R1.",
      "description": "Readme OpenThinker is a family of fine-tuned models from Qwen2.5 on the OpenThoughts-114k dataset , surpassing DeepSeek-R1 distillation models on some benchmarks. The dataset is derived by distilling DeepSeek-R1 using the data pipeline available on github. More info about the dataset can be found on the dataset card at OpenThoughts-114k dataset. Models 32B ollama run openthinker:32b Model Name Dataset Size AIME24 I/II AIME25 I MATH500 GPQA Diamond LCBv2 LIMO-32B 0.8k 56.7 49.3 86.6 58.1 60.0 s1-32B 1k 36.0 25.3 84.8 50.5 40.9 s1.1-32B 1k 64.7 49.3 89.0 60.1 65.5 DeepSeek-R1-Distill-Qwen-32B 800k (closed) 76.7 55.9 89.4 57.6 71.2 OpenThinker-32B 114k 66.0 53.3 90.6 61.6 68.9 7B (default) ollama run openthinker:7b AIME24 MATH500 GPQA-Diamond LCBv2 Easy LCBv2 Medium LCBv2 Hard LCBv2 All OpenThinker-7B 31.3 83.0 42.4 75.3 28.6 6.5 39.9 Bespoke-Stratos-7B 22.7 79.6 38.9 71.4 25.2 0.8 35.8 DeepSeek-R1-Distill-Qwen-7B 60 88.2 46.9 79.7 45.1 14.6 50.1 gpt-4o-0513 8.7 75.8 46.5 87.4 42.7 8.9 50.5 o1-mini 64 85.6 60 92.8 74.7 39.8 72.8 References GitHub Blog post Write Preview <img src=\"/assets/library/openthinker/0ef3a0d3-aae1-4855-b56b-50875e9683e8\" width=\"320\" /> OpenThinker is a family of fine-tuned models from Qwen2.5 on the [OpenThoughts-114k dataset](https://huggingface.co/datasets/open-thoughts/open-thoughts-114k), surpassing DeepSeek-R1 distillation models on some benchmarks. The dataset is derived by distilling DeepSeek-R1 using the data pipeline available on github. More info about the dataset can be found on the dataset card at OpenThoughts-114k dataset. ### Models #### 32B ``` ollama run openthinker:32b ``` |Model Name|Dataset Size|AIME24 I/II|AIME25 I|MATH500|GPQA Diamond|LCBv2| |---|---|---|---|---|---|---| |LIMO-32B|0.8k|56.7|49.3|86.6|58.1|60.0| |s1-32B|1k|36.0|25.3|84.8|50.5|40.9| |s1.1-32B|1k|64.7|49.3|89.0|60.1|65.5| |DeepSeek-R1-Distill-Qwen-32B|800k (closed)|**76.7**|**55.9**|89.4|57.6|**71.2**| |**OpenThinker-32B**|114k|66.0|53.3|**90.6**|**61.6**|68.9|",
      "variants": [
        {
          "tag": "openthinker:latest",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "openthinker:7b",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "openthinker:32b",
          "size_text": "20GB",
          "size_bytes": 21474836480,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "openthinker:7b-v2-q4_K_M",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "openthinker:7b-v2-q8_0",
          "size_text": "8.1GB",
          "size_bytes": 8697308774,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "openthinker:7b-v2-fp16",
          "size_text": "15GB",
          "size_bytes": 16106127360,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "openthinker:7b-q4_K_M",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "openthinker:7b-q8_0",
          "size_text": "8.1GB",
          "size_bytes": 8697308774,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "openthinker:7b-fp16",
          "size_text": "15GB",
          "size_bytes": 16106127360,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "openthinker:32b-v2-q4_K_M",
          "size_text": "20GB",
          "size_bytes": 21474836480,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "openthinker:32b-v2-q8_0",
          "size_text": "35GB",
          "size_bytes": 37580963840,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "openthinker:32b-v2-fp16",
          "size_text": "66GB",
          "size_bytes": 70866960384,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "openthinker:32b-q4_K_M",
          "size_text": "20GB",
          "size_bytes": 21474836480,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "openthinker:32b-q8_0",
          "size_text": "35GB",
          "size_bytes": 37580963840,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "openthinker:32b-fp16",
          "size_text": "66GB",
          "size_bytes": 70866960384,
          "context": "32K",
          "input": "Text"
        }
      ],
      "tags_count": 15
    },
    {
      "slug": "magistral",
      "capabilities": [
        "reasoning",
        "thinking",
        "tools"
      ],
      "pulls": 597000,
      "pulls_text": "597K \n  Downloads",
      "blurb": "magistral Magistral is a small, efficient model with 24B parameters. 24b 5 Tags Updated 4 months ago",
      "name": "Magistral is a small, efficient reasoning model with 24B parameters.",
      "description": "Readme Magistral is the first reasoning model by Mistral AI, excelling in domain-specific, transparent, and multilingual reasoning. Key features Reasoning: Capable of long chains of reasoning traces before providing an answer. Multilingual: Supports dozens of languages, including English, French, German, Greek, Hindi, Indonesian, Italian, Japanese, Korean, Malay, Nepali, Polish, Portuguese, Romanian, Russian, Serbian, Spanish, Swedish, Turkish, Ukrainian, Vietnamese, Arabic, Bengali, Chinese, and Farsi. Apache 2.0 License: Open license allowing usage and modification for both commercial and non-commercial purposes. Context Window: A 128k context window, but performance might degrade past 40k . Hence we recommend setting the maximum model length to 40k. Magistral is ideal for general purpose use requiring longer thought processing and better accuracy than with non-reasoning LLMs. From legal research and financial forecasting to software development and creative storytelling — this model solves multi-step challenges where transparency and precision are critical. Business strategy and operations. Building on our flagship models , Magistral is designed for research, strategic planning, operational optimization, and data-driven decision making — whether executing risk assessment and modelling with multiple factors, or calculating optimal delivery windows under constraints. Regulated industries and sectors. Legal, finance, healthcare, and government professionals get traceable reasoning that meets compliance requirements. Every conclusion can be traced back through its logical steps, providing auditability for high-stakes environments with domain-specialized AI. Systems, software, and data engineering. Magistral enhances coding and development use cases: compared to non-reasoning models, it significantly improves project planning, backend architecture, frontend design, and data engineering through sequenced, multi-step actions involving external tools or API. Content and ",
      "variants": [
        {
          "tag": "magistral:latest",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "39K",
          "input": "Text"
        },
        {
          "tag": "magistral:24b",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "39K",
          "input": "Text"
        },
        {
          "tag": "magistral:24b-small-2506-q4_K_M",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "39K",
          "input": "Text"
        },
        {
          "tag": "magistral:24b-small-2506-q8_0",
          "size_text": "25GB",
          "size_bytes": 26843545600,
          "context": "39K",
          "input": "Text"
        },
        {
          "tag": "magistral:24b-small-2506-fp16",
          "size_text": "47GB",
          "size_bytes": 50465865728,
          "context": "39K",
          "input": "Text"
        }
      ],
      "tags_count": 5
    },
    {
      "slug": "qwen3-coder",
      "capabilities": [
        "tools"
      ],
      "pulls": 588500,
      "pulls_text": "588.5K \n  Downloads",
      "blurb": "qwen3-coder Alibaba's performant long context models for agentic and coding tasks. cloud 30b 480b 10 Tags Updated 1 month ago",
      "name": "Alibaba's performant long context models for agentic and coding tasks.",
      "description": "Readme Qwen3-Coder is the most agentic code model to date in the Qwen series. Get started 480B Cloud ollama run qwen3-coder:480b-cloud Local ollama run qwen3-coder:480b Running locally requires a minimum of 250GB of memory or unified memory. 30B ollama run qwen3-coder:30b Overview qwen3-coder:30b offers 30B total parameters with only 3.3B activated, delivering strong performance while maintaining efficiency. Exceptional agentic capabilities for real-world software engineering tasks through advanced long-horizon reinforcement learning on SWE-Bench and similar benchmarks. Long context support with 256K tokens natively and up to 1M tokens using extrapolation methods, optimized for repository-scale understanding. Scaled pretraining on 7.5T tokens (70% code ratio) while preserving strong general and mathematical abilities. Execution-driven reinforcement learning that significantly boosts code execution success rates across diverse real-world coding tasks. Reference Blog Write Preview ![Qwen 3 logo](/assets/library/qwen3/a5541098-87ba-4184-a5af-2b63312c2522) **Qwen3-Coder** is the most agentic code model to date in the Qwen series. ### Get started **480B** Cloud ``` ollama run qwen3-coder:480b-cloud ``` Local ``` ollama run qwen3-coder:480b ``` Running locally requires a minimum of 250GB of memory or unified memory. **30B** ``` ollama run qwen3-coder:30b ``` ### Overview `qwen3-coder:30b` offers 30B total parameters with only 3.3B activated, delivering strong performance while maintaining efficiency. - Exceptional agentic capabilities for real-world software engineering tasks through advanced long-horizon reinforcement learning on SWE-Bench and similar benchmarks. - Long context support with 256K tokens natively and up to 1M tokens using extrapolation methods, optimized for repository-scale understanding. - Scaled pretraining on 7.5T tokens (70% code ratio) while preserving strong general and mathematical abilities. - Execution-driven reinforcement learning that significa",
      "variants": [
        {
          "tag": "qwen3-coder:latest",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-coder:30b",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-coder:480b",
          "size_text": "290GB",
          "size_bytes": 311385128960,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-coder:30b-a3b-q4_K_M",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-coder:30b-a3b-q8_0",
          "size_text": "32GB",
          "size_bytes": 34359738368,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-coder:30b-a3b-fp16",
          "size_text": "61GB",
          "size_bytes": 65498251264,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-coder:480b-a35b-q4_K_M",
          "size_text": "290GB",
          "size_bytes": 311385128960,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-coder:480b-a35b-q8_0",
          "size_text": "510GB",
          "size_bytes": 547608330240,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-coder:480b-a35b-fp16",
          "size_text": "960GB",
          "size_bytes": 1030792151040,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-coder:480b-cloud",
          "size_text": null,
          "size_bytes": null,
          "context": "256K",
          "input": "Text"
        }
      ],
      "tags_count": 10
    },
    {
      "slug": "deepscaler",
      "capabilities": [
        "reasoning"
      ],
      "pulls": 581000,
      "pulls_text": "581K \n  Downloads",
      "blurb": "deepscaler A fine-tuned version of Deepseek-R1-Distilled-Qwen-1.5B that surpasses the performance of OpenAI’s o1-preview with just 1.5B parameters on popular math evaluations. 1.5b 5 Tags Updated 8 months ago",
      "name": "A fine-tuned version of Deepseek-R1-Distilled-Qwen-1.5B that surpasses the performance of OpenAI’s o1-preview with just 1.5B parameters on popular math evaluations.",
      "description": "Readme DeepScaleR 🚀 Democratizing Reinforcement Learning for LLMs 🌟 DeepScaleR-1.5B-Preview is a language model fine-tuned from DeepSeek-R1-Distilled-Qwen-1.5B using distributed reinforcement learning (RL) to scale up to long context lengths. The model achieves 43.1% Pass@1 accuracy on AIME 2024, representing a 15% improvement over the base model (28.8%) and surpassing OpenAI’s O1-Preview performance with just 1.5B parameters. Model AIME 2024 MATH 500 AMC 2023 Minerva Math Olympiad Bench Avg. DeepScaleR-1.5B-Preview 43.1 87.8 73.6 30.2 50.0 57.0 DeepSeek-R1-Distill-Qwen-1.5B 28.8 82.8 62.9 26.5 43.3 48.9 O1-Preview 40.0 81.4 - - - - Data Our training dataset consists of approximately 40,000 unique problem-answer pairs compiled from: AIME problems (1984-2023) AMC problems (prior to 2023) Omni-MATH dataset Still dataset Evaluation We evaluate our model on competition-level mathematics benchmarks, including AIME 2024, AMC 2023, MATH-500, Minerva Math, and OlympiadBench. Below, Pass@1 accuracy is reported, averaged over 16 samples for each problem. Model AIME 2024 MATH 500 AMC 2023 Minerva Math OlympiadBench Avg. Qwen-2.5-Math-7B-Instruct 13.3 79.8 50.6 34.6 40.7 43.8 rStar-Math-7B 26.7 78.4 47.5 - 47.1 - Eurus-2-7B-PRIME 26.7 79.2 57.8 38.6 42.1 48.9 Qwen2.5-7B-SimpleRL 26.7 82.4 62.5 39.7 43.3 50.9 DeepSeek-R1-Distill-Qwen-1.5B 28.8 82.8 62.9 26.5 43.3 48.9 Still-1.5B 32.5 84.4 66.7 29.0 45.4 51.6 DeepScaleR-1.5B-Preview 43.1 87.8 73.6 30.2 50.0 57.0 O1-Preview 40.0 81.4 - - - - We compare DeepScaleR with the base DeepSeek model we use, as well as recent academic works exploring RL for reasoning tasks. DeepScaleR significantly outperforms the base model across all benchmarks, achieving a 14.4% absolute gain on AIME2024 and an 8.1% overall improvement. Additionally, DeepScaleR surpasses recent academic works such as rSTAR, Prime, and SimpleRL, which are finetuned from 7B models. DeepScaleR achieves O1-preview-level performance with only 1.5B parameters—a remarkable eff",
      "variants": [
        {
          "tag": "deepscaler:latest",
          "size_text": "3.6GB",
          "size_bytes": 3865470566,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "deepscaler:1.5b",
          "size_text": "3.6GB",
          "size_bytes": 3865470566,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "deepscaler:1.5b-preview-q4_K_M",
          "size_text": "1.1GB",
          "size_bytes": 1181116006,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "deepscaler:1.5b-preview-q8_0",
          "size_text": "1.9GB",
          "size_bytes": 2040109465,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "deepscaler:1.5b-preview-fp16",
          "size_text": "3.6GB",
          "size_bytes": 3865470566,
          "context": "128K",
          "input": "Text"
        }
      ],
      "tags_count": 5
    },
    {
      "slug": "dolphin-phi",
      "capabilities": [],
      "pulls": 577000,
      "pulls_text": "577K \n  Downloads",
      "blurb": "dolphin-phi 2.7B uncensored Dolphin model by Eric Hartford, based on the Phi language model by Microsoft Research. 2.7b 15 Tags Updated 1 year ago",
      "name": "2.7B uncensored Dolphin model by Eric Hartford, based on the Phi language model by Microsoft Research.",
      "description": "Readme Dolphin Phi 2.6 is an uncensored model based on the 2.7B Phi model by Microsoft Research, using similar datasets as other versions of this model such as Dolphin Mixtral . It was created by Eric Hartford and Cognitive Computations. References HuggingFace Write Preview <img src=\"https://github.com/jmorganca/ollama/assets/251292/c65aad4e-7a58-4645-97f2-5e8ca158275f\" width=\"280\" /> Dolphin Phi 2.6 is an uncensored model based on the 2.7B [Phi](https://ollama.ai/library/phi) model by Microsoft Research, using similar datasets as other versions of this model such as [Dolphin Mixtral](https://ollama.ai/library/dolphin-mixtral). It was created by [Eric Hartford](https://erichartford.com/) and Cognitive Computations. ## References [HuggingFace](https://huggingface.co/cognitivecomputations/dolphin-2_6-phi-2) Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "dolphin-phi:latest",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "dolphin-phi:2.7b",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "dolphin-phi:2.7b-v2.6",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "dolphin-phi:2.7b-v2.6-q2_K",
          "size_text": "1.2GB",
          "size_bytes": 1288490188,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "dolphin-phi:2.7b-v2.6-q3_K_S",
          "size_text": "1.3GB",
          "size_bytes": 1395864371,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "dolphin-phi:2.7b-v2.6-q3_K_M",
          "size_text": "1.5GB",
          "size_bytes": 1610612736,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "dolphin-phi:2.7b-v2.6-q3_K_L",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "dolphin-phi:2.7b-v2.6-q4_0",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "dolphin-phi:2.7b-v2.6-q4_K_S",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "dolphin-phi:2.7b-v2.6-q4_K_M",
          "size_text": "1.8GB",
          "size_bytes": 1932735283,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "dolphin-phi:2.7b-v2.6-q5_0",
          "size_text": "1.9GB",
          "size_bytes": 2040109465,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "dolphin-phi:2.7b-v2.6-q5_K_S",
          "size_text": "1.9GB",
          "size_bytes": 2040109465,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "dolphin-phi:2.7b-v2.6-q5_K_M",
          "size_text": "2.1GB",
          "size_bytes": 2254857830,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "dolphin-phi:2.7b-v2.6-q6_K",
          "size_text": "2.3GB",
          "size_bytes": 2469606195,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "dolphin-phi:2.7b-v2.6-q8_0",
          "size_text": "3.0GB",
          "size_bytes": 3221225472,
          "context": "2K",
          "input": "Text"
        }
      ],
      "tags_count": 15
    },
    {
      "slug": "dolphin-llama3",
      "capabilities": [],
      "pulls": 524000,
      "pulls_text": "524K \n  Downloads",
      "blurb": "dolphin-llama3 Dolphin 2.9 is a new model with 8B and 70B sizes by Eric Hartford based on Llama 3 that has a variety of instruction, conversational, and coding skills. 8b 70b 53 Tags Updated 1 year ago",
      "name": "Dolphin 2.9 is a new model with 8B and 70B sizes by Eric Hartford based on Llama 3 that has a variety of instruction, conversational, and coding skills.",
      "description": "Readme 🐬 Dolphin 2.9 Llama 3 Dolphin-2.9 has a variety of instruction, conversational, and coding skills. It also has initial agentic abilities and supports function calling. Dolphin is uncensored . The dataset has been filtered to remove alignment and bias. This makes the model more compliant. Curated and trained by Eric Hartford, Lucas Atkins, and Fernando Fernandes, and Cognitive Computations. Sizes dolphin-llama3:8b dolphin-llama3:70b 256K Context Window Note: using a 256k context window requires at least 64GB of memory. Dolphin Llama 3 also has a 256k context window version. To extend the context window use: API curl http://localhost:11434/api/generate -d '{ \"model\": \"dolphin-llama3:8b-256k\", \"prompt\": \"Why is the sky blue?\", \"options\": { \"num_ctx\": 256000 } }' CLI ollama run dolphin-llama3:8b-256k >>> /set parameter num_ctx 256000 References HuggingFace Write Preview # 🐬 Dolphin 2.9 Llama 3 Dolphin-2.9 has a variety of instruction, conversational, and coding skills. It also has initial agentic abilities and supports function calling. Dolphin is [uncensored](https://erichartford.com/uncensored-models). The dataset has been filtered to remove alignment and bias. This makes the model more compliant. Curated and trained by Eric Hartford, Lucas Atkins, and Fernando Fernandes, and Cognitive Computations. ## Sizes * `dolphin-llama3:8b` * `dolphin-llama3:70b` ## 256K Context Window > Note: using a 256k context window requires at least 64GB of memory. Dolphin Llama 3 also has a 256k context window version. To extend the context window use: <sub>API</sub> ``` curl http://localhost:11434/api/generate -d '{ \"model\": \"dolphin-llama3:8b-256k\", \"prompt\": \"Why is the sky blue?\", \"options\": { \"num_ctx\": 256000 } }' ``` <sub>CLI</sub> ``` ollama run dolphin-llama3:8b-256k >>> /set parameter num_ctx 256000 ``` ## References [HuggingFace](https://huggingface.co/cognitivecomputations/dolphin-2.9-llama3-8b) Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "dolphin-llama3:latest",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "dolphin-llama3:v2.9",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "dolphin-llama3:8b",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "dolphin-llama3:70b",
          "size_text": "40GB",
          "size_bytes": 42949672960,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "dolphin-llama3:8b-256k",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "250K",
          "input": "Text"
        },
        {
          "tag": "dolphin-llama3:8b-256k-v2.9",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "250K",
          "input": "Text"
        },
        {
          "tag": "dolphin-llama3:8b-256k-v2.9-q2_K",
          "size_text": "3.2GB",
          "size_bytes": 3435973836,
          "context": "250K",
          "input": "Text"
        },
        {
          "tag": "dolphin-llama3:8b-256k-v2.9-q3_K_S",
          "size_text": "3.7GB",
          "size_bytes": 3972844748,
          "context": "250K",
          "input": "Text"
        },
        {
          "tag": "dolphin-llama3:8b-256k-v2.9-q3_K_M",
          "size_text": "4.0GB",
          "size_bytes": 4294967296,
          "context": "250K",
          "input": "Text"
        },
        {
          "tag": "dolphin-llama3:8b-256k-v2.9-q3_K_L",
          "size_text": "4.3GB",
          "size_bytes": 4617089843,
          "context": "250K",
          "input": "Text"
        },
        {
          "tag": "dolphin-llama3:8b-256k-v2.9-q4_0",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "250K",
          "input": "Text"
        },
        {
          "tag": "dolphin-llama3:8b-256k-v2.9-q4_1",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "250K",
          "input": "Text"
        },
        {
          "tag": "dolphin-llama3:8b-256k-v2.9-q4_K_S",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "250K",
          "input": "Text"
        },
        {
          "tag": "dolphin-llama3:8b-256k-v2.9-q4_K_M",
          "size_text": "4.9GB",
          "size_bytes": 5261334937,
          "context": "250K",
          "input": "Text"
        },
        {
          "tag": "dolphin-llama3:8b-256k-v2.9-q5_0",
          "size_text": "5.6GB",
          "size_bytes": 6012954214,
          "context": "250K",
          "input": "Text"
        },
        {
          "tag": "dolphin-llama3:8b-256k-v2.9-q5_1",
          "size_text": "6.1GB",
          "size_bytes": 6549825126,
          "context": "250K",
          "input": "Text"
        },
        {
          "tag": "dolphin-llama3:8b-256k-v2.9-q5_K_S",
          "size_text": "5.6GB",
          "size_bytes": 6012954214,
          "context": "250K",
          "input": "Text"
        },
        {
          "tag": "dolphin-llama3:8b-256k-v2.9-q5_K_M",
          "size_text": "5.7GB",
          "size_bytes": 6120328396,
          "context": "250K",
          "input": "Text"
        },
        {
          "tag": "dolphin-llama3:8b-256k-v2.9-q6_K",
          "size_text": "6.6GB",
          "size_bytes": 7086696038,
          "context": "250K",
          "input": "Text"
        },
        {
          "tag": "dolphin-llama3:8b-256k-v2.9-q8_0",
          "size_text": "8.5GB",
          "size_bytes": 9126805504,
          "context": "250K",
          "input": "Text"
        },
        {
          "tag": "dolphin-llama3:8b-256k-v2.9-fp16",
          "size_text": "16GB",
          "size_bytes": 17179869184,
          "context": "250K",
          "input": "Text"
        },
        {
          "tag": "dolphin-llama3:8b-v2.9",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "dolphin-llama3:8b-v2.9-q2_K",
          "size_text": "3.2GB",
          "size_bytes": 3435973836,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "dolphin-llama3:8b-v2.9-q3_K_S",
          "size_text": "3.7GB",
          "size_bytes": 3972844748,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "dolphin-llama3:8b-v2.9-q3_K_M",
          "size_text": "4.0GB",
          "size_bytes": 4294967296,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "dolphin-llama3:8b-v2.9-q3_K_L",
          "size_text": "4.3GB",
          "size_bytes": 4617089843,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "dolphin-llama3:8b-v2.9-q4_0",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "dolphin-llama3:8b-v2.9-q4_1",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "dolphin-llama3:8b-v2.9-q4_K_S",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "dolphin-llama3:8b-v2.9-q4_K_M",
          "size_text": "4.9GB",
          "size_bytes": 5261334937,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "dolphin-llama3:8b-v2.9-q5_0",
          "size_text": "5.6GB",
          "size_bytes": 6012954214,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "dolphin-llama3:8b-v2.9-q5_1",
          "size_text": "6.1GB",
          "size_bytes": 6549825126,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "dolphin-llama3:8b-v2.9-q5_K_S",
          "size_text": "5.6GB",
          "size_bytes": 6012954214,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "dolphin-llama3:8b-v2.9-q5_K_M",
          "size_text": "5.7GB",
          "size_bytes": 6120328396,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "dolphin-llama3:8b-v2.9-q6_K",
          "size_text": "6.6GB",
          "size_bytes": 7086696038,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "dolphin-llama3:8b-v2.9-q8_0",
          "size_text": "8.5GB",
          "size_bytes": 9126805504,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "dolphin-llama3:8b-v2.9-fp16",
          "size_text": "16GB",
          "size_bytes": 17179869184,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "dolphin-llama3:70b-v2.9",
          "size_text": "40GB",
          "size_bytes": 42949672960,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "dolphin-llama3:70b-v2.9-q2_K",
          "size_text": "26GB",
          "size_bytes": 27917287424,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "dolphin-llama3:70b-v2.9-q3_K_S",
          "size_text": "31GB",
          "size_bytes": 33285996544,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "dolphin-llama3:70b-v2.9-q3_K_M",
          "size_text": "34GB",
          "size_bytes": 36507222016,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "dolphin-llama3:70b-v2.9-q3_K_L",
          "size_text": "37GB",
          "size_bytes": 39728447488,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "dolphin-llama3:70b-v2.9-q4_0",
          "size_text": "40GB",
          "size_bytes": 42949672960,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "dolphin-llama3:70b-v2.9-q4_1",
          "size_text": "44GB",
          "size_bytes": 47244640256,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "dolphin-llama3:70b-v2.9-q4_K_S",
          "size_text": "40GB",
          "size_bytes": 42949672960,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "dolphin-llama3:70b-v2.9-q4_K_M",
          "size_text": "43GB",
          "size_bytes": 46170898432,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "dolphin-llama3:70b-v2.9-q5_0",
          "size_text": "49GB",
          "size_bytes": 52613349376,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "dolphin-llama3:70b-v2.9-q5_1",
          "size_text": "53GB",
          "size_bytes": 56908316672,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "dolphin-llama3:70b-v2.9-q5_K_S",
          "size_text": "49GB",
          "size_bytes": 52613349376,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "dolphin-llama3:70b-v2.9-q5_K_M",
          "size_text": "50GB",
          "size_bytes": 53687091200,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "dolphin-llama3:70b-v2.9-q6_K",
          "size_text": "58GB",
          "size_bytes": 62277025792,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "dolphin-llama3:70b-v2.9-q8_0",
          "size_text": "75GB",
          "size_bytes": 80530636800,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "dolphin-llama3:70b-v2.9-fp16",
          "size_text": "141GB",
          "size_bytes": 151397597184,
          "context": "8K",
          "input": "Text"
        }
      ],
      "tags_count": 53
    },
    {
      "slug": "codestral",
      "capabilities": [],
      "pulls": 507300,
      "pulls_text": "507.3K \n  Downloads",
      "blurb": "codestral Codestral is Mistral AI’s first-ever code model designed for code generation tasks. 22b 17 Tags Updated 1 year ago",
      "name": "Codestral is Mistral AI’s first-ever code model designed for code generation tasks.",
      "description": "Readme Codestral Codestral is Mistral AI’s first-ever code model designed for code generation tasks. It is a 22B model. Fluent in 80+ programming languages Codestral is trained on a dataset of over 80 programming languages, including Python, Java, C, C++, JavaScript, Swift, Fortran and Bash. The model can complete coding functions, write tests, and complete any partial code using a fill-in-the-middle mechanism. Benchmarks Reference Mistral AI - Codestral: Hello, World Write Preview # Codestral ![Codestral Hello World](https://ollama.com/assets/mchiang0610/test1/76bfd7f0-fccd-431b-95fe-dddf5a422dcf) Codestral is Mistral AI's first-ever code model designed for code generation tasks. It is a 22B model. ### Fluent in 80+ programming languages Codestral is trained on a dataset of over 80 programming languages, including Python, Java, C, C++, JavaScript, Swift, Fortran and Bash. The model can complete coding functions, write tests, and complete any partial code using a fill-in-the-middle mechanism. ### Benchmarks ![With its larger context window of 32k (compared to 4k, 8k or 16k for competitors), Codestral outperforms all other models in RepoBench, a long-range eval for code generation](https://ollama.com/assets/mchiang0610/test1/f5ea9604-669b-4de0-957d-41245e33636c) ![Benchmarks](https://ollama.com/assets/mchiang0610/test1/e42a01d3-4f36-4d60-a0c8-74ad10159cdd) ![FIM benchmarks](https://ollama.com/assets/mchiang0610/test1/8ca203e2-7bfd-4081-98c8-1ecc3cc46ef8) ### Reference [Mistral AI - Codestral: Hello, World](https://mistral.ai/news/codestral/) Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "codestral:latest",
          "size_text": "13GB",
          "size_bytes": 13958643712,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "codestral:v0.1",
          "size_text": "13GB",
          "size_bytes": 13958643712,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "codestral:22b",
          "size_text": "13GB",
          "size_bytes": 13958643712,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "codestral:22b-v0.1-q2_K",
          "size_text": "8.3GB",
          "size_bytes": 8912057139,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "codestral:22b-v0.1-q3_K_S",
          "size_text": "9.6GB",
          "size_bytes": 10307921510,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "codestral:22b-v0.1-q3_K_M",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "codestral:22b-v0.1-q3_K_L",
          "size_text": "12GB",
          "size_bytes": 12884901888,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "codestral:22b-v0.1-q4_0",
          "size_text": "13GB",
          "size_bytes": 13958643712,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "codestral:22b-v0.1-q4_1",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "codestral:22b-v0.1-q4_K_S",
          "size_text": "13GB",
          "size_bytes": 13958643712,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "codestral:22b-v0.1-q4_K_M",
          "size_text": "13GB",
          "size_bytes": 13958643712,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "codestral:22b-v0.1-q5_0",
          "size_text": "15GB",
          "size_bytes": 16106127360,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "codestral:22b-v0.1-q5_1",
          "size_text": "17GB",
          "size_bytes": 18253611008,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "codestral:22b-v0.1-q5_K_S",
          "size_text": "15GB",
          "size_bytes": 16106127360,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "codestral:22b-v0.1-q5_K_M",
          "size_text": "16GB",
          "size_bytes": 17179869184,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "codestral:22b-v0.1-q6_K",
          "size_text": "18GB",
          "size_bytes": 19327352832,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "codestral:22b-v0.1-q8_0",
          "size_text": "24GB",
          "size_bytes": 25769803776,
          "context": "32K",
          "input": "Text"
        }
      ],
      "tags_count": 17
    },
    {
      "slug": "smollm",
      "capabilities": [],
      "pulls": 488600,
      "pulls_text": "488.6K \n  Downloads",
      "blurb": "smollm 🪐 A family of small models with 135M, 360M, and 1.7B parameters, trained on a new high-quality dataset. 135m 360m 1.7b 94 Tags Updated 1 year ago",
      "name": "🪐 A family of small models with 135M, 360M, and 1.7B parameters, trained on a new high-quality dataset.",
      "description": "Readme SmolLM is a series of small language models available in three sizes: 135M, 360M, and 1.7B parameters. References Blog post Hugging Face Write Preview <img src=\"https://ollama.com/assets/library/smollm/50a4746f-b96d-4948-aa52-7ff06524f485\" width=\"512\" /> SmolLM is a series of small language models available in three sizes: 135M, 360M, and 1.7B parameters. ## References [Blog post](https://huggingface.co/blog/smollm) [Hugging Face](https://huggingface.co/collections/HuggingFaceTB/smollm-6695016cad7167254ce15966) Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "smollm:latest",
          "size_text": "991MB",
          "size_bytes": 1039138816,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:135m",
          "size_text": "92MB",
          "size_bytes": 96468992,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:360m",
          "size_text": "229MB",
          "size_bytes": 240123904,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:1.7b",
          "size_text": "991MB",
          "size_bytes": 1039138816,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:135m-base-v0.2-q2_K",
          "size_text": "88MB",
          "size_bytes": 92274688,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:135m-base-v0.2-q3_K_S",
          "size_text": "88MB",
          "size_bytes": 92274688,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:135m-base-v0.2-q3_K_M",
          "size_text": "94MB",
          "size_bytes": 98566144,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:135m-base-v0.2-q3_K_L",
          "size_text": "98MB",
          "size_bytes": 102760448,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:135m-base-v0.2-q4_0",
          "size_text": "92MB",
          "size_bytes": 96468992,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:135m-base-v0.2-q4_1",
          "size_text": "98MB",
          "size_bytes": 102760448,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:135m-base-v0.2-q4_K_S",
          "size_text": "102MB",
          "size_bytes": 106954752,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:135m-base-v0.2-q4_K_M",
          "size_text": "105MB",
          "size_bytes": 110100480,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:135m-base-v0.2-q5_0",
          "size_text": "105MB",
          "size_bytes": 110100480,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:135m-base-v0.2-q5_1",
          "size_text": "112MB",
          "size_bytes": 117440512,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:135m-base-v0.2-q5_K_S",
          "size_text": "110MB",
          "size_bytes": 115343360,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:135m-base-v0.2-q5_K_M",
          "size_text": "112MB",
          "size_bytes": 117440512,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:135m-base-v0.2-q6_K",
          "size_text": "138MB",
          "size_bytes": 144703488,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:135m-base-v0.2-q8_0",
          "size_text": "145MB",
          "size_bytes": 152043520,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:135m-base-v0.2-fp16",
          "size_text": "271MB",
          "size_bytes": 284164096,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:135m-instruct-v0.2-q2_K",
          "size_text": "88MB",
          "size_bytes": 92274688,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:135m-instruct-v0.2-q3_K_S",
          "size_text": "88MB",
          "size_bytes": 92274688,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:135m-instruct-v0.2-q3_K_M",
          "size_text": "94MB",
          "size_bytes": 98566144,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:135m-instruct-v0.2-q3_K_L",
          "size_text": "98MB",
          "size_bytes": 102760448,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:135m-instruct-v0.2-q4_0",
          "size_text": "92MB",
          "size_bytes": 96468992,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:135m-instruct-v0.2-q4_1",
          "size_text": "98MB",
          "size_bytes": 102760448,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:135m-instruct-v0.2-q4_K_S",
          "size_text": "102MB",
          "size_bytes": 106954752,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:135m-instruct-v0.2-q4_K_M",
          "size_text": "105MB",
          "size_bytes": 110100480,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:135m-instruct-v0.2-q5_0",
          "size_text": "105MB",
          "size_bytes": 110100480,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:135m-instruct-v0.2-q5_1",
          "size_text": "112MB",
          "size_bytes": 117440512,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:135m-instruct-v0.2-q5_K_S",
          "size_text": "110MB",
          "size_bytes": 115343360,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:135m-instruct-v0.2-q5_K_M",
          "size_text": "112MB",
          "size_bytes": 117440512,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:135m-instruct-v0.2-q6_K",
          "size_text": "138MB",
          "size_bytes": 144703488,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:135m-instruct-v0.2-q8_0",
          "size_text": "145MB",
          "size_bytes": 152043520,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:135m-instruct-v0.2-fp16",
          "size_text": "271MB",
          "size_bytes": 284164096,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:360m-base-v0.2-q2_K",
          "size_text": "219MB",
          "size_bytes": 229638144,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:360m-base-v0.2-q3_K_S",
          "size_text": "219MB",
          "size_bytes": 229638144,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:360m-base-v0.2-q3_K_M",
          "size_text": "235MB",
          "size_bytes": 246415360,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:360m-base-v0.2-q3_K_L",
          "size_text": "246MB",
          "size_bytes": 257949696,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:360m-base-v0.2-q4_0",
          "size_text": "229MB",
          "size_bytes": 240123904,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:360m-base-v0.2-q4_1",
          "size_text": "249MB",
          "size_bytes": 261095424,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:360m-base-v0.2-q4_K_S",
          "size_text": "260MB",
          "size_bytes": 272629760,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:360m-base-v0.2-q4_K_M",
          "size_text": "271MB",
          "size_bytes": 284164096,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:360m-base-v0.2-q5_0",
          "size_text": "268MB",
          "size_bytes": 281018368,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:360m-base-v0.2-q5_1",
          "size_text": "288MB",
          "size_bytes": 301989888,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:360m-base-v0.2-q5_K_S",
          "size_text": "283MB",
          "size_bytes": 296747008,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:360m-base-v0.2-q5_K_M",
          "size_text": "290MB",
          "size_bytes": 304087040,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:360m-base-v0.2-q6_K",
          "size_text": "367MB",
          "size_bytes": 384827392,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:360m-base-v0.2-q8_0",
          "size_text": "386MB",
          "size_bytes": 404750336,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:360m-base-v0.2-fp16",
          "size_text": "726MB",
          "size_bytes": 761266176,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:360m-instruct-v0.2-q2_K",
          "size_text": "219MB",
          "size_bytes": 229638144,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:360m-instruct-v0.2-q3_K_S",
          "size_text": "219MB",
          "size_bytes": 229638144,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:360m-instruct-v0.2-q3_K_M",
          "size_text": "235MB",
          "size_bytes": 246415360,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:360m-instruct-v0.2-q3_K_L",
          "size_text": "246MB",
          "size_bytes": 257949696,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:360m-instruct-v0.2-q4_0",
          "size_text": "229MB",
          "size_bytes": 240123904,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:360m-instruct-v0.2-q4_1",
          "size_text": "249MB",
          "size_bytes": 261095424,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:360m-instruct-v0.2-q4_K_S",
          "size_text": "260MB",
          "size_bytes": 272629760,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:360m-instruct-v0.2-q4_K_M",
          "size_text": "271MB",
          "size_bytes": 284164096,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:360m-instruct-v0.2-q5_0",
          "size_text": "268MB",
          "size_bytes": 281018368,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:360m-instruct-v0.2-q5_1",
          "size_text": "288MB",
          "size_bytes": 301989888,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:360m-instruct-v0.2-q5_K_S",
          "size_text": "283MB",
          "size_bytes": 296747008,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:360m-instruct-v0.2-q5_K_M",
          "size_text": "290MB",
          "size_bytes": 304087040,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:360m-instruct-v0.2-q6_K",
          "size_text": "367MB",
          "size_bytes": 384827392,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:360m-instruct-v0.2-q8_0",
          "size_text": "386MB",
          "size_bytes": 404750336,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:360m-instruct-v0.2-fp16",
          "size_text": "726MB",
          "size_bytes": 761266176,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:1.7b-base-v0.2-q2_K",
          "size_text": "675MB",
          "size_bytes": 707788800,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:1.7b-base-v0.2-q3_K_S",
          "size_text": "777MB",
          "size_bytes": 814743552,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:1.7b-base-v0.2-q3_K_M",
          "size_text": "860MB",
          "size_bytes": 901775360,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:1.7b-base-v0.2-q3_K_L",
          "size_text": "933MB",
          "size_bytes": 978321408,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:1.7b-base-v0.2-q4_0",
          "size_text": "991MB",
          "size_bytes": 1039138816,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:1.7b-base-v0.2-q4_1",
          "size_text": "1.1GB",
          "size_bytes": 1181116006,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:1.7b-base-v0.2-q4_K_S",
          "size_text": "999MB",
          "size_bytes": 1047527424,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:1.7b-base-v0.2-q4_K_M",
          "size_text": "1.1GB",
          "size_bytes": 1181116006,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:1.7b-base-v0.2-q5_0",
          "size_text": "1.2GB",
          "size_bytes": 1288490188,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:1.7b-base-v0.2-q5_1",
          "size_text": "1.3GB",
          "size_bytes": 1395864371,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:1.7b-base-v0.2-q5_K_S",
          "size_text": "1.2GB",
          "size_bytes": 1288490188,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:1.7b-base-v0.2-q5_K_M",
          "size_text": "1.2GB",
          "size_bytes": 1288490188,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:1.7b-base-v0.2-q6_K",
          "size_text": "1.4GB",
          "size_bytes": 1503238553,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:1.7b-base-v0.2-q8_0",
          "size_text": "1.8GB",
          "size_bytes": 1932735283,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:1.7b-base-v0.2-fp16",
          "size_text": "3.4GB",
          "size_bytes": 3650722201,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:1.7b-instruct-v0.2-q2_K",
          "size_text": "675MB",
          "size_bytes": 707788800,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:1.7b-instruct-v0.2-q3_K_S",
          "size_text": "777MB",
          "size_bytes": 814743552,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:1.7b-instruct-v0.2-q3_K_M",
          "size_text": "860MB",
          "size_bytes": 901775360,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:1.7b-instruct-v0.2-q3_K_L",
          "size_text": "933MB",
          "size_bytes": 978321408,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:1.7b-instruct-v0.2-q4_0",
          "size_text": "991MB",
          "size_bytes": 1039138816,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:1.7b-instruct-v0.2-q4_1",
          "size_text": "1.1GB",
          "size_bytes": 1181116006,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:1.7b-instruct-v0.2-q4_K_S",
          "size_text": "999MB",
          "size_bytes": 1047527424,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:1.7b-instruct-v0.2-q4_K_M",
          "size_text": "1.1GB",
          "size_bytes": 1181116006,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:1.7b-instruct-v0.2-q5_0",
          "size_text": "1.2GB",
          "size_bytes": 1288490188,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:1.7b-instruct-v0.2-q5_1",
          "size_text": "1.3GB",
          "size_bytes": 1395864371,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:1.7b-instruct-v0.2-q5_K_S",
          "size_text": "1.2GB",
          "size_bytes": 1288490188,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:1.7b-instruct-v0.2-q5_K_M",
          "size_text": "1.2GB",
          "size_bytes": 1288490188,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:1.7b-instruct-v0.2-q6_K",
          "size_text": "1.4GB",
          "size_bytes": 1503238553,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:1.7b-instruct-v0.2-q8_0",
          "size_text": "1.8GB",
          "size_bytes": 1932735283,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "smollm:1.7b-instruct-v0.2-fp16",
          "size_text": "3.4GB",
          "size_bytes": 3650722201,
          "context": "2K",
          "input": "Text"
        }
      ],
      "tags_count": 94
    },
    {
      "slug": "phi4-mini",
      "capabilities": [
        "multimodal",
        "reasoning",
        "tools"
      ],
      "pulls": 467100,
      "pulls_text": "467.1K \n  Downloads",
      "blurb": "phi4-mini Phi-4-mini brings significant enhancements in multilingual support, reasoning, and mathematics, and now, the long-awaited function calling feature is finally supported. 3.8b 5 Tags Updated 8 months ago",
      "name": "Phi-4-mini brings significant enhancements in multilingual support, reasoning, and mathematics, and now, the long-awaited function calling feature is finally supported.",
      "description": "Readme Note: this model requires Ollama 0.5.13 or later. Phi-4-mini-instruct is a lightweight open model built upon synthetic data and filtered publicly available websites - with a focus on high-quality, reasoning dense data. The model belongs to the Phi-4 model family and supports 128K token context length. The model underwent an enhancement process, incorporating both supervised fine-tuning and direct preference optimization to support precise instruction adherence and robust safety measures. Primary use cases The model is intended for broad multilingual commercial and research use. The model provides uses for general purpose AI systems and applications which require: Memory/compute constrained environments Latency bound scenarios Strong reasoning (especially math and logic). The model is designed to accelerate research on language and multimodal models, for use as a building block for generative AI powered features. References Hugging Face Blog post Write Preview > Note: this model requires [Ollama 0.5.13](https://github.com/ollama/ollama/releases/tag/v0.5.13) or later. <img src=\"https://ollama.com/assets/library/phi3.5/dbf19a17-e3fd-46b6-a059-e6b4f1fae59f\" width=\"320\" /> Phi-4-mini-instruct is a lightweight open model built upon synthetic data and filtered publicly available websites - with a focus on high-quality, reasoning dense data. The model belongs to the Phi-4 model family and supports 128K token context length. The model underwent an enhancement process, incorporating both supervised fine-tuning and direct preference optimization to support precise instruction adherence and robust safety measures. ## Primary use cases The model is intended for broad multilingual commercial and research use. The model provides uses for general purpose AI systems and applications which require: - Memory/compute constrained environments - Latency bound scenarios - Strong reasoning (especially math and logic). - The model is designed to accelerate research on language and mu",
      "variants": [
        {
          "tag": "phi4-mini:latest",
          "size_text": "2.5GB",
          "size_bytes": 2684354560,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "phi4-mini:3.8b",
          "size_text": "2.5GB",
          "size_bytes": 2684354560,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "phi4-mini:3.8b-q4_K_M",
          "size_text": "2.5GB",
          "size_bytes": 2684354560,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "phi4-mini:3.8b-q8_0",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi4-mini:3.8b-fp16",
          "size_text": "7.7GB",
          "size_bytes": 8267812044,
          "context": "4K",
          "input": "Text"
        }
      ],
      "tags_count": 5
    },
    {
      "slug": "wizardlm2",
      "capabilities": [
        "reasoning"
      ],
      "pulls": 457600,
      "pulls_text": "457.6K \n  Downloads",
      "blurb": "wizardlm2 State of the art large language model from Microsoft AI with improved performance on complex chat, multilingual, and agent use cases. 7b 8x22b 22 Tags Updated 1 year ago",
      "name": "State of the art large language model from Microsoft AI with improved performance on complex chat, multilingual, reasoning and agent use cases.",
      "description": "Readme WizardLM-2 is a next generation state-of-the-art large language model with improved performance on complex chat, multilingual, reasoning and agent use cases. This family includes three cutting-edge models: wizardlm2:7b : fastest model, comparable performance with 10x larger open-source models. wizardlm2:8x22b : the most advanced model, and the best opensource LLM in Microsoft’s internal evaluation on highly complex tasks. wizardlm2:70b : model with top-tier reasoning capabilities for its size (coming soon) References Blog Post HuggingFace Write Preview WizardLM-2 is a next generation state-of-the-art large language model with improved performance on complex chat, multilingual, reasoning and agent use cases. This family includes three cutting-edge models: * `wizardlm2:7b`: fastest model, comparable performance with 10x larger open-source models. * `wizardlm2:8x22b`: the most advanced model, and the best opensource LLM in Microsoft's internal evaluation on highly complex tasks. * `wizardlm2:70b`: model with top-tier reasoning capabilities for its size (coming soon) ## References [Blog Post](https://wizardlm.github.io/WizardLM2/) [HuggingFace](https://huggingface.co/collections/microsoft/wizardlm-661d403f71e6c8257dbd598a) Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "wizardlm2:latest",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "wizardlm2:7b",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "wizardlm2:8x22b",
          "size_text": "80GB",
          "size_bytes": 85899345920,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "wizardlm2:7b-q2_K",
          "size_text": "2.7GB",
          "size_bytes": 2899102924,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "wizardlm2:7b-q3_K_S",
          "size_text": "3.2GB",
          "size_bytes": 3435973836,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "wizardlm2:7b-q3_K_M",
          "size_text": "3.5GB",
          "size_bytes": 3758096384,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "wizardlm2:7b-q3_K_L",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "wizardlm2:7b-q4_0",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "wizardlm2:7b-q4_1",
          "size_text": "4.6GB",
          "size_bytes": 4939212390,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "wizardlm2:7b-q4_K_S",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "wizardlm2:7b-q4_K_M",
          "size_text": "4.4GB",
          "size_bytes": 4724464025,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "wizardlm2:7b-q5_0",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "wizardlm2:7b-q5_1",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "wizardlm2:7b-q5_K_S",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "wizardlm2:7b-q5_K_M",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "wizardlm2:7b-q6_K",
          "size_text": "5.9GB",
          "size_bytes": 6335076761,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "wizardlm2:7b-q8_0",
          "size_text": "7.7GB",
          "size_bytes": 8267812044,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "wizardlm2:7b-fp16",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "wizardlm2:8x22b-q2_K",
          "size_text": "52GB",
          "size_bytes": 55834574848,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "wizardlm2:8x22b-q4_0",
          "size_text": "80GB",
          "size_bytes": 85899345920,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "wizardlm2:8x22b-q8_0",
          "size_text": "149GB",
          "size_bytes": 159987531776,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "wizardlm2:8x22b-fp16",
          "size_text": "281GB",
          "size_bytes": 301721452544,
          "context": "64K",
          "input": "Text"
        }
      ],
      "tags_count": 22
    },
    {
      "slug": "devstral",
      "capabilities": [
        "tools",
        "vision"
      ],
      "pulls": 428200,
      "pulls_text": "428.2K \n  Downloads",
      "blurb": "devstral Devstral: the best open source model for coding agents 24b 5 Tags Updated 3 months ago",
      "name": "Devstral: the best open source model for coding agents",
      "description": "Readme Devstral is an agentic LLM for software engineering tasks built under a collaboration between Mistral AI and All Hands AI 🙌. Devstral excels at using tools to explore codebases, editing multiple files and power software engineering agents. The model achieves remarkable performance on SWE-bench which positionates it as the #1 open source model. It is finetuned from Mistral Small 3.1, therefore it has a long context window of up to 128k tokens. As a coding agent, Devstral is text-only and before fine-tuning from Mistral-Small-3.1 the vision encoder was removed. Key Features: Agentic coding : Devstral is designed to excel at agentic coding tasks, making it a great choice for software engineering agents. Lightweight : with its compact size of just 24 billion parameters, Devstral is light enough to run on a single RTX 4090 or a Mac with 32GB RAM, making it an appropriate model for local deployment and on-device use. Apache 2.0 License : Open license allowing usage and modification for both commercial and non-commercial purposes. Context Window : A 128k context window. SWE-Bench Devstral achieves a score of 46.8% on SWE-Bench Verified , outperforming prior open-source state-of-the-art by 6%. Model Scaffold SWE-Bench Verified (%) Devstral OpenHands Scaffold 46.8 GPT-4.1-mini OpenAI Scaffold 23.6 Claude 3.5 Haiku Anthropic Scaffold 40.6 SWE-smith-LM 32B SWE-agent Scaffold 40.2 When evaluated under the same test scaffold (OpenHands, provided by All Hands AI 🙌), Devstral exceeds far larger models such as Deepseek-V3-0324 and Qwen3 232B-A22B. In the chart below, we also compare Devstral to closed and open models evaluated under any scaffold (including ones custom for the model). Here, we find that Devstral achieves substantially better performance than a number of closed-source alternatives. For example, Devstral surpasses the recent GPT-4.1-mini by over 20%. Reference Blog Write Preview <img src=\"/assets/library/mistral-small3.1/88f81c26-7028-4f08-b906-92b873d5536e\" wi",
      "variants": [
        {
          "tag": "devstral:latest",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "devstral:24b",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "devstral:24b-small-2505-q4_K_M",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "devstral:24b-small-2505-q8_0",
          "size_text": "25GB",
          "size_bytes": 26843545600,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "devstral:24b-small-2505-fp16",
          "size_text": "47GB",
          "size_bytes": 50465865728,
          "context": "128K",
          "input": "Text"
        }
      ],
      "tags_count": 5
    },
    {
      "slug": "granite3.2-vision",
      "capabilities": [
        "tools",
        "vision"
      ],
      "pulls": 418900,
      "pulls_text": "418.9K \n  Downloads",
      "blurb": "granite3.2- A compact and efficient -language model, specifically designed for visual document understanding, enabling automated content extraction from tables, charts, infographics, plots, diagrams, and more. 2b 5 Tags Updated 8 months ago",
      "name": "A compact and efficient vision-language model, specifically designed for visual document understanding, enabling automated content extraction from tables, charts, infographics, plots, diagrams, and more.",
      "description": "Readme Note: this model requires Ollama 0.5.13 . A compact and efficient vision-language model, specifically designed for visual document understanding, enabling automated content extraction from tables, charts, infographics, plots, diagrams, and more. The model was trained on a meticulously curated instruction-following dataset, comprising diverse public datasets and synthetic datasets tailored to support a wide range of document understanding and general image tasks. It was trained by fine-tuning a Granite large language model with both image and text modalities. References Hugging Face Write Preview > Note: this model requires [Ollama 0.5.13](https://github.com/ollama/ollama/releases/tag/v0.5.13). A compact and efficient vision-language model, specifically designed for visual document understanding, enabling automated content extraction from tables, charts, infographics, plots, diagrams, and more. The model was trained on a meticulously curated instruction-following dataset, comprising diverse public datasets and synthetic datasets tailored to support a wide range of document understanding and general image tasks. It was trained by fine-tuning a Granite large language model with both image and text modalities. ## References [Hugging Face](https://huggingface.co/ibm-granite/granite-vision-3.2-2b) Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "granite3.2-vision:latest",
          "size_text": "2.4GB",
          "size_bytes": 2576980377,
          "context": "16K",
          "input": "Vision"
        },
        {
          "tag": "granite3.2-vision:2b",
          "size_text": "2.4GB",
          "size_bytes": 2576980377,
          "context": "16K",
          "input": "Vision"
        },
        {
          "tag": "granite3.2-vision:2b-q4_K_M",
          "size_text": "2.4GB",
          "size_bytes": 2576980377,
          "context": "16K",
          "input": "Vision"
        },
        {
          "tag": "granite3.2-vision:2b-q8_0",
          "size_text": "3.6GB",
          "size_bytes": 3865470566,
          "context": "16K",
          "input": "Vision"
        },
        {
          "tag": "granite3.2-vision:2b-fp16",
          "size_text": "6.0GB",
          "size_bytes": 6442450944,
          "context": "16K",
          "input": "Vision"
        }
      ],
      "tags_count": 5
    },
    {
      "slug": "dolphin-mistral",
      "capabilities": [],
      "pulls": 409900,
      "pulls_text": "409.9K \n  Downloads",
      "blurb": "dolphin-mistral The uncensored Dolphin model based on Mistral that excels at coding tasks. Updated to version 2.8. 7b 120 Tags Updated 1 year ago",
      "name": "The uncensored Dolphin model based on Mistral that excels at coding tasks. Updated to version 2.8.",
      "description": "Readme The Dolphin model by Eric Hartford, based on Mistral version 0.2 released in March 2024. This model is uncensored, available for both commercial and non-commercial use, and excels at coding. Versions Tag Date Notes v2.8 latest 03/31/2024 Based on Mistral 0.2 with support for a context window of 32K tokens. v2.6 12/27/2023 Fixed a training configuration issue that improved quality, and improvements to the training dataset for empathy. v2.2.1 10/30/2023 This is a checkpoint release, to fix overfit training v2.2 10/29/2023 Added conversation and empathy data. v2.1 10/11/2023 Enhanced with the airoboros dataset. v2.0 10/2/2023 Initial release of the model. References HuggingFace Write Preview <img src=\"https://github.com/jmorganca/ollama/assets/251292/c65aad4e-7a58-4645-97f2-5e8ca158275f\" width=\"280\" /> The Dolphin model by Eric Hartford, based on [Mistral](https://ollama.ai/library/mistral) version 0.2 released in March 2024. This model is uncensored, available for both commercial and non-commercial use, and excels at coding. ## Versions | Tag | Date | Notes | | --------------- | ---------- | ----------------------------------------------------------------------------------------------------------------- | | `v2.8` `latest` | 03/31/2024 | Based on Mistral 0.2 with support for a context window of 32K tokens. | | `v2.6` | 12/27/2023 | Fixed a training configuration issue that improved quality, and improvements to the training dataset for empathy. | | `v2.2.1` | 10/30/2023 | This is a checkpoint release, to fix overfit training | | `v2.2` | 10/29/2023 | Added conversation and empathy data. | | `v2.1` | 10/11/2023 | Enhanced with the airoboros dataset. | | `v2.0` | 10/2/2023 | Initial release of the model. | ## References [HuggingFace](https://huggingface.co/cognitivecomputations) Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "dolphin-mistral:latest",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:v2",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:v2.1",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:v2.2",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:v2.2.1",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:v2.6",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:v2.8",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2-q2_K",
          "size_text": "3.1GB",
          "size_bytes": 3328599654,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2-q3_K_S",
          "size_text": "3.2GB",
          "size_bytes": 3435973836,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2-q3_K_M",
          "size_text": "3.5GB",
          "size_bytes": 3758096384,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2-q3_K_L",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2-q4_0",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2-q4_1",
          "size_text": "4.6GB",
          "size_bytes": 4939212390,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2-q4_K_S",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2-q4_K_M",
          "size_text": "4.4GB",
          "size_bytes": 4724464025,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2-q5_0",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2-q5_1",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2-q5_K_S",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2-q5_K_M",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2-q6_K",
          "size_text": "5.9GB",
          "size_bytes": 6335076761,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2-q8_0",
          "size_text": "7.7GB",
          "size_bytes": 8267812044,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2-fp16",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.1",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.1-q2_K",
          "size_text": "3.1GB",
          "size_bytes": 3328599654,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.1-q3_K_S",
          "size_text": "3.2GB",
          "size_bytes": 3435973836,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.1-q3_K_M",
          "size_text": "3.5GB",
          "size_bytes": 3758096384,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.1-q3_K_L",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.1-q4_0",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.1-q4_1",
          "size_text": "4.6GB",
          "size_bytes": 4939212390,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.1-q4_K_S",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.1-q4_K_M",
          "size_text": "4.4GB",
          "size_bytes": 4724464025,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.1-q5_0",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.1-q5_1",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.1-q5_K_S",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.1-q5_K_M",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.1-q6_K",
          "size_text": "5.9GB",
          "size_bytes": 6335076761,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.1-q8_0",
          "size_text": "7.7GB",
          "size_bytes": 8267812044,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.1-fp16",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.2",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.2-q2_K",
          "size_text": "3.1GB",
          "size_bytes": 3328599654,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.2-q3_K_S",
          "size_text": "3.2GB",
          "size_bytes": 3435973836,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.2-q3_K_M",
          "size_text": "3.5GB",
          "size_bytes": 3758096384,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.2-q3_K_L",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.2-q4_0",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.2-q4_1",
          "size_text": "4.6GB",
          "size_bytes": 4939212390,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.2-q4_K_S",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.2-q4_K_M",
          "size_text": "4.4GB",
          "size_bytes": 4724464025,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.2-q5_0",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.2-q5_1",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.2-q5_K_S",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.2-q5_K_M",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.2-q6_K",
          "size_text": "5.9GB",
          "size_bytes": 6335076761,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.2-q8_0",
          "size_text": "7.7GB",
          "size_bytes": 8267812044,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.2-fp16",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.2.1",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.2.1-q2_K",
          "size_text": "3.1GB",
          "size_bytes": 3328599654,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.2.1-q3_K_S",
          "size_text": "3.2GB",
          "size_bytes": 3435973836,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.2.1-q3_K_M",
          "size_text": "3.5GB",
          "size_bytes": 3758096384,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.2.1-q3_K_L",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.2.1-q4_0",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.2.1-q4_1",
          "size_text": "4.6GB",
          "size_bytes": 4939212390,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.2.1-q4_K_S",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.2.1-q4_K_M",
          "size_text": "4.4GB",
          "size_bytes": 4724464025,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.2.1-q5_0",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.2.1-q5_1",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.2.1-q5_K_S",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.2.1-q5_K_M",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.2.1-q6_K",
          "size_text": "5.9GB",
          "size_bytes": 6335076761,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.2.1-q8_0",
          "size_text": "7.7GB",
          "size_bytes": 8267812044,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.2.1-fp16",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.6",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.6-dpo-laser",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.6-dpo-laser-q2_K",
          "size_text": "3.1GB",
          "size_bytes": 3328599654,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.6-q2_K",
          "size_text": "3.1GB",
          "size_bytes": 3328599654,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.6-dpo-laser-q3_K_S",
          "size_text": "3.2GB",
          "size_bytes": 3435973836,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.6-q3_K_S",
          "size_text": "3.2GB",
          "size_bytes": 3435973836,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.6-dpo-laser-q3_K_M",
          "size_text": "3.5GB",
          "size_bytes": 3758096384,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.6-q3_K_M",
          "size_text": "3.5GB",
          "size_bytes": 3758096384,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.6-dpo-laser-q3_K_L",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.6-q3_K_L",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.6-dpo-laser-q4_0",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.6-q4_0",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.6-dpo-laser-q4_1",
          "size_text": "4.6GB",
          "size_bytes": 4939212390,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.6-q4_1",
          "size_text": "4.6GB",
          "size_bytes": 4939212390,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.6-dpo-laser-q4_K_S",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.6-q4_K_S",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.6-dpo-laser-q4_K_M",
          "size_text": "4.4GB",
          "size_bytes": 4724464025,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.6-q4_K_M",
          "size_text": "4.4GB",
          "size_bytes": 4724464025,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.6-dpo-laser-q5_0",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.6-q5_0",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.6-dpo-laser-q5_1",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.6-q5_1",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.6-dpo-laser-q5_K_S",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.6-q5_K_S",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.6-dpo-laser-q5_K_M",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.6-q5_K_M",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.6-dpo-laser-q6_K",
          "size_text": "5.9GB",
          "size_bytes": 6335076761,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.6-q6_K",
          "size_text": "5.9GB",
          "size_bytes": 6335076761,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.6-dpo-laser-q8_0",
          "size_text": "7.7GB",
          "size_bytes": 8267812044,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.6-q8_0",
          "size_text": "7.7GB",
          "size_bytes": 8267812044,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.6-dpo-laser-fp16",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.6-fp16",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.8",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.8-q2_K",
          "size_text": "2.7GB",
          "size_bytes": 2899102924,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.8-q3_K_S",
          "size_text": "3.2GB",
          "size_bytes": 3435973836,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.8-q3_K_M",
          "size_text": "3.5GB",
          "size_bytes": 3758096384,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.8-q3_K_L",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.8-q4_0",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.8-q4_1",
          "size_text": "4.6GB",
          "size_bytes": 4939212390,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.8-q4_K_S",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.8-q4_K_M",
          "size_text": "4.4GB",
          "size_bytes": 4724464025,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.8-q5_0",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.8-q5_1",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.8-q5_K_S",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.8-q5_K_M",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.8-q6_K",
          "size_text": "5.9GB",
          "size_bytes": 6335076761,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.8-q8_0",
          "size_text": "7.7GB",
          "size_bytes": 8267812044,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dolphin-mistral:7b-v2.8-fp16",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "32K",
          "input": "Text"
        }
      ],
      "tags_count": 120
    },
    {
      "slug": "moondream",
      "capabilities": [
        "thinking",
        "vision"
      ],
      "pulls": 377400,
      "pulls_text": "377.4K \n  Downloads",
      "blurb": "moondream moondream2 is a small language model designed to run efficiently on edge devices. 1.8b 18 Tags Updated 1 year ago",
      "name": "moondream2 is a small vision language model designed to run efficiently on edge devices.",
      "description": "Readme 🌔 moondream 2 Moondream 2 requires Ollama 0.1.33 or later “a tiny vision language model that kicks ass and runs anywhere” Limitations The model may generate inaccurate statements, and struggle to understand intricate or nuanced instructions. The model may not be free from societal biases. Users should be aware of this and exercise caution and critical thinking when using the model. The model may generate offensive, inappropriate, or hurtful content if it is prompted to do so. References GitHub Hugging Face Write Preview # 🌔 moondream 2 > Moondream 2 requires Ollama 0.1.33 or later _\"a tiny vision language model that kicks ass and runs anywhere\"_ ### Limitations * The model may generate inaccurate statements, and struggle to understand intricate or nuanced instructions. * The model may not be free from societal biases. Users should be aware of this and exercise caution and critical thinking when using the model. * The model may generate offensive, inappropriate, or hurtful content if it is prompted to do so. ## References [GitHub](https://github.com/vikhyat/moondream) [Hugging Face](https://huggingface.co/vikhyatk/moondream2) Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "moondream:latest",
          "size_text": "1.7GB",
          "size_bytes": 1825361100,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "moondream:v2",
          "size_text": "1.7GB",
          "size_bytes": 1825361100,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "moondream:1.8b",
          "size_text": "1.7GB",
          "size_bytes": 1825361100,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "moondream:1.8b-v2-q2_K",
          "size_text": "1.5GB",
          "size_bytes": 1610612736,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "moondream:1.8b-v2-q3_K_S",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "moondream:1.8b-v2-q3_K_M",
          "size_text": "1.7GB",
          "size_bytes": 1825361100,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "moondream:1.8b-v2-q3_K_L",
          "size_text": "1.7GB",
          "size_bytes": 1825361100,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "moondream:1.8b-v2-q4_0",
          "size_text": "1.7GB",
          "size_bytes": 1825361100,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "moondream:1.8b-v2-q4_1",
          "size_text": "1.8GB",
          "size_bytes": 1932735283,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "moondream:1.8b-v2-q4_K_S",
          "size_text": "1.7GB",
          "size_bytes": 1825361100,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "moondream:1.8b-v2-q4_K_M",
          "size_text": "1.8GB",
          "size_bytes": 1932735283,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "moondream:1.8b-v2-q5_0",
          "size_text": "1.9GB",
          "size_bytes": 2040109465,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "moondream:1.8b-v2-q5_1",
          "size_text": "2.0GB",
          "size_bytes": 2147483648,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "moondream:1.8b-v2-q5_K_S",
          "size_text": "1.9GB",
          "size_bytes": 2040109465,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "moondream:1.8b-v2-q5_K_M",
          "size_text": "2.0GB",
          "size_bytes": 2147483648,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "moondream:1.8b-v2-q6_K",
          "size_text": "2.1GB",
          "size_bytes": 2254857830,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "moondream:1.8b-v2-q8_0",
          "size_text": "2.4GB",
          "size_bytes": 2576980377,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "moondream:1.8b-v2-fp16",
          "size_text": "3.7GB",
          "size_bytes": 3972844748,
          "context": "2K",
          "input": "Text"
        }
      ],
      "tags_count": 18
    },
    {
      "slug": "command-r",
      "capabilities": [
        "tools"
      ],
      "pulls": 361900,
      "pulls_text": "361.9K \n  Downloads",
      "blurb": "command-r Command R is a Large Language Model optimized for conversational interaction and long context tasks. 35b 32 Tags Updated 1 year ago",
      "name": "Command R is a Large Language Model optimized for conversational interaction and long context tasks.",
      "description": "Readme Command R is a generative model optimized for long context tasks such as retrieval-augmented generation (RAG) and using external APIs and tools. As a model built for companies to implement at scale, Command R boasts: Strong accuracy on RAG and Tool Use Low latency, and high throughput Longer 128k context Strong capabilities across 10 key languages There are currently two versions of Command R: Original release tagged v0.1 August 2024 update tagged 08-2024 References Blog Post Hugging Face Write Preview <img src=\"https://github.com/ollama/ollama/assets/251292/0f8ef1fc-f0ba-4e69-b171-36c387aa5349\" width=\"480\" /> Command R is a generative model optimized for long context tasks such as retrieval-augmented generation (RAG) and using external APIs and tools. As a model built for companies to implement at scale, Command R boasts: - Strong accuracy on RAG and Tool Use - Low latency, and high throughput - Longer 128k context - Strong capabilities across 10 key languages There are currently two versions of Command R: - Original release tagged **v0.1** - August 2024 update tagged **08-2024** ## References - [Blog Post](https://txt.cohere.com/command-r/) - [Hugging Face](https://huggingface.co/CohereForAI/c4ai-command-r-v01) Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "command-r:latest",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "command-r:v0.1",
          "size_text": "20GB",
          "size_bytes": 21474836480,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "command-r:35b",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "command-r:35b-08-2024-q2_K",
          "size_text": "13GB",
          "size_bytes": 13958643712,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "command-r:35b-08-2024-q3_K_S",
          "size_text": "15GB",
          "size_bytes": 16106127360,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "command-r:35b-08-2024-q3_K_M",
          "size_text": "16GB",
          "size_bytes": 17179869184,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "command-r:35b-08-2024-q3_K_L",
          "size_text": "18GB",
          "size_bytes": 19327352832,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "command-r:35b-08-2024-q4_0",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "command-r:35b-08-2024-q4_1",
          "size_text": "21GB",
          "size_bytes": 22548578304,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "command-r:35b-08-2024-q4_K_S",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "command-r:35b-08-2024-q4_K_M",
          "size_text": "20GB",
          "size_bytes": 21474836480,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "command-r:35b-08-2024-q5_0",
          "size_text": "22GB",
          "size_bytes": 23622320128,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "command-r:35b-08-2024-q5_1",
          "size_text": "24GB",
          "size_bytes": 25769803776,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "command-r:35b-08-2024-q5_K_S",
          "size_text": "22GB",
          "size_bytes": 23622320128,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "command-r:35b-08-2024-q5_K_M",
          "size_text": "23GB",
          "size_bytes": 24696061952,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "command-r:35b-08-2024-q6_K",
          "size_text": "27GB",
          "size_bytes": 28991029248,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "command-r:35b-08-2024-q8_0",
          "size_text": "34GB",
          "size_bytes": 36507222016,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "command-r:35b-08-2024-fp16",
          "size_text": "65GB",
          "size_bytes": 69793218560,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "command-r:35b-v0.1-q2_K",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "command-r:35b-v0.1-q3_K_S",
          "size_text": "16GB",
          "size_bytes": 17179869184,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "command-r:35b-v0.1-q3_K_M",
          "size_text": "18GB",
          "size_bytes": 19327352832,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "command-r:35b-v0.1-q3_K_L",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "command-r:35b-v0.1-q4_0",
          "size_text": "20GB",
          "size_bytes": 21474836480,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "command-r:35b-v0.1-q4_1",
          "size_text": "22GB",
          "size_bytes": 23622320128,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "command-r:35b-v0.1-q4_K_S",
          "size_text": "20GB",
          "size_bytes": 21474836480,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "command-r:35b-v0.1-q4_K_M",
          "size_text": "22GB",
          "size_bytes": 23622320128,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "command-r:35b-v0.1-q5_1",
          "size_text": "26GB",
          "size_bytes": 27917287424,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "command-r:35b-v0.1-q5_K_S",
          "size_text": "24GB",
          "size_bytes": 25769803776,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "command-r:35b-v0.1-q5_K_M",
          "size_text": "25GB",
          "size_bytes": 26843545600,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "command-r:35b-v0.1-q6_K",
          "size_text": "29GB",
          "size_bytes": 31138512896,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "command-r:35b-v0.1-q8_0",
          "size_text": "37GB",
          "size_bytes": 39728447488,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "command-r:35b-v0.1-fp16",
          "size_text": "70GB",
          "size_bytes": 75161927680,
          "context": "128K",
          "input": "Text"
        }
      ],
      "tags_count": 32
    },
    {
      "slug": "deepcoder",
      "capabilities": [
        "reasoning"
      ],
      "pulls": 355700,
      "pulls_text": "355.7K \n  Downloads",
      "blurb": "deepcoder DeepCoder is a fully open-Source 14B coder model at O3-mini level, with a 1.5B version also available. 1.5b 14b 9 Tags Updated 6 months ago",
      "name": "DeepCoder is a fully open-Source 14B coder model at O3-mini level, with a 1.5B version also available.",
      "description": "Readme Through a joint collaboration between the Agentica team and Together AI, DeepCoder-14B-Preview is a code reasoning model finetuned from Deepseek-R1-Distilled-Qwen-14B via distributed RL. It achieves an impressive 60.6% Pass@1 accuracy on LiveCodeBench (+8% improvement), matching the performance of o3-mini-2025-01-031 (Low) and o1-2024-12-17 with just 14B parameters Evaluation Model LCB (v5)(8/1/24-2/1/25) Codeforces Rating Codeforces Percentile HumanEval+ DeepCoder-14B-Preview 60.6 1936 95.3 92.6 DeepSeek-R1-Distill-Qwen-14B 53.0 1791 92.7 92.0 O1-2024-12-17 (Low) 59.5 1991 96.1 90.8 O3-Mini-2025-1-31 (Low) 60.9 1918 94.9 92.6 O1-Preview 42.7 1658 88.5 89 Deepseek-R1 62.8 1948 95.4 92.6 Llama-4-Behemoth 49.4 - - - References Blog post Write Preview <img src=\"/assets/library/deepcoder/7e57e9ba-4b82-4a7a-82f6-3ce9d4cba1c8\" width=\"100\" /> Through a joint collaboration between the Agentica team and Together AI, DeepCoder-14B-Preview is a code reasoning model finetuned from Deepseek-R1-Distilled-Qwen-14B via distributed RL. It achieves an impressive 60.6% Pass@1 accuracy on LiveCodeBench (+8% improvement), matching the performance of `o3-mini-2025-01-031` (Low) and `o1-2024-12-17` with just 14B parameters ## Evaluation | **Model** | LCB (v5)(8/1/24-2/1/25) | Codeforces Rating | Codeforces Percentile | HumanEval+ | | --- | --- | --- | --- | --- | | **DeepCoder-14B-Preview** | ***60.6*** | ***1936*** | ***95.3*** | ***92.6*** | | **DeepSeek-R1-Distill-Qwen-14B** | 53.0 | 1791 | 92.7 | 92.0 | | **O1-2024-12-17 (Low)** | 59.5 | **1991** | **96.1** | 90.8 | | **O3-Mini-2025-1-31 (Low)** | **60.9** | 1918 | 94.9 | 92.6 | | **O1-Preview** | 42.7 | 1658 | 88.5 | 89 | | **Deepseek-R1** | 62.8 | 1948 | 95.4 | 92.6 | | **Llama-4-Behemoth** | 49.4 | - | - | - | ## References [Blog post](https://pretty-radio-b75.notion.site/DeepCoder-A-Fully-Open-Source-14B-Coder-at-O3-mini-Level-1cf81902c14680b3bee5eb349a512a51) Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, ",
      "variants": [
        {
          "tag": "deepcoder:latest",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "deepcoder:1.5b",
          "size_text": "1.1GB",
          "size_bytes": 1181116006,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "deepcoder:14b",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "deepcoder:1.5b-preview-q4_K_M",
          "size_text": "1.1GB",
          "size_bytes": 1181116006,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "deepcoder:1.5b-preview-q8_0",
          "size_text": "1.9GB",
          "size_bytes": 2040109465,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "deepcoder:1.5b-preview-fp16",
          "size_text": "3.6GB",
          "size_bytes": 3865470566,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "deepcoder:14b-preview-q4_K_M",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "deepcoder:14b-preview-q8_0",
          "size_text": "16GB",
          "size_bytes": 17179869184,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "deepcoder:14b-preview-fp16",
          "size_text": "30GB",
          "size_bytes": 32212254720,
          "context": "128K",
          "input": "Text"
        }
      ],
      "tags_count": 9
    },
    {
      "slug": "mistral-small3.1",
      "capabilities": [
        "multimodal",
        "reasoning",
        "tools",
        "vision"
      ],
      "pulls": 349500,
      "pulls_text": "349.5K \n  Downloads",
      "blurb": "mistral-small3.1 Building upon Mistral Small 3, Mistral Small 3.1 (2503) adds state-of-the-art understanding and enhances long context capabilities up to 128k tokens without compromising text performance. 24b 5 Tags Updated 6 months ago",
      "name": "Building upon Mistral Small 3, Mistral Small 3.1 (2503) adds state-of-the-art vision understanding and enhances long context capabilities up to 128k tokens without compromising text performance.",
      "description": "Readme Note: this model requires Ollama 0.6.5 or higher. Download Ollama Building on Mistral Small 3 , this new model comes with improved text performance, multimodal understanding, and an expanded context window of up to 128k tokens. The model outperforms comparable models like Gemma 3 and GPT-4o Mini, while delivering inference speeds of 150 tokens per second. Mistral Small 3.1 is released under an Apache 2.0 license. Key features and capabilities Lightweight: Mistral Small 3.1 can run on a single RTX 4090 or a Mac with 32GB RAM. This makes it a great fit for on-device use cases. Fast-response conversational assistance: Ideal for virtual assistants and other applications where quick, accurate responses are essential. Low-latency function calling: Capable of rapid function execution within automated or agentic workflows Fine-tuning for specialized domains: Mistral Small 3.1 can be fine-tuned to specialize in specific domains, creating accurate subject matter experts. This is particularly useful in fields like legal advice, medical diagnostics, and technical support. Foundation for advanced reasoning: We continue to be impressed by how the community builds on top of open Mistral models. Just in the last few weeks, we have seen several excellent reasoning models built on Mistral Small 3, such as the DeepHermes 24B by Nous Research. To that end, we are releasing both base and instruct checkpoints for Mistral Small 3.1 to enable further downstream customization of the model. References Blog post Write Preview > Note: this model requires Ollama 0.6.5 or higher. [Download Ollama](https://ollama.com/download) <img src=\"/assets/library/mistral-small3.1/88f81c26-7028-4f08-b906-92b873d5536e\" width=\"120\" /> Building on [Mistral Small 3](https://ollama.com/library/mistral-small), this new model comes with improved text performance, multimodal understanding, and an expanded context window of up to 128k tokens. The model outperforms comparable models like Gemma 3 and GPT-4o Mini",
      "variants": [
        {
          "tag": "mistral-small3.1:latest",
          "size_text": "15GB",
          "size_bytes": 16106127360,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "mistral-small3.1:24b",
          "size_text": "15GB",
          "size_bytes": 16106127360,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "mistral-small3.1:24b-instruct-2503-q4_K_M",
          "size_text": "15GB",
          "size_bytes": 16106127360,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "mistral-small3.1:24b-instruct-2503-q8_0",
          "size_text": "26GB",
          "size_bytes": 27917287424,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "mistral-small3.1:24b-instruct-2503-fp16",
          "size_text": "48GB",
          "size_bytes": 51539607552,
          "context": "128K",
          "input": "Text"
        }
      ],
      "tags_count": 5
    },
    {
      "slug": "hermes3",
      "capabilities": [
        "reasoning",
        "tools"
      ],
      "pulls": 343600,
      "pulls_text": "343.6K \n  Downloads",
      "blurb": "hermes3 Hermes 3 is the latest version of the flagship Hermes series of LLMs by Nous Research 3b 8b 70b 405b 65 Tags Updated 10 months ago",
      "name": "Hermes 3 is the latest version of the flagship Hermes series of LLMs by Nous Research",
      "description": "Readme Hermes 3 is a generalist language model with many improvements over Hermes 2, including advanced agentic capabilities, much better roleplaying, reasoning, multi-turn conversation, long context coherence, and improvements across the board. The ethos of the Hermes series of models is focused on aligning LLMs to the user, with powerful steering capabilities and control given to the end user. The Hermes 3 series builds and expands on the Hermes 2 set of capabilities, including more powerful and reliable function calling and structured output capabilities, generalist assistant capabilities, and improved code generation skills. Hermes 3 series contains four models: 3B , 8B , 70B , and 405B References: Hermes 3 Technical Report Hugging Face Write Preview ![image.png](https://ollama.com/assets/library/hermes3/e8584a99-0cf4-4d0a-9032-3af2fd7db91d) Hermes 3 is a generalist language model with many improvements over Hermes 2, including advanced agentic capabilities, much better roleplaying, reasoning, multi-turn conversation, long context coherence, and improvements across the board. The ethos of the Hermes series of models is focused on aligning LLMs to the user, with powerful steering capabilities and control given to the end user. The Hermes 3 series builds and expands on the Hermes 2 set of capabilities, including more powerful and reliable function calling and structured output capabilities, generalist assistant capabilities, and improved code generation skills. **Hermes 3** series contains four models: **3B**, **8B**, **70B**, and **405B** ## References: * [Hermes 3 Technical Report](https://arxiv.org/abs/2408.11857) * [Hugging Face](https://huggingface.co/collections/NousResearch/hermes-3-66bd6c01399b14b08fe335ea) Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "hermes3:latest",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "hermes3:3b",
          "size_text": "2.0GB",
          "size_bytes": 2147483648,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "hermes3:8b",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "hermes3:70b",
          "size_text": "40GB",
          "size_bytes": 42949672960,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "hermes3:405b",
          "size_text": "229GB",
          "size_bytes": 245886877696,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "hermes3:3b-llama3.2-q2_K",
          "size_text": "1.4GB",
          "size_bytes": 1503238553,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "hermes3:3b-llama3.2-q3_K_S",
          "size_text": "1.5GB",
          "size_bytes": 1610612736,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "hermes3:3b-llama3.2-q3_K_M",
          "size_text": "1.7GB",
          "size_bytes": 1825361100,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "hermes3:3b-llama3.2-q3_K_L",
          "size_text": "1.8GB",
          "size_bytes": 1932735283,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "hermes3:3b-llama3.2-q4_0",
          "size_text": "1.9GB",
          "size_bytes": 2040109465,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "hermes3:3b-llama3.2-q4_1",
          "size_text": "2.1GB",
          "size_bytes": 2254857830,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "hermes3:3b-llama3.2-q4_K_S",
          "size_text": "1.9GB",
          "size_bytes": 2040109465,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "hermes3:3b-llama3.2-q4_K_M",
          "size_text": "2.0GB",
          "size_bytes": 2147483648,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "hermes3:3b-llama3.2-q5_0",
          "size_text": "2.3GB",
          "size_bytes": 2469606195,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "hermes3:3b-llama3.2-q5_1",
          "size_text": "2.4GB",
          "size_bytes": 2576980377,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "hermes3:3b-llama3.2-q5_K_S",
          "size_text": "2.3GB",
          "size_bytes": 2469606195,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "hermes3:3b-llama3.2-q5_K_M",
          "size_text": "2.3GB",
          "size_bytes": 2469606195,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "hermes3:3b-llama3.2-q6_K",
          "size_text": "2.6GB",
          "size_bytes": 2791728742,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "hermes3:3b-llama3.2-q8_0",
          "size_text": "3.4GB",
          "size_bytes": 3650722201,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "hermes3:3b-llama3.2-fp16",
          "size_text": "6.4GB",
          "size_bytes": 6871947673,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "hermes3:8b-llama3.1-q2_K",
          "size_text": "3.2GB",
          "size_bytes": 3435973836,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "hermes3:8b-llama3.1-q3_K_S",
          "size_text": "3.7GB",
          "size_bytes": 3972844748,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "hermes3:8b-llama3.1-q3_K_M",
          "size_text": "4.0GB",
          "size_bytes": 4294967296,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "hermes3:8b-llama3.1-q3_K_L",
          "size_text": "4.3GB",
          "size_bytes": 4617089843,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "hermes3:8b-llama3.1-q4_0",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "hermes3:8b-llama3.1-q4_1",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "hermes3:8b-llama3.1-q4_K_S",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "hermes3:8b-llama3.1-q4_K_M",
          "size_text": "4.9GB",
          "size_bytes": 5261334937,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "hermes3:8b-llama3.1-q5_0",
          "size_text": "5.6GB",
          "size_bytes": 6012954214,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "hermes3:8b-llama3.1-q5_1",
          "size_text": "6.1GB",
          "size_bytes": 6549825126,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "hermes3:8b-llama3.1-q5_K_S",
          "size_text": "5.6GB",
          "size_bytes": 6012954214,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "hermes3:8b-llama3.1-q5_K_M",
          "size_text": "5.7GB",
          "size_bytes": 6120328396,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "hermes3:8b-llama3.1-q6_K",
          "size_text": "6.6GB",
          "size_bytes": 7086696038,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "hermes3:8b-llama3.1-q8_0",
          "size_text": "8.5GB",
          "size_bytes": 9126805504,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "hermes3:8b-llama3.1-fp16",
          "size_text": "16GB",
          "size_bytes": 17179869184,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "hermes3:70b-llama3.1-q2_K",
          "size_text": "26GB",
          "size_bytes": 27917287424,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "hermes3:70b-llama3.1-q3_K_S",
          "size_text": "31GB",
          "size_bytes": 33285996544,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "hermes3:70b-llama3.1-q3_K_M",
          "size_text": "34GB",
          "size_bytes": 36507222016,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "hermes3:70b-llama3.1-q3_K_L",
          "size_text": "37GB",
          "size_bytes": 39728447488,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "hermes3:70b-llama3.1-q4_0",
          "size_text": "40GB",
          "size_bytes": 42949672960,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "hermes3:70b-llama3.1-q4_1",
          "size_text": "44GB",
          "size_bytes": 47244640256,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "hermes3:70b-llama3.1-q4_K_S",
          "size_text": "40GB",
          "size_bytes": 42949672960,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "hermes3:70b-llama3.1-q4_K_M",
          "size_text": "43GB",
          "size_bytes": 46170898432,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "hermes3:70b-llama3.1-q5_0",
          "size_text": "49GB",
          "size_bytes": 52613349376,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "hermes3:70b-llama3.1-q5_1",
          "size_text": "53GB",
          "size_bytes": 56908316672,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "hermes3:70b-llama3.1-q5_K_S",
          "size_text": "49GB",
          "size_bytes": 52613349376,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "hermes3:70b-llama3.1-q5_K_M",
          "size_text": "50GB",
          "size_bytes": 53687091200,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "hermes3:70b-llama3.1-q6_K",
          "size_text": "58GB",
          "size_bytes": 62277025792,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "hermes3:70b-llama3.1-q8_0",
          "size_text": "75GB",
          "size_bytes": 80530636800,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "hermes3:70b-llama3.1-fp16",
          "size_text": "141GB",
          "size_bytes": 151397597184,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "hermes3:405b-llama3.1-q2_K",
          "size_text": "149GB",
          "size_bytes": 159987531776,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "hermes3:405b-llama3.1-q3_K_S",
          "size_text": "175GB",
          "size_bytes": 187904819200,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "hermes3:405b-llama3.1-q3_K_M",
          "size_text": "195GB",
          "size_bytes": 209379655680,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "hermes3:405b-llama3.1-q3_K_L",
          "size_text": "213GB",
          "size_bytes": 228707008512,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "hermes3:405b-llama3.1-q4_0",
          "size_text": "229GB",
          "size_bytes": 245886877696,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "hermes3:405b-llama3.1-q4_1",
          "size_text": "254GB",
          "size_bytes": 272730423296,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "hermes3:405b-llama3.1-q4_K_S",
          "size_text": "231GB",
          "size_bytes": 248034361344,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "hermes3:405b-llama3.1-q4_K_M",
          "size_text": "243GB",
          "size_bytes": 260919263232,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "hermes3:405b-llama3.1-q5_0",
          "size_text": "279GB",
          "size_bytes": 299573968896,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "hermes3:405b-llama3.1-q5_1",
          "size_text": "305GB",
          "size_bytes": 327491256320,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "hermes3:405b-llama3.1-q5_K_S",
          "size_text": "279GB",
          "size_bytes": 299573968896,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "hermes3:405b-llama3.1-q5_K_M",
          "size_text": "287GB",
          "size_bytes": 308163903488,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "hermes3:405b-llama3.1-q6_K",
          "size_text": "333GB",
          "size_bytes": 357556027392,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "hermes3:405b-llama3.1-q8_0",
          "size_text": "431GB",
          "size_bytes": 462782726144,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "hermes3:405b-llama3.1-fp16",
          "size_text": "812GB",
          "size_bytes": 871878361088,
          "context": "128K",
          "input": "Text"
        }
      ],
      "tags_count": 65
    },
    {
      "slug": "phi3.5",
      "capabilities": [
        "reasoning"
      ],
      "pulls": 342300,
      "pulls_text": "342.3K \n  Downloads",
      "blurb": "phi3.5 A lightweight AI model with 3.8 billion parameters with performance overtaking similarly and larger sized models. 3.8b 17 Tags Updated 1 year ago",
      "name": "A lightweight AI model with 3.8 billion parameters with performance overtaking similarly and larger sized models.",
      "description": "Readme Phi-3.5-mini is a lightweight, state-of-the-art open model built upon datasets used for Phi-3 - synthetic data and filtered publicly available websites with a focus on very high-quality, reasoning dense data. The model belongs to the Phi-3 model family and supports 128K token context length. The model underwent a rigorous enhancement process, incorporating both supervised fine-tuning, proximal policy optimization, and direct preference optimization to ensure precise instruction adherence and robust safety measures. Long Context Phi-3.5-mini supports 128K context length, therefore the model is capable of several long context tasks including long document/meeting summarization, long document QA, long document information retrieval. References Hugging Face Write Preview <img src=\"https://ollama.com/assets/library/phi3.5/dbf19a17-e3fd-46b6-a059-e6b4f1fae59f\" width=\"320\" /> Phi-3.5-mini is a lightweight, state-of-the-art open model built upon datasets used for Phi-3 - synthetic data and filtered publicly available websites with a focus on very high-quality, reasoning dense data. The model belongs to the Phi-3 model family and supports 128K token context length. The model underwent a rigorous enhancement process, incorporating both supervised fine-tuning, proximal policy optimization, and direct preference optimization to ensure precise instruction adherence and robust safety measures. ### Long Context Phi-3.5-mini supports 128K context length, therefore the model is capable of several long context tasks including long document/meeting summarization, long document QA, long document information retrieval. ## References [Hugging Face](https://huggingface.co/microsoft/Phi-3.5-mini-instruct) Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "phi3.5:latest",
          "size_text": "2.2GB",
          "size_bytes": 2362232012,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3.5:3.8b",
          "size_text": "2.2GB",
          "size_bytes": 2362232012,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3.5:3.8b-mini-instruct-q2_K",
          "size_text": "1.4GB",
          "size_bytes": 1503238553,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3.5:3.8b-mini-instruct-q3_K_S",
          "size_text": "1.7GB",
          "size_bytes": 1825361100,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3.5:3.8b-mini-instruct-q3_K_M",
          "size_text": "2.0GB",
          "size_bytes": 2147483648,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3.5:3.8b-mini-instruct-q3_K_L",
          "size_text": "2.1GB",
          "size_bytes": 2254857830,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3.5:3.8b-mini-instruct-q4_0",
          "size_text": "2.2GB",
          "size_bytes": 2362232012,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3.5:3.8b-mini-instruct-q4_1",
          "size_text": "2.4GB",
          "size_bytes": 2576980377,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3.5:3.8b-mini-instruct-q4_K_S",
          "size_text": "2.2GB",
          "size_bytes": 2362232012,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3.5:3.8b-mini-instruct-q4_K_M",
          "size_text": "2.4GB",
          "size_bytes": 2576980377,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3.5:3.8b-mini-instruct-q5_0",
          "size_text": "2.6GB",
          "size_bytes": 2791728742,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3.5:3.8b-mini-instruct-q5_1",
          "size_text": "2.9GB",
          "size_bytes": 3113851289,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3.5:3.8b-mini-instruct-q5_K_S",
          "size_text": "2.6GB",
          "size_bytes": 2791728742,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3.5:3.8b-mini-instruct-q5_K_M",
          "size_text": "2.8GB",
          "size_bytes": 3006477107,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3.5:3.8b-mini-instruct-q6_K",
          "size_text": "3.1GB",
          "size_bytes": 3328599654,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3.5:3.8b-mini-instruct-q8_0",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3.5:3.8b-mini-instruct-fp16",
          "size_text": "7.6GB",
          "size_bytes": 8160437862,
          "context": "4K",
          "input": "Text"
        }
      ],
      "tags_count": 17
    },
    {
      "slug": "yi",
      "capabilities": [],
      "pulls": 307600,
      "pulls_text": "307.6K \n  Downloads",
      "blurb": "yi Yi 1.5 is a high-performing, bilingual language model. 6b 9b 34b 174 Tags Updated 1 year ago",
      "name": "Yi 1.5 is a high-performing, bilingual language model.",
      "description": "Readme Yi is a series of large language models trained on a high-quality corpus of 3 trillion tokens that support both the English and Chinese languages. References HuggingFace Write Preview <img src=\"https://github.com/jmorganca/ollama/assets/251292/ac83ff77-b8aa-4dae-a619-d9603ef2edaa\" alt=\"yi\" width=\"200\" /> Yi is a series of large language models trained on a high-quality corpus of 3 trillion tokens that support both the English and Chinese languages. ## References [HuggingFace](https://huggingface.co/01-ai/Yi-34B) Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "yi:latest",
          "size_text": "3.5GB",
          "size_bytes": 3758096384,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:v1.5",
          "size_text": "3.5GB",
          "size_bytes": 3758096384,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:6b",
          "size_text": "3.5GB",
          "size_bytes": 3758096384,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:9b",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:34b",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:6b-200k",
          "size_text": "3.5GB",
          "size_bytes": 3758096384,
          "context": "195K",
          "input": "Text"
        },
        {
          "tag": "yi:6b-200k-q2_K",
          "size_text": "2.6GB",
          "size_bytes": 2791728742,
          "context": "195K",
          "input": "Text"
        },
        {
          "tag": "yi:6b-200k-q3_K_S",
          "size_text": "2.7GB",
          "size_bytes": 2899102924,
          "context": "195K",
          "input": "Text"
        },
        {
          "tag": "yi:6b-200k-q3_K_M",
          "size_text": "3.0GB",
          "size_bytes": 3221225472,
          "context": "195K",
          "input": "Text"
        },
        {
          "tag": "yi:6b-200k-q3_K_L",
          "size_text": "3.2GB",
          "size_bytes": 3435973836,
          "context": "195K",
          "input": "Text"
        },
        {
          "tag": "yi:6b-200k-q4_0",
          "size_text": "3.5GB",
          "size_bytes": 3758096384,
          "context": "195K",
          "input": "Text"
        },
        {
          "tag": "yi:6b-200k-q4_1",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "195K",
          "input": "Text"
        },
        {
          "tag": "yi:6b-200k-q4_K_S",
          "size_text": "3.5GB",
          "size_bytes": 3758096384,
          "context": "195K",
          "input": "Text"
        },
        {
          "tag": "yi:6b-200k-q4_K_M",
          "size_text": "3.7GB",
          "size_bytes": 3972844748,
          "context": "195K",
          "input": "Text"
        },
        {
          "tag": "yi:6b-200k-q5_0",
          "size_text": "4.2GB",
          "size_bytes": 4509715660,
          "context": "195K",
          "input": "Text"
        },
        {
          "tag": "yi:6b-200k-q5_1",
          "size_text": "4.6GB",
          "size_bytes": 4939212390,
          "context": "195K",
          "input": "Text"
        },
        {
          "tag": "yi:6b-200k-q5_K_S",
          "size_text": "4.2GB",
          "size_bytes": 4509715660,
          "context": "195K",
          "input": "Text"
        },
        {
          "tag": "yi:6b-200k-q5_K_M",
          "size_text": "4.3GB",
          "size_bytes": 4617089843,
          "context": "195K",
          "input": "Text"
        },
        {
          "tag": "yi:6b-200k-q6_K",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "195K",
          "input": "Text"
        },
        {
          "tag": "yi:6b-200k-q8_0",
          "size_text": "6.4GB",
          "size_bytes": 6871947673,
          "context": "195K",
          "input": "Text"
        },
        {
          "tag": "yi:6b-200k-fp16",
          "size_text": "12GB",
          "size_bytes": 12884901888,
          "context": "195K",
          "input": "Text"
        },
        {
          "tag": "yi:6b-chat",
          "size_text": "3.5GB",
          "size_bytes": 3758096384,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:6b-chat-q2_K",
          "size_text": "2.6GB",
          "size_bytes": 2791728742,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:6b-chat-v1.5-q2_K",
          "size_text": "2.3GB",
          "size_bytes": 2469606195,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:6b-chat-q3_K_S",
          "size_text": "2.7GB",
          "size_bytes": 2899102924,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:6b-chat-v1.5-q3_K_S",
          "size_text": "2.7GB",
          "size_bytes": 2899102924,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:6b-chat-q3_K_M",
          "size_text": "3.0GB",
          "size_bytes": 3221225472,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:6b-chat-v1.5-q3_K_M",
          "size_text": "3.0GB",
          "size_bytes": 3221225472,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:6b-chat-q3_K_L",
          "size_text": "3.2GB",
          "size_bytes": 3435973836,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:6b-chat-v1.5-q3_K_L",
          "size_text": "3.2GB",
          "size_bytes": 3435973836,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:6b-chat-q4_0",
          "size_text": "3.5GB",
          "size_bytes": 3758096384,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:6b-chat-v1.5-q4_0",
          "size_text": "3.5GB",
          "size_bytes": 3758096384,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:6b-chat-q4_1",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:6b-chat-v1.5-q4_1",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:6b-chat-q4_K_S",
          "size_text": "3.5GB",
          "size_bytes": 3758096384,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:6b-chat-v1.5-q4_K_S",
          "size_text": "3.5GB",
          "size_bytes": 3758096384,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:6b-chat-q4_K_M",
          "size_text": "3.7GB",
          "size_bytes": 3972844748,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:6b-chat-v1.5-q4_K_M",
          "size_text": "3.7GB",
          "size_bytes": 3972844748,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:6b-chat-q5_0",
          "size_text": "4.2GB",
          "size_bytes": 4509715660,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:6b-chat-v1.5-q5_0",
          "size_text": "4.2GB",
          "size_bytes": 4509715660,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:6b-chat-q5_1",
          "size_text": "4.6GB",
          "size_bytes": 4939212390,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:6b-chat-v1.5-q5_1",
          "size_text": "4.6GB",
          "size_bytes": 4939212390,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:6b-chat-q5_K_S",
          "size_text": "4.2GB",
          "size_bytes": 4509715660,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:6b-chat-v1.5-q5_K_S",
          "size_text": "4.2GB",
          "size_bytes": 4509715660,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:6b-chat-q5_K_M",
          "size_text": "4.3GB",
          "size_bytes": 4617089843,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:6b-chat-v1.5-q5_K_M",
          "size_text": "4.3GB",
          "size_bytes": 4617089843,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:6b-chat-q6_K",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:6b-chat-v1.5-q6_K",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:6b-chat-q8_0",
          "size_text": "6.4GB",
          "size_bytes": 6871947673,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:6b-chat-v1.5-q8_0",
          "size_text": "6.4GB",
          "size_bytes": 6871947673,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:6b-chat-fp16",
          "size_text": "12GB",
          "size_bytes": 12884901888,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:6b-chat-v1.5-fp16",
          "size_text": "12GB",
          "size_bytes": 12884901888,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:6b-v1.5",
          "size_text": "3.5GB",
          "size_bytes": 3758096384,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:6b-v1.5-q2_K",
          "size_text": "2.3GB",
          "size_bytes": 2469606195,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:6b-v1.5-q3_K_S",
          "size_text": "2.7GB",
          "size_bytes": 2899102924,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:6b-v1.5-q3_K_M",
          "size_text": "3.0GB",
          "size_bytes": 3221225472,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:6b-v1.5-q3_K_L",
          "size_text": "3.2GB",
          "size_bytes": 3435973836,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:6b-v1.5-q4_0",
          "size_text": "3.5GB",
          "size_bytes": 3758096384,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:6b-v1.5-q4_1",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:6b-v1.5-q4_K_S",
          "size_text": "3.5GB",
          "size_bytes": 3758096384,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:6b-v1.5-q4_K_M",
          "size_text": "3.7GB",
          "size_bytes": 3972844748,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:6b-v1.5-q5_0",
          "size_text": "4.2GB",
          "size_bytes": 4509715660,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:6b-v1.5-q5_1",
          "size_text": "4.6GB",
          "size_bytes": 4939212390,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:6b-v1.5-q5_K_S",
          "size_text": "4.2GB",
          "size_bytes": 4509715660,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:6b-v1.5-q5_K_M",
          "size_text": "4.3GB",
          "size_bytes": 4617089843,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:6b-v1.5-q6_K",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:6b-v1.5-q8_0",
          "size_text": "6.4GB",
          "size_bytes": 6871947673,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:6b-v1.5-fp16",
          "size_text": "12GB",
          "size_bytes": 12884901888,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:6b-q2_K",
          "size_text": "2.6GB",
          "size_bytes": 2791728742,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:6b-q3_K_S",
          "size_text": "2.7GB",
          "size_bytes": 2899102924,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:6b-q3_K_M",
          "size_text": "3.0GB",
          "size_bytes": 3221225472,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:6b-q3_K_L",
          "size_text": "3.2GB",
          "size_bytes": 3435973836,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:6b-q4_0",
          "size_text": "3.5GB",
          "size_bytes": 3758096384,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:6b-q4_1",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:6b-q4_K_S",
          "size_text": "3.5GB",
          "size_bytes": 3758096384,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:6b-q4_K_M",
          "size_text": "3.7GB",
          "size_bytes": 3972844748,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:6b-q5_0",
          "size_text": "4.2GB",
          "size_bytes": 4509715660,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:6b-q5_1",
          "size_text": "4.6GB",
          "size_bytes": 4939212390,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:6b-q5_K_S",
          "size_text": "4.2GB",
          "size_bytes": 4509715660,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:6b-q5_K_M",
          "size_text": "4.3GB",
          "size_bytes": 4617089843,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:6b-q6_K",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:6b-q8_0",
          "size_text": "6.4GB",
          "size_bytes": 6871947673,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:6b-fp16",
          "size_text": "12GB",
          "size_bytes": 12884901888,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:9b-chat",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:9b-chat-v1.5-q2_K",
          "size_text": "3.4GB",
          "size_bytes": 3650722201,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:9b-chat-v1.5-q3_K_S",
          "size_text": "3.9GB",
          "size_bytes": 4187593113,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:9b-chat-v1.5-q3_K_M",
          "size_text": "4.3GB",
          "size_bytes": 4617089843,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:9b-chat-v1.5-q3_K_L",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:9b-chat-v1.5-q4_0",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:9b-chat-v1.5-q4_1",
          "size_text": "5.6GB",
          "size_bytes": 6012954214,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:9b-chat-v1.5-q4_K_S",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:9b-chat-v1.5-q4_K_M",
          "size_text": "5.3GB",
          "size_bytes": 5690831667,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:9b-chat-v1.5-q5_0",
          "size_text": "6.1GB",
          "size_bytes": 6549825126,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:9b-chat-v1.5-q5_1",
          "size_text": "6.6GB",
          "size_bytes": 7086696038,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:9b-chat-v1.5-q5_K_S",
          "size_text": "6.1GB",
          "size_bytes": 6549825126,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:9b-chat-v1.5-q5_K_M",
          "size_text": "6.3GB",
          "size_bytes": 6764573491,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:9b-chat-v1.5-q6_K",
          "size_text": "7.2GB",
          "size_bytes": 7730941132,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:9b-chat-v1.5-q8_0",
          "size_text": "9.4GB",
          "size_bytes": 10093173145,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:9b-chat-v1.5-fp16",
          "size_text": "18GB",
          "size_bytes": 19327352832,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:9b-v1.5",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:9b-v1.5-q2_K",
          "size_text": "3.4GB",
          "size_bytes": 3650722201,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:9b-v1.5-q3_K_S",
          "size_text": "3.9GB",
          "size_bytes": 4187593113,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:9b-v1.5-q3_K_M",
          "size_text": "4.3GB",
          "size_bytes": 4617089843,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:9b-v1.5-q3_K_L",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:9b-v1.5-q4_0",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:9b-v1.5-q4_1",
          "size_text": "5.6GB",
          "size_bytes": 6012954214,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:9b-v1.5-q4_K_S",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:9b-v1.5-q4_K_M",
          "size_text": "5.3GB",
          "size_bytes": 5690831667,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:9b-v1.5-q5_0",
          "size_text": "6.1GB",
          "size_bytes": 6549825126,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:9b-v1.5-q5_1",
          "size_text": "6.6GB",
          "size_bytes": 7086696038,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:9b-v1.5-q5_K_S",
          "size_text": "6.1GB",
          "size_bytes": 6549825126,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:9b-v1.5-q5_K_M",
          "size_text": "6.3GB",
          "size_bytes": 6764573491,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:9b-v1.5-q6_K",
          "size_text": "7.2GB",
          "size_bytes": 7730941132,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:9b-v1.5-q8_0",
          "size_text": "9.4GB",
          "size_bytes": 10093173145,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:9b-v1.5-fp16",
          "size_text": "18GB",
          "size_bytes": 19327352832,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:34b-chat",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:34b-chat-q2_K",
          "size_text": "15GB",
          "size_bytes": 16106127360,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:34b-chat-v1.5-q2_K",
          "size_text": "13GB",
          "size_bytes": 13958643712,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:34b-chat-q3_K_S",
          "size_text": "15GB",
          "size_bytes": 16106127360,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:34b-chat-v1.5-q3_K_S",
          "size_text": "15GB",
          "size_bytes": 16106127360,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:34b-chat-q3_K_M",
          "size_text": "17GB",
          "size_bytes": 18253611008,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:34b-chat-v1.5-q3_K_M",
          "size_text": "17GB",
          "size_bytes": 18253611008,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:34b-chat-q3_K_L",
          "size_text": "18GB",
          "size_bytes": 19327352832,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:34b-chat-v1.5-q3_K_L",
          "size_text": "18GB",
          "size_bytes": 19327352832,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:34b-chat-q4_0",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:34b-chat-v1.5-q4_0",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:34b-chat-q4_1",
          "size_text": "22GB",
          "size_bytes": 23622320128,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:34b-chat-v1.5-q4_1",
          "size_text": "22GB",
          "size_bytes": 23622320128,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:34b-chat-q4_K_S",
          "size_text": "20GB",
          "size_bytes": 21474836480,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:34b-chat-v1.5-q4_K_S",
          "size_text": "20GB",
          "size_bytes": 21474836480,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:34b-chat-q4_K_M",
          "size_text": "21GB",
          "size_bytes": 22548578304,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:34b-chat-v1.5-q4_K_M",
          "size_text": "21GB",
          "size_bytes": 22548578304,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:34b-chat-q5_0",
          "size_text": "24GB",
          "size_bytes": 25769803776,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:34b-chat-v1.5-q5_0",
          "size_text": "24GB",
          "size_bytes": 25769803776,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:34b-chat-q5_1",
          "size_text": "26GB",
          "size_bytes": 27917287424,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:34b-chat-v1.5-q5_1",
          "size_text": "26GB",
          "size_bytes": 27917287424,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:34b-chat-q5_K_S",
          "size_text": "24GB",
          "size_bytes": 25769803776,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:34b-chat-v1.5-q5_K_S",
          "size_text": "24GB",
          "size_bytes": 25769803776,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:34b-chat-q5_K_M",
          "size_text": "24GB",
          "size_bytes": 25769803776,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:34b-chat-v1.5-q5_K_M",
          "size_text": "24GB",
          "size_bytes": 25769803776,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:34b-chat-q6_K",
          "size_text": "28GB",
          "size_bytes": 30064771072,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:34b-chat-v1.5-q6_K",
          "size_text": "28GB",
          "size_bytes": 30064771072,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:34b-chat-q8_0",
          "size_text": "37GB",
          "size_bytes": 39728447488,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:34b-chat-v1.5-q8_0",
          "size_text": "37GB",
          "size_bytes": 39728447488,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:34b-chat-fp16",
          "size_text": "69GB",
          "size_bytes": 74088185856,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:34b-chat-v1.5-fp16",
          "size_text": "69GB",
          "size_bytes": 74088185856,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:34b-v1.5",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:34b-v1.5-q2_K",
          "size_text": "13GB",
          "size_bytes": 13958643712,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:34b-v1.5-q3_K_S",
          "size_text": "15GB",
          "size_bytes": 16106127360,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:34b-v1.5-q3_K_M",
          "size_text": "17GB",
          "size_bytes": 18253611008,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:34b-v1.5-q3_K_L",
          "size_text": "18GB",
          "size_bytes": 19327352832,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:34b-v1.5-q4_0",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:34b-v1.5-q4_1",
          "size_text": "22GB",
          "size_bytes": 23622320128,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:34b-v1.5-q4_K_S",
          "size_text": "20GB",
          "size_bytes": 21474836480,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:34b-v1.5-q4_K_M",
          "size_text": "21GB",
          "size_bytes": 22548578304,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:34b-v1.5-q5_0",
          "size_text": "24GB",
          "size_bytes": 25769803776,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:34b-v1.5-q5_1",
          "size_text": "26GB",
          "size_bytes": 27917287424,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:34b-v1.5-q5_K_S",
          "size_text": "24GB",
          "size_bytes": 25769803776,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:34b-v1.5-q5_K_M",
          "size_text": "24GB",
          "size_bytes": 25769803776,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:34b-v1.5-q6_K",
          "size_text": "28GB",
          "size_bytes": 30064771072,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:34b-v1.5-q8_0",
          "size_text": "37GB",
          "size_bytes": 39728447488,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:34b-v1.5-fp16",
          "size_text": "69GB",
          "size_bytes": 74088185856,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:34b-q2_K",
          "size_text": "15GB",
          "size_bytes": 16106127360,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:34b-q3_K_S",
          "size_text": "15GB",
          "size_bytes": 16106127360,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:34b-q3_K_M",
          "size_text": "17GB",
          "size_bytes": 18253611008,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:34b-q3_K_L",
          "size_text": "18GB",
          "size_bytes": 19327352832,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:34b-q4_0",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:34b-q4_1",
          "size_text": "22GB",
          "size_bytes": 23622320128,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:34b-q4_K_S",
          "size_text": "20GB",
          "size_bytes": 21474836480,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:34b-q4_K_M",
          "size_text": "21GB",
          "size_bytes": 22548578304,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:34b-q5_0",
          "size_text": "24GB",
          "size_bytes": 25769803776,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:34b-q5_1",
          "size_text": "26GB",
          "size_bytes": 27917287424,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:34b-q5_K_S",
          "size_text": "24GB",
          "size_bytes": 25769803776,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "yi:34b-q6_K",
          "size_text": "28GB",
          "size_bytes": 30064771072,
          "context": "4K",
          "input": "Text"
        }
      ],
      "tags_count": 174
    },
    {
      "slug": "granite-code",
      "capabilities": [],
      "pulls": 299000,
      "pulls_text": "299K \n  Downloads",
      "blurb": "granite-code A family of open foundation models by IBM for Code Intelligence 3b 8b 20b 34b 162 Tags Updated 1 year ago",
      "name": "A family of open foundation models by IBM for Code Intelligence",
      "description": "Readme Granite Code is a family of decoder-only code model designed for code generative tasks (e.g. code generation, code explanation, code fixing, etc.). Parameter Sizes 34B Parameters ollama run granite-code:34b 20B Parameters ollama run granite-code:20b 8B Parameters (with 128K context window) ollama run granite-code:8b 3B Parameters (with 128K context window) ollama run granite-code:3b Resources Hugging Face Paper Write Preview ![image.png](https://ollama.com/assets/library/granite-code/e28e37cc-af1b-47d7-bd85-6154b0cf26be) Granite Code is a family of decoder-only code model designed for code generative tasks (e.g. code generation, code explanation, code fixing, etc.). ## Parameter Sizes * 34B Parameters `ollama run granite-code:34b` * 20B Parameters `ollama run granite-code:20b` * 8B Parameters (with 128K context window) `ollama run granite-code:8b` * 3B Parameters (with 128K context window) `ollama run granite-code:3b` ## Resources * [Hugging Face](https://huggingface.co/collections/ibm-granite/granite-code-models-6624c5cec322e4c148c8b330) * [Paper](https://arxiv.org/abs/2405.04324) Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "granite-code:latest",
          "size_text": "2.0GB",
          "size_bytes": 2147483648,
          "context": "125K",
          "input": "Text"
        },
        {
          "tag": "granite-code:3b",
          "size_text": "2.0GB",
          "size_bytes": 2147483648,
          "context": "125K",
          "input": "Text"
        },
        {
          "tag": "granite-code:8b",
          "size_text": "4.6GB",
          "size_bytes": 4939212390,
          "context": "125K",
          "input": "Text"
        },
        {
          "tag": "granite-code:20b",
          "size_text": "12GB",
          "size_bytes": 12884901888,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "granite-code:34b",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "granite-code:3b-base",
          "size_text": "2.0GB",
          "size_bytes": 2147483648,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "granite-code:3b-base-q2_K",
          "size_text": "1.3GB",
          "size_bytes": 1395864371,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "granite-code:3b-base-q3_K_S",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "granite-code:3b-base-q3_K_M",
          "size_text": "1.7GB",
          "size_bytes": 1825361100,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "granite-code:3b-base-q3_K_L",
          "size_text": "1.9GB",
          "size_bytes": 2040109465,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "granite-code:3b-base-q4_0",
          "size_text": "2.0GB",
          "size_bytes": 2147483648,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "granite-code:3b-base-q4_1",
          "size_text": "2.2GB",
          "size_bytes": 2362232012,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "granite-code:3b-base-q4_K_S",
          "size_text": "2.0GB",
          "size_bytes": 2147483648,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "granite-code:3b-base-q4_K_M",
          "size_text": "2.1GB",
          "size_bytes": 2254857830,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "granite-code:3b-base-q5_0",
          "size_text": "2.4GB",
          "size_bytes": 2576980377,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "granite-code:3b-base-q5_1",
          "size_text": "2.6GB",
          "size_bytes": 2791728742,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "granite-code:3b-base-q5_K_S",
          "size_text": "2.4GB",
          "size_bytes": 2576980377,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "granite-code:3b-base-q5_K_M",
          "size_text": "2.5GB",
          "size_bytes": 2684354560,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "granite-code:3b-base-q6_K",
          "size_text": "2.9GB",
          "size_bytes": 3113851289,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "granite-code:3b-base-q8_0",
          "size_text": "3.7GB",
          "size_bytes": 3972844748,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "granite-code:3b-base-fp16",
          "size_text": "7.0GB",
          "size_bytes": 7516192768,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "granite-code:3b-instruct",
          "size_text": "2.0GB",
          "size_bytes": 2147483648,
          "context": "125K",
          "input": "Text"
        },
        {
          "tag": "granite-code:3b-instruct-128k-q2_K",
          "size_text": "1.3GB",
          "size_bytes": 1395864371,
          "context": "125K",
          "input": "Text"
        },
        {
          "tag": "granite-code:3b-instruct-q2_K",
          "size_text": "1.3GB",
          "size_bytes": 1395864371,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "granite-code:3b-instruct-128k-q3_K_S",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": "125K",
          "input": "Text"
        },
        {
          "tag": "granite-code:3b-instruct-q3_K_S",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "granite-code:3b-instruct-128k-q3_K_M",
          "size_text": "1.7GB",
          "size_bytes": 1825361100,
          "context": "125K",
          "input": "Text"
        },
        {
          "tag": "granite-code:3b-instruct-q3_K_M",
          "size_text": "1.7GB",
          "size_bytes": 1825361100,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "granite-code:3b-instruct-128k-q3_K_L",
          "size_text": "1.9GB",
          "size_bytes": 2040109465,
          "context": "125K",
          "input": "Text"
        },
        {
          "tag": "granite-code:3b-instruct-q3_K_L",
          "size_text": "1.9GB",
          "size_bytes": 2040109465,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "granite-code:3b-instruct-128k-q4_0",
          "size_text": "2.0GB",
          "size_bytes": 2147483648,
          "context": "125K",
          "input": "Text"
        },
        {
          "tag": "granite-code:3b-instruct-q4_0",
          "size_text": "2.0GB",
          "size_bytes": 2147483648,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "granite-code:3b-instruct-128k-q4_1",
          "size_text": "2.2GB",
          "size_bytes": 2362232012,
          "context": "125K",
          "input": "Text"
        },
        {
          "tag": "granite-code:3b-instruct-q4_1",
          "size_text": "2.2GB",
          "size_bytes": 2362232012,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "granite-code:3b-instruct-128k-q4_K_S",
          "size_text": "2.0GB",
          "size_bytes": 2147483648,
          "context": "125K",
          "input": "Text"
        },
        {
          "tag": "granite-code:3b-instruct-q4_K_S",
          "size_text": "2.0GB",
          "size_bytes": 2147483648,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "granite-code:3b-instruct-128k-q4_K_M",
          "size_text": "2.1GB",
          "size_bytes": 2254857830,
          "context": "125K",
          "input": "Text"
        },
        {
          "tag": "granite-code:3b-instruct-q4_K_M",
          "size_text": "2.1GB",
          "size_bytes": 2254857830,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "granite-code:3b-instruct-128k-q5_0",
          "size_text": "2.4GB",
          "size_bytes": 2576980377,
          "context": "125K",
          "input": "Text"
        },
        {
          "tag": "granite-code:3b-instruct-q5_0",
          "size_text": "2.4GB",
          "size_bytes": 2576980377,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "granite-code:3b-instruct-128k-q5_1",
          "size_text": "2.6GB",
          "size_bytes": 2791728742,
          "context": "125K",
          "input": "Text"
        },
        {
          "tag": "granite-code:3b-instruct-q5_1",
          "size_text": "2.6GB",
          "size_bytes": 2791728742,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "granite-code:3b-instruct-128k-q5_K_S",
          "size_text": "2.4GB",
          "size_bytes": 2576980377,
          "context": "125K",
          "input": "Text"
        },
        {
          "tag": "granite-code:3b-instruct-q5_K_S",
          "size_text": "2.4GB",
          "size_bytes": 2576980377,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "granite-code:3b-instruct-128k-q5_K_M",
          "size_text": "2.5GB",
          "size_bytes": 2684354560,
          "context": "125K",
          "input": "Text"
        },
        {
          "tag": "granite-code:3b-instruct-q5_K_M",
          "size_text": "2.5GB",
          "size_bytes": 2684354560,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "granite-code:3b-instruct-128k-q6_K",
          "size_text": "2.9GB",
          "size_bytes": 3113851289,
          "context": "125K",
          "input": "Text"
        },
        {
          "tag": "granite-code:3b-instruct-q6_K",
          "size_text": "2.9GB",
          "size_bytes": 3113851289,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "granite-code:3b-instruct-128k-q8_0",
          "size_text": "3.7GB",
          "size_bytes": 3972844748,
          "context": "125K",
          "input": "Text"
        },
        {
          "tag": "granite-code:3b-instruct-q8_0",
          "size_text": "3.7GB",
          "size_bytes": 3972844748,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "granite-code:3b-instruct-128k-fp16",
          "size_text": "7.0GB",
          "size_bytes": 7516192768,
          "context": "125K",
          "input": "Text"
        },
        {
          "tag": "granite-code:3b-instruct-fp16",
          "size_text": "7.0GB",
          "size_bytes": 7516192768,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "granite-code:8b-base",
          "size_text": "4.6GB",
          "size_bytes": 4939212390,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite-code:8b-base-q2_K",
          "size_text": "3.1GB",
          "size_bytes": 3328599654,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite-code:8b-base-q3_K_S",
          "size_text": "3.5GB",
          "size_bytes": 3758096384,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite-code:8b-base-q3_K_M",
          "size_text": "3.9GB",
          "size_bytes": 4187593113,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite-code:8b-base-q3_K_L",
          "size_text": "4.3GB",
          "size_bytes": 4617089843,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite-code:8b-base-q4_0",
          "size_text": "4.6GB",
          "size_bytes": 4939212390,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite-code:8b-base-q4_1",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite-code:8b-base-q4_K_S",
          "size_text": "4.6GB",
          "size_bytes": 4939212390,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite-code:8b-base-q4_K_M",
          "size_text": "4.9GB",
          "size_bytes": 5261334937,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite-code:8b-base-q5_0",
          "size_text": "5.6GB",
          "size_bytes": 6012954214,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite-code:8b-base-q5_1",
          "size_text": "6.1GB",
          "size_bytes": 6549825126,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite-code:8b-base-q5_K_S",
          "size_text": "5.6GB",
          "size_bytes": 6012954214,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite-code:8b-base-q5_K_M",
          "size_text": "5.7GB",
          "size_bytes": 6120328396,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite-code:8b-base-q6_K",
          "size_text": "6.6GB",
          "size_bytes": 7086696038,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite-code:8b-base-q8_0",
          "size_text": "8.6GB",
          "size_bytes": 9234179686,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite-code:8b-base-fp16",
          "size_text": "16GB",
          "size_bytes": 17179869184,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite-code:8b-instruct",
          "size_text": "4.6GB",
          "size_bytes": 4939212390,
          "context": "125K",
          "input": "Text"
        },
        {
          "tag": "granite-code:8b-instruct-q2_K",
          "size_text": "3.1GB",
          "size_bytes": 3328599654,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite-code:8b-instruct-q3_K_S",
          "size_text": "3.5GB",
          "size_bytes": 3758096384,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite-code:8b-instruct-q3_K_M",
          "size_text": "3.9GB",
          "size_bytes": 4187593113,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite-code:8b-instruct-q3_K_L",
          "size_text": "4.3GB",
          "size_bytes": 4617089843,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite-code:8b-instruct-128k-q4_0",
          "size_text": "4.6GB",
          "size_bytes": 4939212390,
          "context": "125K",
          "input": "Text"
        },
        {
          "tag": "granite-code:8b-instruct-q4_0",
          "size_text": "4.6GB",
          "size_bytes": 4939212390,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite-code:8b-instruct-128k-q4_1",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "125K",
          "input": "Text"
        },
        {
          "tag": "granite-code:8b-instruct-q4_1",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite-code:8b-instruct-q4_K_S",
          "size_text": "4.6GB",
          "size_bytes": 4939212390,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite-code:8b-instruct-q4_K_M",
          "size_text": "4.9GB",
          "size_bytes": 5261334937,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite-code:8b-instruct-q5_0",
          "size_text": "5.6GB",
          "size_bytes": 6012954214,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite-code:8b-instruct-q5_1",
          "size_text": "6.1GB",
          "size_bytes": 6549825126,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite-code:8b-instruct-q5_K_S",
          "size_text": "5.6GB",
          "size_bytes": 6012954214,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite-code:8b-instruct-q5_K_M",
          "size_text": "5.7GB",
          "size_bytes": 6120328396,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite-code:8b-instruct-q6_K",
          "size_text": "6.6GB",
          "size_bytes": 7086696038,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite-code:8b-instruct-q8_0",
          "size_text": "8.6GB",
          "size_bytes": 9234179686,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite-code:8b-instruct-fp16",
          "size_text": "16GB",
          "size_bytes": 17179869184,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite-code:20b-base",
          "size_text": "12GB",
          "size_bytes": 12884901888,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "granite-code:20b-base-q2_K",
          "size_text": "7.9GB",
          "size_bytes": 8482560409,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "granite-code:20b-base-q3_K_S",
          "size_text": "8.9GB",
          "size_bytes": 9556302233,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "granite-code:20b-base-q3_K_M",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "granite-code:20b-base-q3_K_L",
          "size_text": "12GB",
          "size_bytes": 12884901888,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "granite-code:20b-base-q4_0",
          "size_text": "12GB",
          "size_bytes": 12884901888,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "granite-code:20b-base-q4_1",
          "size_text": "13GB",
          "size_bytes": 13958643712,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "granite-code:20b-base-q4_K_S",
          "size_text": "12GB",
          "size_bytes": 12884901888,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "granite-code:20b-base-q4_K_M",
          "size_text": "13GB",
          "size_bytes": 13958643712,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "granite-code:20b-base-q5_0",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "granite-code:20b-base-q5_1",
          "size_text": "15GB",
          "size_bytes": 16106127360,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "granite-code:20b-base-q5_K_S",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "granite-code:20b-base-q5_K_M",
          "size_text": "15GB",
          "size_bytes": 16106127360,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "granite-code:20b-base-q6_K",
          "size_text": "17GB",
          "size_bytes": 18253611008,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "granite-code:20b-base-q8_0",
          "size_text": "21GB",
          "size_bytes": 22548578304,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "granite-code:20b-base-fp16",
          "size_text": "40GB",
          "size_bytes": 42949672960,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "granite-code:20b-instruct",
          "size_text": "12GB",
          "size_bytes": 12884901888,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "granite-code:20b-instruct-8k-q2_K",
          "size_text": "7.9GB",
          "size_bytes": 8482560409,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "granite-code:20b-instruct-q2_K",
          "size_text": "7.9GB",
          "size_bytes": 8482560409,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "granite-code:20b-instruct-8k-q3_K_S",
          "size_text": "8.9GB",
          "size_bytes": 9556302233,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "granite-code:20b-instruct-q3_K_S",
          "size_text": "8.9GB",
          "size_bytes": 9556302233,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "granite-code:20b-instruct-8k-q3_K_M",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "granite-code:20b-instruct-q3_K_M",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "granite-code:20b-instruct-8k-q3_K_L",
          "size_text": "12GB",
          "size_bytes": 12884901888,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "granite-code:20b-instruct-q3_K_L",
          "size_text": "12GB",
          "size_bytes": 12884901888,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "granite-code:20b-instruct-8k-q4_0",
          "size_text": "12GB",
          "size_bytes": 12884901888,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "granite-code:20b-instruct-q4_0",
          "size_text": "12GB",
          "size_bytes": 12884901888,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "granite-code:20b-instruct-8k-q4_1",
          "size_text": "13GB",
          "size_bytes": 13958643712,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "granite-code:20b-instruct-q4_1",
          "size_text": "13GB",
          "size_bytes": 13958643712,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "granite-code:20b-instruct-8k-q4_K_S",
          "size_text": "12GB",
          "size_bytes": 12884901888,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "granite-code:20b-instruct-q4_K_S",
          "size_text": "12GB",
          "size_bytes": 12884901888,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "granite-code:20b-instruct-8k-q4_K_M",
          "size_text": "13GB",
          "size_bytes": 13958643712,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "granite-code:20b-instruct-q4_K_M",
          "size_text": "13GB",
          "size_bytes": 13958643712,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "granite-code:20b-instruct-8k-q5_0",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "granite-code:20b-instruct-q5_0",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "granite-code:20b-instruct-8k-q5_1",
          "size_text": "15GB",
          "size_bytes": 16106127360,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "granite-code:20b-instruct-q5_1",
          "size_text": "15GB",
          "size_bytes": 16106127360,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "granite-code:20b-instruct-8k-q5_K_S",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "granite-code:20b-instruct-q5_K_S",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "granite-code:20b-instruct-8k-q5_K_M",
          "size_text": "15GB",
          "size_bytes": 16106127360,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "granite-code:20b-instruct-q5_K_M",
          "size_text": "15GB",
          "size_bytes": 16106127360,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "granite-code:20b-instruct-8k-q6_K",
          "size_text": "17GB",
          "size_bytes": 18253611008,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "granite-code:20b-instruct-q6_K",
          "size_text": "17GB",
          "size_bytes": 18253611008,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "granite-code:20b-instruct-8k-q8_0",
          "size_text": "21GB",
          "size_bytes": 22548578304,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "granite-code:20b-instruct-q8_0",
          "size_text": "21GB",
          "size_bytes": 22548578304,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "granite-code:20b-instruct-8k-fp16",
          "size_text": "40GB",
          "size_bytes": 42949672960,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "granite-code:34b-base",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "granite-code:34b-base-q2_K",
          "size_text": "13GB",
          "size_bytes": 13958643712,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "granite-code:34b-base-q3_K_S",
          "size_text": "15GB",
          "size_bytes": 16106127360,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "granite-code:34b-base-q3_K_M",
          "size_text": "18GB",
          "size_bytes": 19327352832,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "granite-code:34b-base-q3_K_L",
          "size_text": "20GB",
          "size_bytes": 21474836480,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "granite-code:34b-base-q4_0",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "granite-code:34b-base-q4_1",
          "size_text": "21GB",
          "size_bytes": 22548578304,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "granite-code:34b-base-q4_K_S",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "granite-code:34b-base-q4_K_M",
          "size_text": "21GB",
          "size_bytes": 22548578304,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "granite-code:34b-base-q5_0",
          "size_text": "23GB",
          "size_bytes": 24696061952,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "granite-code:34b-base-q5_1",
          "size_text": "25GB",
          "size_bytes": 26843545600,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "granite-code:34b-base-q5_K_S",
          "size_text": "23GB",
          "size_bytes": 24696061952,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "granite-code:34b-base-q5_K_M",
          "size_text": "25GB",
          "size_bytes": 26843545600,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "granite-code:34b-base-q6_K",
          "size_text": "28GB",
          "size_bytes": 30064771072,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "granite-code:34b-base-q8_0",
          "size_text": "36GB",
          "size_bytes": 38654705664,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "granite-code:34b-instruct",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "granite-code:34b-instruct-q2_K",
          "size_text": "13GB",
          "size_bytes": 13958643712,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "granite-code:34b-instruct-q3_K_S",
          "size_text": "15GB",
          "size_bytes": 16106127360,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "granite-code:34b-instruct-q3_K_M",
          "size_text": "18GB",
          "size_bytes": 19327352832,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "granite-code:34b-instruct-q3_K_L",
          "size_text": "20GB",
          "size_bytes": 21474836480,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "granite-code:34b-instruct-q4_0",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "granite-code:34b-instruct-q4_1",
          "size_text": "21GB",
          "size_bytes": 22548578304,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "granite-code:34b-instruct-q4_K_S",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "granite-code:34b-instruct-q4_K_M",
          "size_text": "21GB",
          "size_bytes": 22548578304,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "granite-code:34b-instruct-q5_0",
          "size_text": "23GB",
          "size_bytes": 24696061952,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "granite-code:34b-instruct-q5_1",
          "size_text": "25GB",
          "size_bytes": 26843545600,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "granite-code:34b-instruct-q5_K_S",
          "size_text": "23GB",
          "size_bytes": 24696061952,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "granite-code:34b-instruct-q5_K_M",
          "size_text": "25GB",
          "size_bytes": 26843545600,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "granite-code:34b-instruct-q6_K",
          "size_text": "28GB",
          "size_bytes": 30064771072,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "granite-code:34b-instruct-q8_0",
          "size_text": "36GB",
          "size_bytes": 38654705664,
          "context": "8K",
          "input": "Text"
        }
      ],
      "tags_count": 162
    },
    {
      "slug": "zephyr",
      "capabilities": [],
      "pulls": 294100,
      "pulls_text": "294.1K \n  Downloads",
      "blurb": "zephyr Zephyr is a series of fine-tuned versions of the Mistral and Mixtral models that are trained to act as helpful assistants. 7b 141b 40 Tags Updated 1 year ago",
      "name": "Zephyr is a series of fine-tuned versions of the Mistral and Mixtral models that are trained to act as helpful assistants.",
      "description": "Readme Zephyr is a series of language models that are trained to act as helpful assistants. Zephyr 141B-A35B is the latest model in the series, and is a fine-tuned version of Mixtral 8x22b . Sizes zephyr:141b : A Mixture of Experts (MoE) model with 141B total parameters and 35B active parameters. zephyr:7b : The original Zephyr model Source: HuggingFace Write Preview <img src=\"https://github.com/jmorganca/ollama/assets/251292/8dab1157-909a-455b-a5c4-f7c31e9bcf0e\" width=\"400\" /> Zephyr is a series of language models that are trained to act as helpful assistants. Zephyr 141B-A35B is the latest model in the series, and is a fine-tuned version of [Mixtral 8x22b](https://ollama.com/library/mixtral:8x22b). ## Sizes * `zephyr:141b`: A Mixture of Experts (MoE) model with 141B total parameters and 35B active parameters. * `zephyr:7b`: The original Zephyr model ## Source: * [HuggingFace](https://huggingface.co/HuggingFaceH4) Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "zephyr:latest",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "zephyr:7b",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "zephyr:141b",
          "size_text": "80GB",
          "size_bytes": 85899345920,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "zephyr:7b-alpha",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "zephyr:7b-alpha-q2_K",
          "size_text": "3.1GB",
          "size_bytes": 3328599654,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "zephyr:7b-alpha-q3_K_S",
          "size_text": "3.2GB",
          "size_bytes": 3435973836,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "zephyr:7b-alpha-q3_K_M",
          "size_text": "3.5GB",
          "size_bytes": 3758096384,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "zephyr:7b-alpha-q3_K_L",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "zephyr:7b-alpha-q4_0",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "zephyr:7b-alpha-q4_1",
          "size_text": "4.6GB",
          "size_bytes": 4939212390,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "zephyr:7b-alpha-q4_K_S",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "zephyr:7b-alpha-q4_K_M",
          "size_text": "4.4GB",
          "size_bytes": 4724464025,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "zephyr:7b-alpha-q5_0",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "zephyr:7b-alpha-q5_1",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "zephyr:7b-alpha-q5_K_S",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "zephyr:7b-alpha-q5_K_M",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "zephyr:7b-alpha-q6_K",
          "size_text": "5.9GB",
          "size_bytes": 6335076761,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "zephyr:7b-alpha-q8_0",
          "size_text": "7.7GB",
          "size_bytes": 8267812044,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "zephyr:7b-alpha-fp16",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "zephyr:7b-beta",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "zephyr:7b-beta-q2_K",
          "size_text": "3.1GB",
          "size_bytes": 3328599654,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "zephyr:7b-beta-q3_K_S",
          "size_text": "3.2GB",
          "size_bytes": 3435973836,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "zephyr:7b-beta-q3_K_M",
          "size_text": "3.5GB",
          "size_bytes": 3758096384,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "zephyr:7b-beta-q3_K_L",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "zephyr:7b-beta-q4_0",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "zephyr:7b-beta-q4_1",
          "size_text": "4.6GB",
          "size_bytes": 4939212390,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "zephyr:7b-beta-q4_K_S",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "zephyr:7b-beta-q4_K_M",
          "size_text": "4.4GB",
          "size_bytes": 4724464025,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "zephyr:7b-beta-q5_0",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "zephyr:7b-beta-q5_1",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "zephyr:7b-beta-q5_K_S",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "zephyr:7b-beta-q5_K_M",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "zephyr:7b-beta-q6_K",
          "size_text": "5.9GB",
          "size_bytes": 6335076761,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "zephyr:7b-beta-q8_0",
          "size_text": "7.7GB",
          "size_bytes": 8267812044,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "zephyr:7b-beta-fp16",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "zephyr:141b-v0.1",
          "size_text": "80GB",
          "size_bytes": 85899345920,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "zephyr:141b-v0.1-q2_K",
          "size_text": "52GB",
          "size_bytes": 55834574848,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "zephyr:141b-v0.1-q4_0",
          "size_text": "80GB",
          "size_bytes": 85899345920,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "zephyr:141b-v0.1-q8_0",
          "size_text": "149GB",
          "size_bytes": 159987531776,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "zephyr:141b-v0.1-fp16",
          "size_text": "281GB",
          "size_bytes": 301721452544,
          "context": "64K",
          "input": "Text"
        }
      ],
      "tags_count": 40
    },
    {
      "slug": "mistral-large",
      "capabilities": [
        "reasoning",
        "tools"
      ],
      "pulls": 266500,
      "pulls_text": "266.5K \n  Downloads",
      "blurb": "mistral-large Mistral Large 2 is Mistral's new flagship model that is significantly more capable in code generation, mathematics, and with 128k context window and support for dozens of languages. 123b 32 Tags Updated 11 months ago",
      "name": "Mistral Large 2 is Mistral's new flagship model that is significantly more capable in code generation, mathematics, and reasoning with 128k context window and support for dozens of languages.",
      "description": "Readme Mistral-Large-Instruct-2411 is an advanced dense Large Language Model (LLM) of 123B parameters with state-of-the-art reasoning, knowledge and coding capabilities. Key features Multi-lingual by design: Dozens of languages supported, including English, French, German, Spanish, Italian, Chinese, Japanese, Korean, Portuguese, Dutch and Polish. Proficient in coding: Trained on 80+ coding languages such as Python, Java, C, C++, JavacScript, and Bash. Also trained on more specific languages such as Swift and Fortran. Agentic-centric: Best-in-class agentic capabilities with native function calling and JSON outputting. Advanced Reasoning: State-of-the-art mathematical and reasoning capabilities. Mistral Research License: Allows usage and modification for research and non-commercial usages. Large Context: A large 128k context window. References Blog post Hugging Face Write Preview <img src=\"https://ollama.com/assets/library/mistral-large/1b9ab956-d5ea-4927-a54a-3f969021d3db\" width=\"320\" /> Mistral-Large-Instruct-2411 is an advanced dense Large Language Model (LLM) of 123B parameters with state-of-the-art reasoning, knowledge and coding capabilities. ### Key features * Multi-lingual by design: Dozens of languages supported, including English, French, German, Spanish, Italian, Chinese, Japanese, Korean, Portuguese, Dutch and Polish. * Proficient in coding: Trained on 80+ coding languages such as Python, Java, C, C++, JavacScript, and Bash. Also trained on more specific languages such as Swift and Fortran. * Agentic-centric: Best-in-class agentic capabilities with native function calling and JSON outputting. * Advanced Reasoning: State-of-the-art mathematical and reasoning capabilities. * Mistral Research License: Allows usage and modification for research and non-commercial usages. * Large Context: A large 128k context window. ## References [Blog post](https://mistral.ai/news/mistral-large-2407/) [Hugging Face](https://huggingface.co/mistralai/Mistral-Large-Instruct-2411",
      "variants": [
        {
          "tag": "mistral-large:latest",
          "size_text": "73GB",
          "size_bytes": 78383153152,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "mistral-large:123b",
          "size_text": "73GB",
          "size_bytes": 78383153152,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "mistral-large:123b-instruct-2407-q2_K",
          "size_text": "45GB",
          "size_bytes": 48318382080,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral-large:123b-instruct-2407-q3_K_S",
          "size_text": "53GB",
          "size_bytes": 56908316672,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral-large:123b-instruct-2407-q3_K_M",
          "size_text": "59GB",
          "size_bytes": 63350767616,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral-large:123b-instruct-2407-q3_K_L",
          "size_text": "65GB",
          "size_bytes": 69793218560,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral-large:123b-instruct-2407-q4_0",
          "size_text": "69GB",
          "size_bytes": 74088185856,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral-large:123b-instruct-2407-q4_1",
          "size_text": "77GB",
          "size_bytes": 82678120448,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral-large:123b-instruct-2407-q4_K_S",
          "size_text": "70GB",
          "size_bytes": 75161927680,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral-large:123b-instruct-2407-q4_K_M",
          "size_text": "73GB",
          "size_bytes": 78383153152,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral-large:123b-instruct-2407-q5_0",
          "size_text": "84GB",
          "size_bytes": 90194313216,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral-large:123b-instruct-2407-q5_1",
          "size_text": "92GB",
          "size_bytes": 98784247808,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral-large:123b-instruct-2407-q5_K_S",
          "size_text": "84GB",
          "size_bytes": 90194313216,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral-large:123b-instruct-2407-q5_K_M",
          "size_text": "86GB",
          "size_bytes": 92341796864,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral-large:123b-instruct-2407-q6_K",
          "size_text": "101GB",
          "size_bytes": 108447924224,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral-large:123b-instruct-2407-q8_0",
          "size_text": "130GB",
          "size_bytes": 139586437120,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral-large:123b-instruct-2407-fp16",
          "size_text": "245GB",
          "size_bytes": 263066746880,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral-large:123b-instruct-2411-q2_K",
          "size_text": "45GB",
          "size_bytes": 48318382080,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "mistral-large:123b-instruct-2411-q3_K_S",
          "size_text": "53GB",
          "size_bytes": 56908316672,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "mistral-large:123b-instruct-2411-q3_K_M",
          "size_text": "59GB",
          "size_bytes": 63350767616,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "mistral-large:123b-instruct-2411-q3_K_L",
          "size_text": "65GB",
          "size_bytes": 69793218560,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "mistral-large:123b-instruct-2411-q4_0",
          "size_text": "69GB",
          "size_bytes": 74088185856,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "mistral-large:123b-instruct-2411-q4_1",
          "size_text": "77GB",
          "size_bytes": 82678120448,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "mistral-large:123b-instruct-2411-q4_K_S",
          "size_text": "70GB",
          "size_bytes": 75161927680,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "mistral-large:123b-instruct-2411-q4_K_M",
          "size_text": "73GB",
          "size_bytes": 78383153152,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "mistral-large:123b-instruct-2411-q5_0",
          "size_text": "84GB",
          "size_bytes": 90194313216,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "mistral-large:123b-instruct-2411-q5_1",
          "size_text": "92GB",
          "size_bytes": 98784247808,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "mistral-large:123b-instruct-2411-q5_K_S",
          "size_text": "84GB",
          "size_bytes": 90194313216,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "mistral-large:123b-instruct-2411-q5_K_M",
          "size_text": "86GB",
          "size_bytes": 92341796864,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "mistral-large:123b-instruct-2411-q6_K",
          "size_text": "101GB",
          "size_bytes": 108447924224,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "mistral-large:123b-instruct-2411-q8_0",
          "size_text": "130GB",
          "size_bytes": 139586437120,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "mistral-large:123b-instruct-2411-fp16",
          "size_text": "245GB",
          "size_bytes": 263066746880,
          "context": "128K",
          "input": "Text"
        }
      ],
      "tags_count": 32
    },
    {
      "slug": "wizard-vicuna-uncensored",
      "capabilities": [],
      "pulls": 255600,
      "pulls_text": "255.6K \n  Downloads",
      "blurb": "wizard-vicuna-uncensored Wizard Vicuna Uncensored is a 7B, 13B, and 30B parameter model based on Llama 2 uncensored by Eric Hartford. 7b 13b 30b 49 Tags Updated 2 years ago",
      "name": "Wizard Vicuna Uncensored is a 7B, 13B, and 30B parameter model based on Llama 2 uncensored by Eric Hartford.",
      "description": "Readme Wizard Vicuna Uncensored is a 7B, 13B, and 30B parameter model based on Llama 2 uncensored by Eric Hartford. The models were trained against LLaMA-7B with a subset of the dataset, responses that contained alignment / moralizing were removed. Get started with Wizard Vicuna Uncensored The model used in the example below is the Wizard Vicuna Uncensored model, with 7b parameters, which is a general-use model. API Start Ollama server (Run ollama serve ) Run the model curl -X POST http://localhost:11434/api/generate -d '{ \"model\": \"wizard-vicuna-uncensored\", \"prompt\":\"Who made Rose promise that she would never let go?\" }' CLI Install Ollama Open the terminal and run ollama run wizard-vicuna-uncensored Note: The ollama run command performs an ollama pull if the model is not already downloaded. To download the model without running it, use ollama pull wizard-vicuna-uncensored Memory requirements 7b models generally require at least 8GB of RAM 13b models generally require at least 16GB of RAM 30b models generally require at least 32GB of RAM If you run into issues with higher quantization levels, try using the q4 model or shut down any other programs that are using a lot of memory. Model variants By default, Ollama uses 4-bit quantization. To try other quantization levels, please try the other tags. The number after the q represents the number of bits used for quantization (i.e. q4 means 4-bit quantization). The higher the number, the more accurate the model is, but the slower it runs, and the more memory it requires. Aliases latest, 7b, 7b-q4_0 13b, 13b-q4_0 30b, 30b-q4_0 Model source Wizard Vicuna Uncensored source on Ollama 7b parameters original source: Eric Hartford 13b parameters original source: Eric Hartford 30b parameters original source: Eric Hartford Write Preview Wizard Vicuna Uncensored is a 7B, 13B, and 30B parameter model based on Llama 2 uncensored by Eric Hartford. The models were trained against LLaMA-7B with a subset of the dataset, responses that c",
      "variants": [
        {
          "tag": "wizard-vicuna-uncensored:latest",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-vicuna-uncensored:7b",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-vicuna-uncensored:13b",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-vicuna-uncensored:30b",
          "size_text": "18GB",
          "size_bytes": 19327352832,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-vicuna-uncensored:7b-q2_K",
          "size_text": "2.8GB",
          "size_bytes": 3006477107,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-vicuna-uncensored:7b-q3_K_S",
          "size_text": "2.9GB",
          "size_bytes": 3113851289,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-vicuna-uncensored:7b-q3_K_M",
          "size_text": "3.3GB",
          "size_bytes": 3543348019,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-vicuna-uncensored:7b-q3_K_L",
          "size_text": "3.6GB",
          "size_bytes": 3865470566,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-vicuna-uncensored:7b-q4_0",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-vicuna-uncensored:7b-q4_1",
          "size_text": "4.2GB",
          "size_bytes": 4509715660,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-vicuna-uncensored:7b-q4_K_S",
          "size_text": "3.9GB",
          "size_bytes": 4187593113,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-vicuna-uncensored:7b-q4_K_M",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-vicuna-uncensored:7b-q5_0",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-vicuna-uncensored:7b-q5_1",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-vicuna-uncensored:7b-q5_K_S",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-vicuna-uncensored:7b-q5_K_M",
          "size_text": "4.8GB",
          "size_bytes": 5153960755,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-vicuna-uncensored:7b-q6_K",
          "size_text": "5.5GB",
          "size_bytes": 5905580032,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-vicuna-uncensored:7b-q8_0",
          "size_text": "7.2GB",
          "size_bytes": 7730941132,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-vicuna-uncensored:7b-fp16",
          "size_text": "13GB",
          "size_bytes": 13958643712,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-vicuna-uncensored:13b-q2_K",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-vicuna-uncensored:13b-q3_K_S",
          "size_text": "5.7GB",
          "size_bytes": 6120328396,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-vicuna-uncensored:13b-q3_K_M",
          "size_text": "6.3GB",
          "size_bytes": 6764573491,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-vicuna-uncensored:13b-q3_K_L",
          "size_text": "6.9GB",
          "size_bytes": 7408818585,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-vicuna-uncensored:13b-q4_0",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-vicuna-uncensored:13b-q4_1",
          "size_text": "8.2GB",
          "size_bytes": 8804682956,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-vicuna-uncensored:13b-q4_K_S",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-vicuna-uncensored:13b-q4_K_M",
          "size_text": "7.9GB",
          "size_bytes": 8482560409,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-vicuna-uncensored:13b-q5_0",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-vicuna-uncensored:13b-q5_1",
          "size_text": "9.8GB",
          "size_bytes": 10522669875,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-vicuna-uncensored:13b-q5_K_S",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-vicuna-uncensored:13b-q5_K_M",
          "size_text": "9.2GB",
          "size_bytes": 9878424780,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-vicuna-uncensored:13b-q6_K",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-vicuna-uncensored:13b-q8_0",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-vicuna-uncensored:13b-fp16",
          "size_text": "26GB",
          "size_bytes": 27917287424,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-vicuna-uncensored:30b-q2_K",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-vicuna-uncensored:30b-q3_K_S",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-vicuna-uncensored:30b-q3_K_M",
          "size_text": "16GB",
          "size_bytes": 17179869184,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-vicuna-uncensored:30b-q3_K_L",
          "size_text": "17GB",
          "size_bytes": 18253611008,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-vicuna-uncensored:30b-q4_0",
          "size_text": "18GB",
          "size_bytes": 19327352832,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-vicuna-uncensored:30b-q4_1",
          "size_text": "20GB",
          "size_bytes": 21474836480,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-vicuna-uncensored:30b-q4_K_S",
          "size_text": "18GB",
          "size_bytes": 19327352832,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-vicuna-uncensored:30b-q4_K_M",
          "size_text": "20GB",
          "size_bytes": 21474836480,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-vicuna-uncensored:30b-q5_0",
          "size_text": "22GB",
          "size_bytes": 23622320128,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-vicuna-uncensored:30b-q5_1",
          "size_text": "24GB",
          "size_bytes": 25769803776,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-vicuna-uncensored:30b-q5_K_S",
          "size_text": "22GB",
          "size_bytes": 23622320128,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-vicuna-uncensored:30b-q5_K_M",
          "size_text": "23GB",
          "size_bytes": 24696061952,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-vicuna-uncensored:30b-q6_K",
          "size_text": "27GB",
          "size_bytes": 28991029248,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-vicuna-uncensored:30b-q8_0",
          "size_text": "35GB",
          "size_bytes": 37580963840,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-vicuna-uncensored:30b-fp16",
          "size_text": "65GB",
          "size_bytes": 69793218560,
          "context": "2K",
          "input": "Text"
        }
      ],
      "tags_count": 49
    },
    {
      "slug": "exaone-deep",
      "capabilities": [
        "reasoning"
      ],
      "pulls": 236300,
      "pulls_text": "236.3K \n  Downloads",
      "blurb": "exaone-deep EXAONE Deep exhibits superior capabilities in various tasks including math and coding benchmarks, ranging from 2.4B to 32B parameters developed and released by LG AI Research. 2.4b 7.8b 32b 13 Tags Updated 7 months ago",
      "name": "EXAONE Deep exhibits superior capabilities in various reasoning tasks including math and coding benchmarks, ranging from 2.4B to 32B parameters developed and released by LG AI Research.",
      "description": "Readme EXAONE Deep exhibits superior capabilities in various reasoning tasks including math and coding benchmarks, ranging from 2.4B to 32B parameters developed and released by LG AI Research. Evaluation results show that: EXAONE Deep 2.4B outperforms other models of comparable size EXAONE Deep 7.8B outperforms not only open-weight models of comparable scale but also a proprietary reasoning model OpenAI o1-mini EXAONE Deep 32B demonstrates competitive performance against leading open-weight models. References Hugging Face Write Preview <img src=\"/assets/library/exaone-deep/05e9cd86-a79b-4fd5-9e0f-7f18f1c1a928\" width=\"320\" /> EXAONE Deep exhibits superior capabilities in various reasoning tasks including math and coding benchmarks, ranging from 2.4B to 32B parameters developed and released by LG AI Research. Evaluation results show that: 1. EXAONE Deep 2.4B outperforms other models of comparable size 2. EXAONE Deep 7.8B outperforms not only open-weight models of comparable scale but also a proprietary reasoning model OpenAI o1-mini 3. EXAONE Deep 32B demonstrates competitive performance against leading open-weight models. ![exaone_deep_overall_performance.png](/assets/library/exaone-deep/b107bac2-f509-4a1b-9999-344d976c803e) ## References [Hugging Face](https://huggingface.co/LGAI-EXAONE/EXAONE-Deep-32B) Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "exaone-deep:latest",
          "size_text": "4.8GB",
          "size_bytes": 5153960755,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "exaone-deep:2.4b",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "exaone-deep:7.8b",
          "size_text": "4.8GB",
          "size_bytes": 5153960755,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "exaone-deep:32b",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "exaone-deep:2.4b-q4_K_M",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "exaone-deep:2.4b-q8_0",
          "size_text": "2.8GB",
          "size_bytes": 3006477107,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "exaone-deep:2.4b-fp16",
          "size_text": "5.3GB",
          "size_bytes": 5690831667,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "exaone-deep:7.8b-q4_K_M",
          "size_text": "4.8GB",
          "size_bytes": 5153960755,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "exaone-deep:7.8b-q8_0",
          "size_text": "8.3GB",
          "size_bytes": 8912057139,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "exaone-deep:7.8b-fp16",
          "size_text": "16GB",
          "size_bytes": 17179869184,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "exaone-deep:32b-q4_K_M",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "exaone-deep:32b-q8_0",
          "size_text": "34GB",
          "size_bytes": 36507222016,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "exaone-deep:32b-fp16",
          "size_text": "64GB",
          "size_bytes": 68719476736,
          "context": "32K",
          "input": "Text"
        }
      ],
      "tags_count": 13
    },
    {
      "slug": "starcoder",
      "capabilities": [],
      "pulls": 229000,
      "pulls_text": "229K \n  Downloads",
      "blurb": "starcoder StarCoder is a code generation model trained on 80+ programming languages. 1b 3b 7b 15b 100 Tags Updated 2 years ago",
      "name": "StarCoder is a code generation model trained on 80+ programming languages.",
      "description": "Readme StarCoder models are trained on GitHub code with over 80+ programming languages. Example prompt def print_hello_world(): Example response print(\"Hello world\") Write Preview <img src=\"https://github.com/jmorganca/ollama/assets/251292/1bf014aa-cde3-4f9d-88df-81773106b1dd\" width=\"400\" /> StarCoder models are trained on GitHub code with over 80+ programming languages. ## Example prompt ``` def print_hello_world(): ``` ## Example response ``` print(\"Hello world\") ``` Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "starcoder:latest",
          "size_text": "1.8GB",
          "size_bytes": 1932735283,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:1b",
          "size_text": "726MB",
          "size_bytes": 761266176,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:3b",
          "size_text": "1.8GB",
          "size_bytes": 1932735283,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:7b",
          "size_text": "4.3GB",
          "size_bytes": 4617089843,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:15b",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:1b-base",
          "size_text": "726MB",
          "size_bytes": 761266176,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:1b-base-q2_K",
          "size_text": "552MB",
          "size_bytes": 578813952,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:1b-base-q3_K_S",
          "size_text": "575MB",
          "size_bytes": 602931200,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:1b-base-q3_K_M",
          "size_text": "661MB",
          "size_bytes": 693108736,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:1b-base-q3_K_L",
          "size_text": "720MB",
          "size_bytes": 754974720,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:1b-base-q4_0",
          "size_text": "726MB",
          "size_bytes": 761266176,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:1b-base-q4_1",
          "size_text": "797MB",
          "size_bytes": 835715072,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:1b-base-q4_K_S",
          "size_text": "734MB",
          "size_bytes": 769654784,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:1b-base-q4_K_M",
          "size_text": "792MB",
          "size_bytes": 830472192,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:1b-base-q5_0",
          "size_text": "868MB",
          "size_bytes": 910163968,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:1b-base-q5_1",
          "size_text": "939MB",
          "size_bytes": 984612864,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:1b-base-q5_K_S",
          "size_text": "868MB",
          "size_bytes": 910163968,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:1b-base-q5_K_M",
          "size_text": "910MB",
          "size_bytes": 954204160,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:1b-base-q6_K",
          "size_text": "1.0GB",
          "size_bytes": 1073741824,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:1b-base-q8_0",
          "size_text": "1.3GB",
          "size_bytes": 1395864371,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:1b-base-fp16",
          "size_text": "2.5GB",
          "size_bytes": 2684354560,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:3b-base",
          "size_text": "1.8GB",
          "size_bytes": 1932735283,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:3b-base-q2_K",
          "size_text": "1.4GB",
          "size_bytes": 1503238553,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:3b-base-q3_K_S",
          "size_text": "1.4GB",
          "size_bytes": 1503238553,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:3b-base-q3_K_M",
          "size_text": "1.7GB",
          "size_bytes": 1825361100,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:3b-base-q3_K_L",
          "size_text": "1.8GB",
          "size_bytes": 1932735283,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:3b-base-q4_0",
          "size_text": "1.8GB",
          "size_bytes": 1932735283,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:3b-base-q4_1",
          "size_text": "2.0GB",
          "size_bytes": 2147483648,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:3b-base-q4_K_S",
          "size_text": "1.8GB",
          "size_bytes": 1932735283,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:3b-base-q4_K_M",
          "size_text": "2.0GB",
          "size_bytes": 2147483648,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:3b-base-q5_0",
          "size_text": "2.2GB",
          "size_bytes": 2362232012,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:3b-base-q5_1",
          "size_text": "2.4GB",
          "size_bytes": 2576980377,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:3b-base-q5_K_S",
          "size_text": "2.2GB",
          "size_bytes": 2362232012,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:3b-base-q5_K_M",
          "size_text": "2.3GB",
          "size_bytes": 2469606195,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:3b-base-q6_K",
          "size_text": "2.6GB",
          "size_bytes": 2791728742,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:3b-base-q8_0",
          "size_text": "3.4GB",
          "size_bytes": 3650722201,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:3b-base-fp16",
          "size_text": "6.4GB",
          "size_bytes": 6871947673,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:7b-base",
          "size_text": "4.3GB",
          "size_bytes": 4617089843,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:7b-base-q2_K",
          "size_text": "3.2GB",
          "size_bytes": 3435973836,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:7b-base-q3_K_S",
          "size_text": "3.3GB",
          "size_bytes": 3543348019,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:7b-base-q3_K_M",
          "size_text": "3.9GB",
          "size_bytes": 4187593113,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:7b-base-q3_K_L",
          "size_text": "4.3GB",
          "size_bytes": 4617089843,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:7b-base-q4_0",
          "size_text": "4.3GB",
          "size_bytes": 4617089843,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:7b-base-q4_1",
          "size_text": "4.8GB",
          "size_bytes": 5153960755,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:7b-base-q4_K_S",
          "size_text": "4.3GB",
          "size_bytes": 4617089843,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:7b-base-q4_K_M",
          "size_text": "4.8GB",
          "size_bytes": 5153960755,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:7b-base-q5_0",
          "size_text": "5.2GB",
          "size_bytes": 5583457484,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:7b-base-q5_1",
          "size_text": "5.7GB",
          "size_bytes": 6120328396,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:7b-base-q5_K_S",
          "size_text": "5.2GB",
          "size_bytes": 5583457484,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:7b-base-q5_K_M",
          "size_text": "5.5GB",
          "size_bytes": 5905580032,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:7b-base-q6_K",
          "size_text": "6.2GB",
          "size_bytes": 6657199308,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:7b-base-q8_0",
          "size_text": "8.0GB",
          "size_bytes": 8589934592,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:7b-base-fp16",
          "size_text": "15GB",
          "size_bytes": 16106127360,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:15b-base",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:15b-base-q2_K",
          "size_text": "6.7GB",
          "size_bytes": 7194070220,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:15b-base-q3_K_S",
          "size_text": "6.9GB",
          "size_bytes": 7408818585,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:15b-base-q3_K_M",
          "size_text": "8.2GB",
          "size_bytes": 8804682956,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:15b-base-q3_K_L",
          "size_text": "9.1GB",
          "size_bytes": 9771050598,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:15b-base-q4_0",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:15b-base-q4_1",
          "size_text": "10.0GB",
          "size_bytes": 10737418240,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:15b-base-q4_K_S",
          "size_text": "9.1GB",
          "size_bytes": 9771050598,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:15b-base-q4_K_M",
          "size_text": "10.0GB",
          "size_bytes": 10737418240,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:15b-base-q5_0",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:15b-base-q5_1",
          "size_text": "12GB",
          "size_bytes": 12884901888,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:15b-base-q5_K_S",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:15b-base-q5_K_M",
          "size_text": "12GB",
          "size_bytes": 12884901888,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:15b-base-q6_K",
          "size_text": "13GB",
          "size_bytes": 13958643712,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:15b-base-q8_0",
          "size_text": "17GB",
          "size_bytes": 18253611008,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:15b-base-fp16",
          "size_text": "32GB",
          "size_bytes": 34359738368,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:15b-plus",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:15b-plus-q2_K",
          "size_text": "6.7GB",
          "size_bytes": 7194070220,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:15b-plus-q3_K_S",
          "size_text": "6.9GB",
          "size_bytes": 7408818585,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:15b-plus-q3_K_M",
          "size_text": "8.2GB",
          "size_bytes": 8804682956,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:15b-plus-q3_K_L",
          "size_text": "9.1GB",
          "size_bytes": 9771050598,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:15b-plus-q4_0",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:15b-plus-q4_1",
          "size_text": "10.0GB",
          "size_bytes": 10737418240,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:15b-plus-q4_K_S",
          "size_text": "9.1GB",
          "size_bytes": 9771050598,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:15b-plus-q4_K_M",
          "size_text": "10.0GB",
          "size_bytes": 10737418240,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:15b-plus-q5_0",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:15b-plus-q5_1",
          "size_text": "12GB",
          "size_bytes": 12884901888,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:15b-plus-q5_K_S",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:15b-plus-q5_K_M",
          "size_text": "12GB",
          "size_bytes": 12884901888,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:15b-plus-q6_K",
          "size_text": "13GB",
          "size_bytes": 13958643712,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:15b-plus-q8_0",
          "size_text": "17GB",
          "size_bytes": 18253611008,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:15b-plus-fp16",
          "size_text": "32GB",
          "size_bytes": 34359738368,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:15b-q2_K",
          "size_text": "6.7GB",
          "size_bytes": 7194070220,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:15b-q3_K_S",
          "size_text": "6.9GB",
          "size_bytes": 7408818585,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:15b-q3_K_M",
          "size_text": "8.2GB",
          "size_bytes": 8804682956,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:15b-q3_K_L",
          "size_text": "9.1GB",
          "size_bytes": 9771050598,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:15b-q4_0",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:15b-q4_1",
          "size_text": "10.0GB",
          "size_bytes": 10737418240,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:15b-q4_K_S",
          "size_text": "9.1GB",
          "size_bytes": 9771050598,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:15b-q4_K_M",
          "size_text": "10.0GB",
          "size_bytes": 10737418240,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:15b-q5_0",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:15b-q5_1",
          "size_text": "12GB",
          "size_bytes": 12884901888,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:15b-q5_K_S",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:15b-q5_K_M",
          "size_text": "12GB",
          "size_bytes": 12884901888,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:15b-q6_K",
          "size_text": "13GB",
          "size_bytes": 13958643712,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:15b-q8_0",
          "size_text": "17GB",
          "size_bytes": 18253611008,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starcoder:15b-fp16",
          "size_text": "32GB",
          "size_bytes": 34359738368,
          "context": "8K",
          "input": "Text"
        }
      ],
      "tags_count": 100
    },
    {
      "slug": "nous-hermes",
      "capabilities": [],
      "pulls": 226200,
      "pulls_text": "226.2K \n  Downloads",
      "blurb": "nous-hermes General use models based on Llama and Llama 2 from Nous Research. 7b 13b 63 Tags Updated 2 years ago",
      "name": "General use models based on Llama and Llama 2 from Nous Research.",
      "description": "Readme Nous Hermes was released by Nous Research. There are two main variants here, a 13B parameter model based on Llama, and a 7B and 13B parameter model based on Llama 2. They are all general-use models trained with the same datasets. Get started with Nous Hermes The model used in the example below is the Nous Hermes Llama 2 model, with 7b parameters, which is a general chat model. API Start Ollama server (Run ollama serve ) Run the model curl -X POST http://localhost:11434/api/generate -d '{ \"model\": \"nous-hermes\", \"prompt\":\"Explain the process of how a refrigerator works to keep the contents inside cold.\" }' CLI Install Ollama Open the terminal and run ollama run nous-hermes Note: The ollama run command performs an ollama pull if the model is not already downloaded. To download the model without running it, use ollama pull nous-hermes Memory requirements 7b models generally require at least 8GB of RAM 13b models generally require at least 16GB of RAM If you run into issues with higher quantization levels, try using the q4 model or shut down any other programs that are using a lot of memory. Model variants Ollama offers many variants of the Nous Hermes model that are quantized based on the official models to run well locally. Nous Hermes Llama 2 is the original Nous Hermes model based on the original Llama model. Example: ollama run nous-hermes Nous Hermes Llama 1 is the original Nous Hermes model based on the original Llama model. Example: ollama run nous-hermes:13b-q4_0 By default, Ollama uses 4-bit quantization. To try other quantization levels, please try the other tags. The number after the q represents the number of bits used for quantization (i.e. q4 means 4-bit quantization). The higher the number, the more accurate the model is, but the slower it runs, and the more memory it requires. Aliases latest, 7b, 7b-llama2, 7b-llama2-q4_0 13b, 13b-llama2, 13b-llama2-q4_0 Model source Nous Hermes Llama 2 source on Ollama 7b parameters original source: Nous Researc",
      "variants": [
        {
          "tag": "nous-hermes:latest",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes:7b",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes:13b",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes:7b-llama2",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes:7b-llama2-q2_K",
          "size_text": "2.8GB",
          "size_bytes": 3006477107,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes:7b-llama2-q3_K_S",
          "size_text": "2.9GB",
          "size_bytes": 3113851289,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes:7b-llama2-q3_K_M",
          "size_text": "3.3GB",
          "size_bytes": 3543348019,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes:7b-llama2-q3_K_L",
          "size_text": "3.6GB",
          "size_bytes": 3865470566,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes:7b-llama2-q4_0",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes:7b-llama2-q4_1",
          "size_text": "4.2GB",
          "size_bytes": 4509715660,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes:7b-llama2-q4_K_S",
          "size_text": "3.9GB",
          "size_bytes": 4187593113,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes:7b-llama2-q4_K_M",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes:7b-llama2-q5_0",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes:7b-llama2-q5_1",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes:7b-llama2-q5_K_S",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes:7b-llama2-q5_K_M",
          "size_text": "4.8GB",
          "size_bytes": 5153960755,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes:7b-llama2-q6_K",
          "size_text": "5.5GB",
          "size_bytes": 5905580032,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes:7b-llama2-q8_0",
          "size_text": "7.2GB",
          "size_bytes": 7730941132,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes:7b-llama2-fp16",
          "size_text": "13GB",
          "size_bytes": 13958643712,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes:13b-llama2",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes:13b-llama2-q2_K",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes:13b-llama2-q3_K_S",
          "size_text": "5.7GB",
          "size_bytes": 6120328396,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes:13b-llama2-q3_K_M",
          "size_text": "6.3GB",
          "size_bytes": 6764573491,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes:13b-llama2-q3_K_L",
          "size_text": "6.9GB",
          "size_bytes": 7408818585,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes:13b-llama2-q4_0",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes:13b-llama2-q4_1",
          "size_text": "8.2GB",
          "size_bytes": 8804682956,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes:13b-llama2-q4_K_S",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes:13b-llama2-q4_K_M",
          "size_text": "7.9GB",
          "size_bytes": 8482560409,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes:13b-llama2-q5_0",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes:13b-llama2-q5_1",
          "size_text": "9.8GB",
          "size_bytes": 10522669875,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes:13b-llama2-q5_K_S",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes:13b-llama2-q5_K_M",
          "size_text": "9.2GB",
          "size_bytes": 9878424780,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes:13b-llama2-q6_K",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes:13b-llama2-q8_0",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes:13b-llama2-fp16",
          "size_text": "26GB",
          "size_bytes": 27917287424,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes:13b-q2_K",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes:13b-q3_K_S",
          "size_text": "5.7GB",
          "size_bytes": 6120328396,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes:13b-q3_K_M",
          "size_text": "6.3GB",
          "size_bytes": 6764573491,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes:13b-q3_K_L",
          "size_text": "6.9GB",
          "size_bytes": 7408818585,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes:13b-q4_0",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes:13b-q4_1",
          "size_text": "8.2GB",
          "size_bytes": 8804682956,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes:13b-q4_K_S",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes:13b-q4_K_M",
          "size_text": "7.9GB",
          "size_bytes": 8482560409,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes:13b-q5_0",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes:13b-q5_1",
          "size_text": "9.8GB",
          "size_bytes": 10522669875,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes:13b-q5_K_S",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes:13b-q5_K_M",
          "size_text": "9.2GB",
          "size_bytes": 9878424780,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes:13b-q6_K",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes:13b-q8_0",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes:13b-fp16",
          "size_text": "26GB",
          "size_bytes": 27917287424,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes:70b-llama2-q2_K",
          "size_text": "29GB",
          "size_bytes": 31138512896,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes:70b-llama2-q3_K_S",
          "size_text": "30GB",
          "size_bytes": 32212254720,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes:70b-llama2-q3_K_M",
          "size_text": "33GB",
          "size_bytes": 35433480192,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes:70b-llama2-q3_K_L",
          "size_text": "36GB",
          "size_bytes": 38654705664,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes:70b-llama2-q4_0",
          "size_text": "39GB",
          "size_bytes": 41875931136,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes:70b-llama2-q4_1",
          "size_text": "43GB",
          "size_bytes": 46170898432,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes:70b-llama2-q4_K_S",
          "size_text": "39GB",
          "size_bytes": 41875931136,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes:70b-llama2-q4_K_M",
          "size_text": "41GB",
          "size_bytes": 44023414784,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes:70b-llama2-q5_0",
          "size_text": "47GB",
          "size_bytes": 50465865728,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes:70b-llama2-q5_1",
          "size_text": "52GB",
          "size_bytes": 55834574848,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes:70b-llama2-q5_K_M",
          "size_text": "49GB",
          "size_bytes": 52613349376,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes:70b-llama2-q6_K",
          "size_text": "57GB",
          "size_bytes": 61203283968,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes:70b-llama2-fp16",
          "size_text": "138GB",
          "size_bytes": 148176371712,
          "context": "4K",
          "input": "Text"
        }
      ],
      "tags_count": 63
    },
    {
      "slug": "falcon",
      "capabilities": [],
      "pulls": 216200,
      "pulls_text": "216.2K \n  Downloads",
      "blurb": "falcon A large language model built by the Technology Innovation Institute (TII) for use in summarization, text generation, and chat bots. 7b 40b 180b 38 Tags Updated 2 years ago",
      "name": "A large language model built by the Technology Innovation Institute (TII) for use in summarization, text generation, and chat bots.",
      "description": "Readme Falcon is a family of high-performing large language models model built by the Technology Innovation Institute (TII), a research center part of Abu Dhabi government’s advanced technology research council overseeing technology research. CLI ollama run falcon \"Why is the sky blue?\" API curl -X POST http://localhost:11434/api/generate -d '{ \"model\": \"falcon\", \"prompt\": \"Why is the sky blue?\" }' Parameter counts Parameter Count Recommended memory 7 billion 8GB View ollama run falcon:7b 40 billion 32GB View ollama run falcon:40b 180 billion 192GB View ollama run falcon:180b Variations chat Chat models are fine-tuned on chat and instruction datasets with a mix of several large-scale conversational datasets. instruct Instruct models follow instructions and are fine-tuned on the baize instructional dataset. text Text models are the base foundation model without any fine-tuning for conversations, and are best used for simple text completion. Falcon 180B As of September 2023, the 180 billion parameter model, Falcon 180B, is the best-performing openly released LLM. It sits somewhere in between OpenAI’s GPT 3.5 and GPT 4. For running Falcon 180B, a powerful system is recommended with at least 192GB of total memory. Note: Falcon 180B is released under a different license than its smaller siblings that restricts commercial use under certain conditions. See the model details and license for more information. More information TII’s website Falcon 180B announcement TII on HuggingFace Write Preview <img src=\"https://github.com/jmorganca/ollama/assets/251292/af914f26-9f0b-4e66-947a-33ae0d558ccf\" alt=\"Technology Innovation Institute Logo\" width=\"240\"/> Falcon is a family of high-performing large language models model built by the Technology Innovation Institute (TII), a research center part of Abu Dhabi government’s advanced technology research council overseeing technology research. ### CLI ``` ollama run falcon \"Why is the sky blue?\" ``` ### API ``` curl -X POST http://localho",
      "variants": [
        {
          "tag": "falcon:latest",
          "size_text": "4.2GB",
          "size_bytes": 4509715660,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "falcon:instruct",
          "size_text": "4.2GB",
          "size_bytes": 4509715660,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "falcon:text",
          "size_text": "4.2GB",
          "size_bytes": 4509715660,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "falcon:7b",
          "size_text": "4.2GB",
          "size_bytes": 4509715660,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "falcon:40b",
          "size_text": "24GB",
          "size_bytes": 25769803776,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "falcon:180b",
          "size_text": "101GB",
          "size_bytes": 108447924224,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "falcon:7b-instruct",
          "size_text": "4.2GB",
          "size_bytes": 4509715660,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "falcon:7b-instruct-q4_0",
          "size_text": "4.2GB",
          "size_bytes": 4509715660,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "falcon:7b-instruct-q4_1",
          "size_text": "4.6GB",
          "size_bytes": 4939212390,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "falcon:7b-instruct-q5_0",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "falcon:7b-instruct-q5_1",
          "size_text": "5.5GB",
          "size_bytes": 5905580032,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "falcon:7b-instruct-q8_0",
          "size_text": "7.7GB",
          "size_bytes": 8267812044,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "falcon:7b-instruct-fp16",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "falcon:7b-text",
          "size_text": "4.2GB",
          "size_bytes": 4509715660,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "falcon:7b-text-q4_0",
          "size_text": "4.2GB",
          "size_bytes": 4509715660,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "falcon:7b-text-q4_1",
          "size_text": "4.6GB",
          "size_bytes": 4939212390,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "falcon:7b-text-q5_0",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "falcon:7b-text-q5_1",
          "size_text": "5.5GB",
          "size_bytes": 5905580032,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "falcon:7b-text-q8_0",
          "size_text": "7.7GB",
          "size_bytes": 8267812044,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "falcon:7b-text-fp16",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "falcon:40b-instruct",
          "size_text": "24GB",
          "size_bytes": 25769803776,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "falcon:40b-instruct-q4_0",
          "size_text": "24GB",
          "size_bytes": 25769803776,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "falcon:40b-instruct-q4_1",
          "size_text": "26GB",
          "size_bytes": 27917287424,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "falcon:40b-instruct-q5_0",
          "size_text": "29GB",
          "size_bytes": 31138512896,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "falcon:40b-instruct-q5_1",
          "size_text": "32GB",
          "size_bytes": 34359738368,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "falcon:40b-instruct-q8_0",
          "size_text": "44GB",
          "size_bytes": 47244640256,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "falcon:40b-instruct-fp16",
          "size_text": "84GB",
          "size_bytes": 90194313216,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "falcon:40b-text",
          "size_text": "24GB",
          "size_bytes": 25769803776,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "falcon:40b-text-q4_0",
          "size_text": "24GB",
          "size_bytes": 25769803776,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "falcon:40b-text-q4_1",
          "size_text": "26GB",
          "size_bytes": 27917287424,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "falcon:40b-text-q5_0",
          "size_text": "29GB",
          "size_bytes": 31138512896,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "falcon:40b-text-q5_1",
          "size_text": "32GB",
          "size_bytes": 34359738368,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "falcon:40b-text-q8_0",
          "size_text": "44GB",
          "size_bytes": 47244640256,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "falcon:40b-text-fp16",
          "size_text": "84GB",
          "size_bytes": 90194313216,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "falcon:180b-chat",
          "size_text": "101GB",
          "size_bytes": 108447924224,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "falcon:180b-chat-q4_0",
          "size_text": "101GB",
          "size_bytes": 108447924224,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "falcon:180b-text",
          "size_text": "101GB",
          "size_bytes": 108447924224,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "falcon:180b-text-q4_0",
          "size_text": "101GB",
          "size_bytes": 108447924224,
          "context": "2K",
          "input": "Text"
        }
      ],
      "tags_count": 38
    },
    {
      "slug": "deepseek-llm",
      "capabilities": [
        "reasoning"
      ],
      "pulls": 212000,
      "pulls_text": "212K \n  Downloads",
      "blurb": "deepseek-llm An advanced language model crafted with 2 trillion bilingual tokens. 7b 67b 64 Tags Updated 1 year ago",
      "name": "An advanced language model crafted with 2 trillion bilingual tokens.",
      "description": "Readme DeepSeek LLM is an advanced language model available in both 7 billion and 67 billion parameters. Both a chat and base variation are available. Superior General Capabilities: DeepSeek LLM 67B Base outperforms Llama2 70B Base in areas such as reasoning, coding, math, and Chinese comprehension. Proficient in Coding and Math: DeepSeek LLM 67B Chat exhibits outstanding performance in coding (using the HumanEval benchmark) and mathematics (using the GSM8K benchmark). References GitHub HuggingFace Write Preview <img src=\"https://github.com/jmorganca/ollama/assets/251292/0e62c3d2-6b1d-408a-a44d-273fc72697e6\" width=\"240\" /> DeepSeek LLM is an advanced language model available in both 7 billion and 67 billion parameters. Both a `chat` and `base` variation are available. <img src=\"https://github.com/jmorganca/ollama/assets/3325447/c74a21e9-1eb9-4036-9f83-6c3a027134c4\" width=\"500\" /> * Superior General Capabilities: DeepSeek LLM 67B Base outperforms Llama2 70B Base in areas such as reasoning, coding, math, and Chinese comprehension. * Proficient in Coding and Math: DeepSeek LLM 67B Chat exhibits outstanding performance in coding (using the HumanEval benchmark) and mathematics (using the GSM8K benchmark). ## References [GitHub](https://github.com/deepseek-ai/deepseek-LLM) [HuggingFace](https://huggingface.co/deepseek-ai/deepseek-llm-7b-chat) Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "deepseek-llm:latest",
          "size_text": "4.0GB",
          "size_bytes": 4294967296,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-llm:7b",
          "size_text": "4.0GB",
          "size_bytes": 4294967296,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-llm:67b",
          "size_text": "38GB",
          "size_bytes": 40802189312,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-llm:7b-base",
          "size_text": "4.0GB",
          "size_bytes": 4294967296,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-llm:7b-base-q2_K",
          "size_text": "3.0GB",
          "size_bytes": 3221225472,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-llm:7b-base-q3_K_S",
          "size_text": "3.1GB",
          "size_bytes": 3328599654,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-llm:7b-base-q3_K_M",
          "size_text": "3.5GB",
          "size_bytes": 3758096384,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-llm:7b-base-q3_K_L",
          "size_text": "3.7GB",
          "size_bytes": 3972844748,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-llm:7b-base-q4_0",
          "size_text": "4.0GB",
          "size_bytes": 4294967296,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-llm:7b-base-q4_1",
          "size_text": "4.4GB",
          "size_bytes": 4724464025,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-llm:7b-base-q4_K_S",
          "size_text": "4.0GB",
          "size_bytes": 4294967296,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-llm:7b-base-q4_K_M",
          "size_text": "4.2GB",
          "size_bytes": 4509715660,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-llm:7b-base-q5_0",
          "size_text": "4.8GB",
          "size_bytes": 5153960755,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-llm:7b-base-q5_1",
          "size_text": "5.2GB",
          "size_bytes": 5583457484,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-llm:7b-base-q5_K_S",
          "size_text": "4.8GB",
          "size_bytes": 5153960755,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-llm:7b-base-q5_K_M",
          "size_text": "4.9GB",
          "size_bytes": 5261334937,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-llm:7b-base-q6_K",
          "size_text": "5.7GB",
          "size_bytes": 6120328396,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-llm:7b-base-q8_0",
          "size_text": "7.3GB",
          "size_bytes": 7838315315,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-llm:7b-base-fp16",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-llm:7b-chat",
          "size_text": "4.0GB",
          "size_bytes": 4294967296,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-llm:7b-chat-q2_K",
          "size_text": "3.0GB",
          "size_bytes": 3221225472,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-llm:7b-chat-q3_K_S",
          "size_text": "3.1GB",
          "size_bytes": 3328599654,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-llm:7b-chat-q3_K_M",
          "size_text": "3.5GB",
          "size_bytes": 3758096384,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-llm:7b-chat-q3_K_L",
          "size_text": "3.7GB",
          "size_bytes": 3972844748,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-llm:7b-chat-q4_0",
          "size_text": "4.0GB",
          "size_bytes": 4294967296,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-llm:7b-chat-q4_1",
          "size_text": "4.4GB",
          "size_bytes": 4724464025,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-llm:7b-chat-q4_K_S",
          "size_text": "4.0GB",
          "size_bytes": 4294967296,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-llm:7b-chat-q4_K_M",
          "size_text": "4.2GB",
          "size_bytes": 4509715660,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-llm:7b-chat-q5_0",
          "size_text": "4.8GB",
          "size_bytes": 5153960755,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-llm:7b-chat-q5_1",
          "size_text": "5.2GB",
          "size_bytes": 5583457484,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-llm:7b-chat-q5_K_S",
          "size_text": "4.8GB",
          "size_bytes": 5153960755,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-llm:7b-chat-q5_K_M",
          "size_text": "4.9GB",
          "size_bytes": 5261334937,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-llm:7b-chat-q6_K",
          "size_text": "5.7GB",
          "size_bytes": 6120328396,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-llm:7b-chat-q8_0",
          "size_text": "7.3GB",
          "size_bytes": 7838315315,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-llm:7b-chat-fp16",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-llm:67b-base",
          "size_text": "38GB",
          "size_bytes": 40802189312,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-llm:67b-base-q2_K",
          "size_text": "28GB",
          "size_bytes": 30064771072,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-llm:67b-base-q3_K_S",
          "size_text": "29GB",
          "size_bytes": 31138512896,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-llm:67b-base-q3_K_M",
          "size_text": "33GB",
          "size_bytes": 35433480192,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-llm:67b-base-q3_K_L",
          "size_text": "36GB",
          "size_bytes": 38654705664,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-llm:67b-base-q4_0",
          "size_text": "38GB",
          "size_bytes": 40802189312,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-llm:67b-base-q4_1",
          "size_text": "42GB",
          "size_bytes": 45097156608,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-llm:67b-base-q4_K_S",
          "size_text": "38GB",
          "size_bytes": 40802189312,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-llm:67b-base-q4_K_M",
          "size_text": "40GB",
          "size_bytes": 42949672960,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-llm:67b-base-q5_0",
          "size_text": "46GB",
          "size_bytes": 49392123904,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-llm:67b-base-q5_1",
          "size_text": "51GB",
          "size_bytes": 54760833024,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-llm:67b-base-q5_K_S",
          "size_text": "46GB",
          "size_bytes": 49392123904,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-llm:67b-base-q5_K_M",
          "size_text": "48GB",
          "size_bytes": 51539607552,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-llm:67b-base-q6_K",
          "size_text": "55GB",
          "size_bytes": 59055800320,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-llm:67b-base-q8_0",
          "size_text": "72GB",
          "size_bytes": 77309411328,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-llm:67b-base-fp16",
          "size_text": "135GB",
          "size_bytes": 144955146240,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-llm:67b-chat",
          "size_text": "38GB",
          "size_bytes": 40802189312,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-llm:67b-chat-q2_K",
          "size_text": "28GB",
          "size_bytes": 30064771072,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-llm:67b-chat-q3_K_S",
          "size_text": "29GB",
          "size_bytes": 31138512896,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-llm:67b-chat-q3_K_M",
          "size_text": "33GB",
          "size_bytes": 35433480192,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-llm:67b-chat-q3_K_L",
          "size_text": "36GB",
          "size_bytes": 38654705664,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-llm:67b-chat-q4_0",
          "size_text": "38GB",
          "size_bytes": 40802189312,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-llm:67b-chat-q4_1",
          "size_text": "42GB",
          "size_bytes": 45097156608,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-llm:67b-chat-q4_K_S",
          "size_text": "38GB",
          "size_bytes": 40802189312,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-llm:67b-chat-q4_K_M",
          "size_text": "40GB",
          "size_bytes": 42949672960,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-llm:67b-chat-q5_0",
          "size_text": "46GB",
          "size_bytes": 49392123904,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-llm:67b-chat-q5_1",
          "size_text": "51GB",
          "size_bytes": 54760833024,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-llm:67b-chat-q5_K_S",
          "size_text": "46GB",
          "size_bytes": 49392123904,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-llm:67b-chat-fp16",
          "size_text": "135GB",
          "size_bytes": 144955146240,
          "context": "4K",
          "input": "Text"
        }
      ],
      "tags_count": 64
    },
    {
      "slug": "vicuna",
      "capabilities": [],
      "pulls": 206300,
      "pulls_text": "206.3K \n  Downloads",
      "blurb": "vicuna General use chat model based on Llama and Llama 2 with 2K to 16K context sizes. 7b 13b 33b 111 Tags Updated 2 years ago",
      "name": "General use chat model based on Llama and Llama 2 with 2K to 16K context sizes.",
      "description": "Readme Vicuna is a chat assistant model. It includes 3 different variants in 3 different sizes. v1.3 is trained by fine-tuning Llama and has a context size of 2048 tokens. v1.5 is trained by fine-tuning Llama 2 and has a context size of 2048 tokens. v1.5-16k is trained by fine-tuning Llama 2 and has a context size of 16k tokens. All three variants are trained using conversations collected from ShareGPT. Example prompts What is the meaning of life? Explain it in 5 paragraphs. References HuggingFace Write Preview Vicuna is a chat assistant model. It includes 3 different variants in 3 different sizes. v1.3 is trained by fine-tuning Llama and has a context size of 2048 tokens. v1.5 is trained by fine-tuning Llama 2 and has a context size of 2048 tokens. v1.5-16k is trained by fine-tuning Llama 2 and has a context size of 16k tokens. All three variants are trained using conversations collected from ShareGPT. ## Example prompts ``` What is the meaning of life? Explain it in 5 paragraphs. ``` ## References [HuggingFace](https://huggingface.co/lmsys) Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "vicuna:latest",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "vicuna:7b",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "vicuna:13b",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "vicuna:33b",
          "size_text": "18GB",
          "size_bytes": 19327352832,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "vicuna:7b-16k",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "vicuna:7b-v1.5-16k-q2_K",
          "size_text": "2.8GB",
          "size_bytes": 3006477107,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "vicuna:7b-v1.5-q2_K",
          "size_text": "2.8GB",
          "size_bytes": 3006477107,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "vicuna:7b-v1.5-16k-q3_K_S",
          "size_text": "2.9GB",
          "size_bytes": 3113851289,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "vicuna:7b-v1.5-q3_K_S",
          "size_text": "2.9GB",
          "size_bytes": 3113851289,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "vicuna:7b-v1.5-16k-q3_K_M",
          "size_text": "3.3GB",
          "size_bytes": 3543348019,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "vicuna:7b-v1.5-q3_K_M",
          "size_text": "3.3GB",
          "size_bytes": 3543348019,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "vicuna:7b-v1.5-16k-q3_K_L",
          "size_text": "3.6GB",
          "size_bytes": 3865470566,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "vicuna:7b-v1.5-q3_K_L",
          "size_text": "3.6GB",
          "size_bytes": 3865470566,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "vicuna:7b-v1.5-16k-q4_0",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "vicuna:7b-v1.5-q4_0",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "vicuna:7b-v1.5-16k-q4_1",
          "size_text": "4.2GB",
          "size_bytes": 4509715660,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "vicuna:7b-v1.5-q4_1",
          "size_text": "4.2GB",
          "size_bytes": 4509715660,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "vicuna:7b-v1.5-16k-q4_K_S",
          "size_text": "3.9GB",
          "size_bytes": 4187593113,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "vicuna:7b-v1.5-q4_K_S",
          "size_text": "3.9GB",
          "size_bytes": 4187593113,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "vicuna:7b-v1.5-16k-q4_K_M",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "vicuna:7b-v1.5-q4_K_M",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "vicuna:7b-v1.5-16k-q5_0",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "vicuna:7b-v1.5-q5_0",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "vicuna:7b-v1.5-16k-q5_1",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "vicuna:7b-v1.5-q5_1",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "vicuna:7b-v1.5-16k-q5_K_S",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "vicuna:7b-v1.5-q5_K_S",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "vicuna:7b-v1.5-16k-q5_K_M",
          "size_text": "4.8GB",
          "size_bytes": 5153960755,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "vicuna:7b-v1.5-q5_K_M",
          "size_text": "4.8GB",
          "size_bytes": 5153960755,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "vicuna:7b-v1.5-16k-q6_K",
          "size_text": "5.5GB",
          "size_bytes": 5905580032,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "vicuna:7b-v1.5-q6_K",
          "size_text": "5.5GB",
          "size_bytes": 5905580032,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "vicuna:7b-v1.5-16k-q8_0",
          "size_text": "7.2GB",
          "size_bytes": 7730941132,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "vicuna:7b-v1.5-q8_0",
          "size_text": "7.2GB",
          "size_bytes": 7730941132,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "vicuna:7b-v1.5-16k-fp16",
          "size_text": "13GB",
          "size_bytes": 13958643712,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "vicuna:7b-v1.5-fp16",
          "size_text": "13GB",
          "size_bytes": 13958643712,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "vicuna:7b-q2_K",
          "size_text": "2.8GB",
          "size_bytes": 3006477107,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "vicuna:7b-q3_K_S",
          "size_text": "2.9GB",
          "size_bytes": 3113851289,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "vicuna:7b-q3_K_M",
          "size_text": "3.3GB",
          "size_bytes": 3543348019,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "vicuna:7b-q3_K_L",
          "size_text": "3.6GB",
          "size_bytes": 3865470566,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "vicuna:7b-q4_0",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "vicuna:7b-q4_1",
          "size_text": "4.2GB",
          "size_bytes": 4509715660,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "vicuna:7b-q4_K_S",
          "size_text": "3.9GB",
          "size_bytes": 4187593113,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "vicuna:7b-q4_K_M",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "vicuna:7b-q5_0",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "vicuna:7b-q5_1",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "vicuna:7b-q5_K_S",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "vicuna:7b-q5_K_M",
          "size_text": "4.8GB",
          "size_bytes": 5153960755,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "vicuna:7b-q6_K",
          "size_text": "5.5GB",
          "size_bytes": 5905580032,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "vicuna:7b-q8_0",
          "size_text": "7.2GB",
          "size_bytes": 7730941132,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "vicuna:7b-fp16",
          "size_text": "13GB",
          "size_bytes": 13958643712,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "vicuna:13b-16k",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "vicuna:13b-v1.5-16k-q2_K",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "vicuna:13b-v1.5-q2_K",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "vicuna:13b-v1.5-16k-q3_K_S",
          "size_text": "5.7GB",
          "size_bytes": 6120328396,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "vicuna:13b-v1.5-q3_K_S",
          "size_text": "5.7GB",
          "size_bytes": 6120328396,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "vicuna:13b-v1.5-16k-q3_K_M",
          "size_text": "6.3GB",
          "size_bytes": 6764573491,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "vicuna:13b-v1.5-q3_K_M",
          "size_text": "6.3GB",
          "size_bytes": 6764573491,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "vicuna:13b-v1.5-16k-q3_K_L",
          "size_text": "6.9GB",
          "size_bytes": 7408818585,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "vicuna:13b-v1.5-q3_K_L",
          "size_text": "6.9GB",
          "size_bytes": 7408818585,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "vicuna:13b-v1.5-16k-q4_0",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "vicuna:13b-v1.5-q4_0",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "vicuna:13b-v1.5-16k-q4_1",
          "size_text": "8.2GB",
          "size_bytes": 8804682956,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "vicuna:13b-v1.5-q4_1",
          "size_text": "8.2GB",
          "size_bytes": 8804682956,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "vicuna:13b-v1.5-16k-q4_K_S",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "vicuna:13b-v1.5-q4_K_S",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "vicuna:13b-v1.5-16k-q4_K_M",
          "size_text": "7.9GB",
          "size_bytes": 8482560409,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "vicuna:13b-v1.5-q4_K_M",
          "size_text": "7.9GB",
          "size_bytes": 8482560409,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "vicuna:13b-v1.5-16k-q5_0",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "vicuna:13b-v1.5-q5_0",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "vicuna:13b-v1.5-16k-q5_1",
          "size_text": "9.8GB",
          "size_bytes": 10522669875,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "vicuna:13b-v1.5-q5_1",
          "size_text": "9.8GB",
          "size_bytes": 10522669875,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "vicuna:13b-v1.5-16k-q5_K_S",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "vicuna:13b-v1.5-q5_K_S",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "vicuna:13b-v1.5-16k-q5_K_M",
          "size_text": "9.2GB",
          "size_bytes": 9878424780,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "vicuna:13b-v1.5-q5_K_M",
          "size_text": "9.2GB",
          "size_bytes": 9878424780,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "vicuna:13b-v1.5-16k-q6_K",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "vicuna:13b-v1.5-q6_K",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "vicuna:13b-v1.5-16k-q8_0",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "vicuna:13b-v1.5-q8_0",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "vicuna:13b-v1.5-16k-fp16",
          "size_text": "26GB",
          "size_bytes": 27917287424,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "vicuna:13b-v1.5-fp16",
          "size_text": "26GB",
          "size_bytes": 27917287424,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "vicuna:13b-q2_K",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "vicuna:13b-q3_K_S",
          "size_text": "5.7GB",
          "size_bytes": 6120328396,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "vicuna:13b-q3_K_M",
          "size_text": "6.3GB",
          "size_bytes": 6764573491,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "vicuna:13b-q3_K_L",
          "size_text": "6.9GB",
          "size_bytes": 7408818585,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "vicuna:13b-q4_0",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "vicuna:13b-q4_1",
          "size_text": "8.2GB",
          "size_bytes": 8804682956,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "vicuna:13b-q4_K_S",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "vicuna:13b-q4_K_M",
          "size_text": "7.9GB",
          "size_bytes": 8482560409,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "vicuna:13b-q5_0",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "vicuna:13b-q5_1",
          "size_text": "9.8GB",
          "size_bytes": 10522669875,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "vicuna:13b-q5_K_S",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "vicuna:13b-q5_K_M",
          "size_text": "9.2GB",
          "size_bytes": 9878424780,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "vicuna:13b-q6_K",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "vicuna:13b-q8_0",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "vicuna:13b-fp16",
          "size_text": "26GB",
          "size_bytes": 27917287424,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "vicuna:33b-q2_K",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "vicuna:33b-q3_K_S",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "vicuna:33b-q3_K_M",
          "size_text": "16GB",
          "size_bytes": 17179869184,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "vicuna:33b-q3_K_L",
          "size_text": "17GB",
          "size_bytes": 18253611008,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "vicuna:33b-q4_0",
          "size_text": "18GB",
          "size_bytes": 19327352832,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "vicuna:33b-q4_1",
          "size_text": "20GB",
          "size_bytes": 21474836480,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "vicuna:33b-q4_K_S",
          "size_text": "18GB",
          "size_bytes": 19327352832,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "vicuna:33b-q4_K_M",
          "size_text": "20GB",
          "size_bytes": 21474836480,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "vicuna:33b-q5_0",
          "size_text": "22GB",
          "size_bytes": 23622320128,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "vicuna:33b-q5_1",
          "size_text": "24GB",
          "size_bytes": 25769803776,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "vicuna:33b-q5_K_S",
          "size_text": "22GB",
          "size_bytes": 23622320128,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "vicuna:33b-q5_K_M",
          "size_text": "23GB",
          "size_bytes": 24696061952,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "vicuna:33b-q6_K",
          "size_text": "27GB",
          "size_bytes": 28991029248,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "vicuna:33b-q8_0",
          "size_text": "35GB",
          "size_bytes": 37580963840,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "vicuna:33b-fp16",
          "size_text": "65GB",
          "size_bytes": 69793218560,
          "context": "2K",
          "input": "Text"
        }
      ],
      "tags_count": 111
    },
    {
      "slug": "openchat",
      "capabilities": [],
      "pulls": 205100,
      "pulls_text": "205.1K \n  Downloads",
      "blurb": "openchat A family of open-source models trained on a wide variety of data, surpassing ChatGPT on various benchmarks. Updated to version 3.5-0106. 7b 50 Tags Updated 1 year ago",
      "name": "A family of open-source models trained on a wide variety of data, surpassing ChatGPT on various benchmarks. Updated to version 3.5-0106.",
      "description": "Readme OpenChat is set of open-source language models, fine-tuned with C-RLFT: a strategy inspired by offline reinforcement learning. Updated to OpenChat-3.5-1210, this new version of the model model excels at coding tasks and scores very high on many open-source LLM benchmarks. References GitHub HuggingFace Write Preview <img src=\"https://github.com/jmorganca/ollama/assets/251292/b39cea20-f0e9-4f63-a5b1-fa0c2368c286\" width=\"260\" /> OpenChat is set of open-source language models, fine-tuned with C-RLFT: a strategy inspired by offline reinforcement learning. Updated to OpenChat-3.5-1210, this new version of the model model excels at coding tasks and scores very high on many open-source LLM benchmarks. ## References [GitHub](https://github.com/imoneoi/openchat) [HuggingFace](https://huggingface.co/openchat/openchat_3.5) Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "openchat:latest",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "openchat:7b",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "openchat:7b-v3.5",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "openchat:7b-v3.5-0106",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "openchat:7b-v3.5-0106-q2_K",
          "size_text": "3.1GB",
          "size_bytes": 3328599654,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "openchat:7b-v3.5-q2_K",
          "size_text": "3.1GB",
          "size_bytes": 3328599654,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "openchat:7b-v3.5-0106-q3_K_S",
          "size_text": "3.2GB",
          "size_bytes": 3435973836,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "openchat:7b-v3.5-q3_K_S",
          "size_text": "3.2GB",
          "size_bytes": 3435973836,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "openchat:7b-v3.5-0106-q3_K_M",
          "size_text": "3.5GB",
          "size_bytes": 3758096384,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "openchat:7b-v3.5-q3_K_M",
          "size_text": "3.5GB",
          "size_bytes": 3758096384,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "openchat:7b-v3.5-0106-q3_K_L",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "openchat:7b-v3.5-q3_K_L",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "openchat:7b-v3.5-0106-q4_0",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "openchat:7b-v3.5-q4_0",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "openchat:7b-v3.5-0106-q4_1",
          "size_text": "4.6GB",
          "size_bytes": 4939212390,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "openchat:7b-v3.5-q4_1",
          "size_text": "4.6GB",
          "size_bytes": 4939212390,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "openchat:7b-v3.5-0106-q4_K_S",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "openchat:7b-v3.5-q4_K_S",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "openchat:7b-v3.5-0106-q4_K_M",
          "size_text": "4.4GB",
          "size_bytes": 4724464025,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "openchat:7b-v3.5-q4_K_M",
          "size_text": "4.4GB",
          "size_bytes": 4724464025,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "openchat:7b-v3.5-0106-q5_0",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "openchat:7b-v3.5-q5_0",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "openchat:7b-v3.5-0106-q5_1",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "openchat:7b-v3.5-q5_1",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "openchat:7b-v3.5-0106-q5_K_S",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "openchat:7b-v3.5-0106-q5_K_M",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "openchat:7b-v3.5-0106-q6_K",
          "size_text": "5.9GB",
          "size_bytes": 6335076761,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "openchat:7b-v3.5-0106-q8_0",
          "size_text": "7.7GB",
          "size_bytes": 8267812044,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "openchat:7b-v3.5-0106-fp16",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "openchat:7b-v3.5-1210",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "openchat:7b-v3.5-1210-q2_K",
          "size_text": "3.1GB",
          "size_bytes": 3328599654,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "openchat:7b-v3.5-1210-q3_K_S",
          "size_text": "3.2GB",
          "size_bytes": 3435973836,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "openchat:7b-v3.5-1210-q3_K_M",
          "size_text": "3.5GB",
          "size_bytes": 3758096384,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "openchat:7b-v3.5-1210-q3_K_L",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "openchat:7b-v3.5-1210-q4_0",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "openchat:7b-v3.5-1210-q4_1",
          "size_text": "4.6GB",
          "size_bytes": 4939212390,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "openchat:7b-v3.5-1210-q4_K_S",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "openchat:7b-v3.5-1210-q4_K_M",
          "size_text": "4.4GB",
          "size_bytes": 4724464025,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "openchat:7b-v3.5-1210-q5_0",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "openchat:7b-v3.5-1210-q5_1",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "openchat:7b-v3.5-1210-q5_K_S",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "openchat:7b-v3.5-q5_K_S",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "openchat:7b-v3.5-1210-q5_K_M",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "openchat:7b-v3.5-q5_K_M",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "openchat:7b-v3.5-1210-q6_K",
          "size_text": "5.9GB",
          "size_bytes": 6335076761,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "openchat:7b-v3.5-q6_K",
          "size_text": "5.9GB",
          "size_bytes": 6335076761,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "openchat:7b-v3.5-1210-q8_0",
          "size_text": "7.7GB",
          "size_bytes": 8267812044,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "openchat:7b-v3.5-q8_0",
          "size_text": "7.7GB",
          "size_bytes": 8267812044,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "openchat:7b-v3.5-1210-fp16",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "openchat:7b-v3.5-fp16",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "8K",
          "input": "Text"
        }
      ],
      "tags_count": 50
    },
    {
      "slug": "opencoder",
      "capabilities": [],
      "pulls": 197100,
      "pulls_text": "197.1K \n  Downloads",
      "blurb": "opencoder OpenCoder is an open and reproducible code LLM family which includes 1.5B and 8B models, supporting chat in English and Chinese languages. 1.5b 8b 9 Tags Updated 11 months ago",
      "name": "OpenCoder is an open and reproducible code LLM family which includes 1.5B and 8B models, supporting chat in English and Chinese languages.",
      "description": "Readme OpenCoder is an open and reproducible code LLM family which includes 1.5B and 8B models, supporting both English and Chinese languages. Starting from scratch, OpenCoder is pretrained on 2.5 trillion tokens composed of 90% raw code and 10% code-related web data, and supervised finetuned on over 4.5M high-quality SFT examples, finally reaching the performance of top-tier code LLMs. We provide not only model weights and inference code, but also the reproducible training data, the complete data processing pipeline, rigorous experimental ablation results, and detailed training protocols. Empowering researchers to build and innovate, OpenCoder is your open foundation for advancing code AI. Complete Open Source : OpenCoder ensures full transparency by releasing not only the model weights and forthcoming inference code but also the complete data-cleaning code for training. This release includes high-quality synthetic data, an extensive set of checkpoints, and a dataset of over 4.5 million supervised fine-tuning (SFT) entries, making OpenCoder one of the most comprehensively open-sourced models available. Comprehensive Experimental Analysis : OpenCoder is rigorously tested through extensive ablation studies on various data-cleaning strategies and training processes, including file-level and repository-level deduplication experiments, ensuring thorough exploration and validation of the model’s performance. High-Quality Synthetic Data : OpenCoder provides a fully developed synthetic data generation process and over 4.5 million SFT data entries, establishing a robust data foundation for model training and evaluation. Exceptional Performance : OpenCoder achieves high performance across multiple language model benchmarks, positioning it among the leading open-source models for code. References GitHub Paper Hugging Face Write Preview <img src=\"/assets/library/opencoder/6078034f-fdbf-47c2-9b63-69ce506c0225\" width=\"280\" /> **OpenCoder** is an open and reproducible code LLM fa",
      "variants": [
        {
          "tag": "opencoder:latest",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "opencoder:1.5b",
          "size_text": "1.4GB",
          "size_bytes": 1503238553,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "opencoder:8b",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "opencoder:1.5b-instruct-q4_K_M",
          "size_text": "1.4GB",
          "size_bytes": 1503238553,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "opencoder:1.5b-instruct-q8_0",
          "size_text": "2.0GB",
          "size_bytes": 2147483648,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "opencoder:1.5b-instruct-fp16",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "opencoder:8b-instruct-q4_K_M",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "opencoder:8b-instruct-q8_0",
          "size_text": "8.3GB",
          "size_bytes": 8912057139,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "opencoder:8b-instruct-fp16",
          "size_text": "16GB",
          "size_bytes": 17179869184,
          "context": "8K",
          "input": "Text"
        }
      ],
      "tags_count": 9
    },
    {
      "slug": "deepseek-v2",
      "capabilities": [],
      "pulls": 196900,
      "pulls_text": "196.9K \n  Downloads",
      "blurb": "deepseek-v2 A strong, economical, and efficient Mixture-of-Experts language model. 16b 236b 34 Tags Updated 1 year ago",
      "name": "A strong, economical, and efficient Mixture-of-Experts language model.",
      "description": "Readme Note: this model requires Ollama 0.1.40 . DeepSeek-V2 is a a strong Mixture-of-Experts (MoE) language model characterized by economical training and efficient inference. Note: this model is bilingual in English and Chinese. The model comes in two sizes: 16B Lite: ollama run deepseek-v2:16b 236B: ollama run deepseek-v2:236b References GitHub Write Preview <img src=\"https://ollama.com/assets/library/deepseek-v2/9422627b-6d24-401b-8d49-5d316df9956b\" width=\"320\" /> > Note: this model requires [Ollama 0.1.40](https://github.com/ollama/ollama/releases/tag/v0.1.40). DeepSeek-V2 is a a strong Mixture-of-Experts (MoE) language model characterized by economical training and efficient inference. Note: this model is bilingual in English and Chinese. The model comes in two sizes: * 16B Lite: `ollama run deepseek-v2:16b` * 236B: `ollama run deepseek-v2:236b` ## References [GitHub](https://github.com/deepseek-ai/DeepSeek-V2) Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "deepseek-v2:latest",
          "size_text": "8.9GB",
          "size_bytes": 9556302233,
          "context": "160K",
          "input": "Text"
        },
        {
          "tag": "deepseek-v2:lite",
          "size_text": "8.9GB",
          "size_bytes": 9556302233,
          "context": "160K",
          "input": "Text"
        },
        {
          "tag": "deepseek-v2:16b",
          "size_text": "8.9GB",
          "size_bytes": 9556302233,
          "context": "160K",
          "input": "Text"
        },
        {
          "tag": "deepseek-v2:236b",
          "size_text": "133GB",
          "size_bytes": 142807662592,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-v2:16b-lite-chat-q2_K",
          "size_text": "6.4GB",
          "size_bytes": 6871947673,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-v2:16b-lite-chat-q3_K_S",
          "size_text": "7.5GB",
          "size_bytes": 8053063680,
          "context": "160K",
          "input": "Text"
        },
        {
          "tag": "deepseek-v2:16b-lite-chat-q3_K_M",
          "size_text": "8.1GB",
          "size_bytes": 8697308774,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-v2:16b-lite-chat-q3_K_L",
          "size_text": "8.5GB",
          "size_bytes": 9126805504,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-v2:16b-lite-chat-q4_0",
          "size_text": "8.9GB",
          "size_bytes": 9556302233,
          "context": "160K",
          "input": "Text"
        },
        {
          "tag": "deepseek-v2:16b-lite-chat-q4_1",
          "size_text": "9.9GB",
          "size_bytes": 10630044057,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-v2:16b-lite-chat-q4_K_S",
          "size_text": "9.5GB",
          "size_bytes": 10200547328,
          "context": "160K",
          "input": "Text"
        },
        {
          "tag": "deepseek-v2:16b-lite-chat-q4_K_M",
          "size_text": "10GB",
          "size_bytes": 10737418240,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-v2:16b-lite-chat-q5_0",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-v2:16b-lite-chat-q5_1",
          "size_text": "12GB",
          "size_bytes": 12884901888,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-v2:16b-lite-chat-q5_K_S",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-v2:16b-lite-chat-q5_K_M",
          "size_text": "12GB",
          "size_bytes": 12884901888,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-v2:16b-lite-chat-q6_K",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-v2:16b-lite-chat-q8_0",
          "size_text": "17GB",
          "size_bytes": 18253611008,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-v2:16b-lite-chat-fp16",
          "size_text": "31GB",
          "size_bytes": 33285996544,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-v2:236b-chat-q2_K",
          "size_text": "86GB",
          "size_bytes": 92341796864,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-v2:236b-chat-q3_K_S",
          "size_text": "102GB",
          "size_bytes": 109521666048,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-v2:236b-chat-q3_K_M",
          "size_text": "113GB",
          "size_bytes": 121332826112,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-v2:236b-chat-q3_K_L",
          "size_text": "122GB",
          "size_bytes": 130996502528,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-v2:236b-chat-q4_0",
          "size_text": "133GB",
          "size_bytes": 142807662592,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-v2:236b-chat-q4_1",
          "size_text": "148GB",
          "size_bytes": 158913789952,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-v2:236b-chat-q4_K_S",
          "size_text": "134GB",
          "size_bytes": 143881404416,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-v2:236b-chat-q4_K_M",
          "size_text": "142GB",
          "size_bytes": 152471339008,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-v2:236b-chat-q5_0",
          "size_text": "162GB",
          "size_bytes": 173946175488,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-v2:236b-chat-q5_1",
          "size_text": "177GB",
          "size_bytes": 190052302848,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-v2:236b-chat-q5_K_S",
          "size_text": "162GB",
          "size_bytes": 173946175488,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-v2:236b-chat-q5_K_M",
          "size_text": "167GB",
          "size_bytes": 179314884608,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-v2:236b-chat-q6_K",
          "size_text": "194GB",
          "size_bytes": 208305913856,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-v2:236b-chat-q8_0",
          "size_text": "251GB",
          "size_bytes": 269509197824,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-v2:236b-chat-fp16",
          "size_text": "472GB",
          "size_bytes": 506806140928,
          "context": "4K",
          "input": "Text"
        }
      ],
      "tags_count": 34
    },
    {
      "slug": "mistral-openorca",
      "capabilities": [],
      "pulls": 190000,
      "pulls_text": "190K \n  Downloads",
      "blurb": "mistral-openorca Mistral OpenOrca is a 7 billion parameter model, fine-tuned on top of the Mistral 7B model using the OpenOrca dataset. 7b 17 Tags Updated 2 years ago",
      "name": "Mistral OpenOrca is a 7 billion parameter model, fine-tuned on top of the Mistral 7B model using the OpenOrca dataset.",
      "description": "Readme Mistral OpenOrca is a 7 billion parameter model, fine-tuned on top of the Mistral 7B model using the OpenOrca dataset. HuggingFace Leaderboard evals place this model as leader for all models smaller than 30B at the release time, outperforming all other 7B and 13B models. Usage CLI ollama run mistral-openorca \"Why is the sky blue?\" API curl -X POST http://localhost:11434/api/generate -d '{ \"model\": \"mistral-openorca\", \"prompt\": \"Why is the sky blue?\" }' More information HuggingFace repo Write Preview <img src=\"https://github.com/jmorganca/ollama/assets/3325447/e2ab7f70-f708-4b59-83af-6cf48f7115b5\" width=\"360\" /> Mistral OpenOrca is a 7 billion parameter model, fine-tuned on top of the Mistral 7B model using the OpenOrca dataset. HuggingFace Leaderboard evals place this model as leader for all models smaller than 30B at the release time, outperforming all other 7B and 13B models. ## Usage ### CLI ``` ollama run mistral-openorca \"Why is the sky blue?\" ``` ### API ``` curl -X POST http://localhost:11434/api/generate -d '{ \"model\": \"mistral-openorca\", \"prompt\": \"Why is the sky blue?\" }' ``` ## More information * [HuggingFace repo](https://huggingface.co/Open-Orca/Mistral-7B-OpenOrca) Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "mistral-openorca:latest",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral-openorca:7b",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral-openorca:7b-q2_K",
          "size_text": "3.1GB",
          "size_bytes": 3328599654,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral-openorca:7b-q3_K_S",
          "size_text": "3.2GB",
          "size_bytes": 3435973836,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral-openorca:7b-q3_K_M",
          "size_text": "3.5GB",
          "size_bytes": 3758096384,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral-openorca:7b-q3_K_L",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral-openorca:7b-q4_0",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral-openorca:7b-q4_1",
          "size_text": "4.6GB",
          "size_bytes": 4939212390,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral-openorca:7b-q4_K_S",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral-openorca:7b-q4_K_M",
          "size_text": "4.4GB",
          "size_bytes": 4724464025,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral-openorca:7b-q5_0",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral-openorca:7b-q5_1",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral-openorca:7b-q5_K_S",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral-openorca:7b-q5_K_M",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral-openorca:7b-q6_K",
          "size_text": "5.9GB",
          "size_bytes": 6335076761,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral-openorca:7b-q8_0",
          "size_text": "7.7GB",
          "size_bytes": 8267812044,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral-openorca:7b-fp16",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "32K",
          "input": "Text"
        }
      ],
      "tags_count": 17
    },
    {
      "slug": "openhermes",
      "capabilities": [],
      "pulls": 189100,
      "pulls_text": "189.1K \n  Downloads",
      "blurb": "openhermes OpenHermes 2.5 is a 7B model fine-tuned by Teknium on Mistral with fully open datasets. 35 Tags Updated 1 year ago",
      "name": "OpenHermes 2.5 is a 7B model fine-tuned by Teknium on Mistral with fully open datasets.",
      "description": "Readme Open Hermes 2 a Mistral 7B fine-tuned with fully open datasets. Matching 70B models on benchmarks, this model has strong multi-turn chat skills and system prompt capabilities. In total, the model was trained on 900,000 instructions, and surpasses all previous versions of Nous-Hermes 13B and below. Versions Tag Date Notes v2.5 latest 11/02/2023 Added ~100k examples of Code Instructions v2 10/16/2023 Initial release of Open Hermes 2 Usage CLI ollama run openhermes API Example: curl -X POST http://localhost:11434/api/generate -d '{ \"model\": \"openhermes\", \"prompt\": \"Here is a story about llamas eating grass\" }' References Hugging Face Write Preview <img src=\"https://github.com/jmorganca/ollama/assets/3325447/abc656b5-6d3b-47ef-8d1c-c0445f780168\" width=\"320\" /> Open Hermes 2 a Mistral 7B fine-tuned with fully open datasets. Matching 70B models on benchmarks, this model has strong multi-turn chat skills and system prompt capabilities. In total, the model was trained on 900,000 instructions, and surpasses all previous versions of Nous-Hermes 13B and below. ### Versions | Tag | Date | Notes | | --------------- | ---------- | ----------------------------------------- | | `v2.5` `latest` | 11/02/2023 | Added ~100k examples of Code Instructions | | `v2` | 10/16/2023 | Initial release of Open Hermes 2 | ## Usage ### CLI ``` ollama run openhermes ``` ### API Example: ```bash curl -X POST http://localhost:11434/api/generate -d '{ \"model\": \"openhermes\", \"prompt\": \"Here is a story about llamas eating grass\" }' ``` ## References [Hugging Face](https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B) Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "openhermes:latest",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "openhermes:v2",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "openhermes:v2.5",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "openhermes:7b-mistral-v2-q2_K",
          "size_text": "3.1GB",
          "size_bytes": 3328599654,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "openhermes:7b-mistral-v2-q3_K_S",
          "size_text": "3.2GB",
          "size_bytes": 3435973836,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "openhermes:7b-mistral-v2-q3_K_M",
          "size_text": "3.5GB",
          "size_bytes": 3758096384,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "openhermes:7b-mistral-v2-q3_K_L",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "openhermes:7b-mistral-v2-q4_0",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "openhermes:7b-mistral-v2-q4_1",
          "size_text": "4.6GB",
          "size_bytes": 4939212390,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "openhermes:7b-mistral-v2-q4_K_S",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "openhermes:7b-mistral-v2-q4_K_M",
          "size_text": "4.4GB",
          "size_bytes": 4724464025,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "openhermes:7b-mistral-v2-q5_0",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "openhermes:7b-mistral-v2-q5_1",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "openhermes:7b-mistral-v2-q5_K_S",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "openhermes:7b-mistral-v2-q5_K_M",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "openhermes:7b-mistral-v2-q6_K",
          "size_text": "5.9GB",
          "size_bytes": 6335076761,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "openhermes:7b-mistral-v2-q8_0",
          "size_text": "7.7GB",
          "size_bytes": 8267812044,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "openhermes:7b-mistral-v2-fp16",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "openhermes:7b-mistral-v2.5-q2_K",
          "size_text": "3.1GB",
          "size_bytes": 3328599654,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "openhermes:7b-mistral-v2.5-q3_K_S",
          "size_text": "3.2GB",
          "size_bytes": 3435973836,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "openhermes:7b-mistral-v2.5-q3_K_M",
          "size_text": "3.5GB",
          "size_bytes": 3758096384,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "openhermes:7b-mistral-v2.5-q3_K_L",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "openhermes:7b-mistral-v2.5-q4_0",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "openhermes:7b-mistral-v2.5-q4_1",
          "size_text": "4.6GB",
          "size_bytes": 4939212390,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "openhermes:7b-mistral-v2.5-q4_K_S",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "openhermes:7b-mistral-v2.5-q4_K_M",
          "size_text": "4.4GB",
          "size_bytes": 4724464025,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "openhermes:7b-mistral-v2.5-q5_0",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "openhermes:7b-mistral-v2.5-q5_1",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "openhermes:7b-mistral-v2.5-q5_K_S",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "openhermes:7b-mistral-v2.5-q5_K_M",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "openhermes:7b-mistral-v2.5-q6_K",
          "size_text": "5.9GB",
          "size_bytes": 6335076761,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "openhermes:7b-mistral-v2.5-q8_0",
          "size_text": "7.7GB",
          "size_bytes": 8267812044,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "openhermes:7b-mistral-v2.5-fp16",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "openhermes:7b-v2",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "openhermes:7b-v2.5",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        }
      ],
      "tags_count": 35
    },
    {
      "slug": "codegeex4",
      "capabilities": [],
      "pulls": 189000,
      "pulls_text": "189K \n  Downloads",
      "blurb": "codegeex4 A versatile model for AI software development scenarios, including code completion. 9b 17 Tags Updated 1 year ago",
      "name": "A versatile model for AI software development scenarios, including code completion.",
      "description": "Readme Note: this model requires Ollama 0.2 or later. CodeGeeX4 is an open multilingual code generation model continually trained on the GLM-4-9B , significantly enhancing its code generation capabilities. CodeGeeX4-ALL-9B has achieved highly competitive performance on public benchmarks, such as BigCodeBench and NaturalCodeBench. It is currently the most powerful code generation model with less than 10B parameters, even surpassing much larger general-purpose models, achieving the best balance in terms of inference speed and model performance. References GitHub Hugging Face Write Preview ![logo.jpg](https://ollama.com/assets/library/codegeex4/b3a2d858-363f-4a6c-a9c7-056f74bc2910) > Note: this model requires [Ollama 0.2](https://github.com/ollama/ollama/releases/tag/v0.2.0) or later. CodeGeeX4 is an open multilingual code generation model continually trained on the [GLM-4-9B](https://ollama.com/library/glm4), significantly enhancing its code generation capabilities. CodeGeeX4-ALL-9B has achieved highly competitive performance on public benchmarks, such as BigCodeBench and NaturalCodeBench. It is currently the most powerful code generation model with less than 10B parameters, even surpassing much larger general-purpose models, achieving the best balance in terms of inference speed and model performance. # References [GitHub](https://github.com/THUDM/CodeGeeX4) [Hugging Face](https://huggingface.co/THUDM/codegeex4-all-9b) Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "codegeex4:latest",
          "size_text": "5.5GB",
          "size_bytes": 5905580032,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "codegeex4:9b",
          "size_text": "5.5GB",
          "size_bytes": 5905580032,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "codegeex4:9b-all-q2_K",
          "size_text": "4.0GB",
          "size_bytes": 4294967296,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "codegeex4:9b-all-q3_K_S",
          "size_text": "4.6GB",
          "size_bytes": 4939212390,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "codegeex4:9b-all-q3_K_M",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "codegeex4:9b-all-q3_K_L",
          "size_text": "5.3GB",
          "size_bytes": 5690831667,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "codegeex4:9b-all-q4_0",
          "size_text": "5.5GB",
          "size_bytes": 5905580032,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "codegeex4:9b-all-q4_1",
          "size_text": "6.0GB",
          "size_bytes": 6442450944,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "codegeex4:9b-all-q4_K_S",
          "size_text": "5.8GB",
          "size_bytes": 6227702579,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "codegeex4:9b-all-q4_K_M",
          "size_text": "6.3GB",
          "size_bytes": 6764573491,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "codegeex4:9b-all-q5_0",
          "size_text": "6.6GB",
          "size_bytes": 7086696038,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "codegeex4:9b-all-q5_1",
          "size_text": "7.1GB",
          "size_bytes": 7623566950,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "codegeex4:9b-all-q5_K_S",
          "size_text": "6.7GB",
          "size_bytes": 7194070220,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "codegeex4:9b-all-q5_K_M",
          "size_text": "7.1GB",
          "size_bytes": 7623566950,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "codegeex4:9b-all-q6_K",
          "size_text": "8.3GB",
          "size_bytes": 8912057139,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "codegeex4:9b-all-q8_0",
          "size_text": "10.0GB",
          "size_bytes": 10737418240,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "codegeex4:9b-all-fp16",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "128K",
          "input": "Text"
        }
      ],
      "tags_count": 17
    },
    {
      "slug": "codeqwen",
      "capabilities": [],
      "pulls": 182300,
      "pulls_text": "182.3K \n  Downloads",
      "blurb": "codeqwen CodeQwen1.5 is a large language model pretrained on a large amount of code data. 7b 30 Tags Updated 1 year ago",
      "name": "CodeQwen1.5 is a large language model pretrained on a large amount of code data.",
      "description": "Readme CodeQwen1.5 is based on Qwen1.5. It is trained on 3 trillion tokens of code data. Its major features include: Strong code generation capabilities and competitive performance across a series of benchmarks Support for long context understanding and generation with a maximum context length of 64K tokens Support for 92 coding languages Excellent performance in Text-to-SQL, fixing bugs and other coding use cases. References Blog Post GitHub HuggingFace Write Preview <img src=\"https://github.com/ollama/ollama/assets/251292/c17cc1ce-9eb9-4658-8b02-b2f15bbaf1c1\" width=\"360\" /> CodeQwen1.5 is based on Qwen1.5. It is trained on 3 trillion tokens of code data. Its major features include: * Strong code generation capabilities and competitive performance across a series of benchmarks * Support for long context understanding and generation with a maximum context length of 64K tokens * Support for 92 coding languages * Excellent performance in Text-to-SQL, fixing bugs and other coding use cases. ## References [Blog Post](https://qwenlm.github.io/blog/codeqwen1.5/) [GitHub](https://github.com/QwenLM/Qwen) [HuggingFace](https://huggingface.co/Qwen) Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "codeqwen:latest",
          "size_text": "4.2GB",
          "size_bytes": 4509715660,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "codeqwen:chat",
          "size_text": "4.2GB",
          "size_bytes": 4509715660,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "codeqwen:code",
          "size_text": "4.2GB",
          "size_bytes": 4509715660,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "codeqwen:v1.5",
          "size_text": "4.2GB",
          "size_bytes": 4509715660,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "codeqwen:7b",
          "size_text": "4.2GB",
          "size_bytes": 4509715660,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "codeqwen:7b-chat",
          "size_text": "4.2GB",
          "size_bytes": 4509715660,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "codeqwen:7b-chat-v1.5-q2_K",
          "size_text": "3.1GB",
          "size_bytes": 3328599654,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "codeqwen:7b-chat-v1.5-q3_K_S",
          "size_text": "3.5GB",
          "size_bytes": 3758096384,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "codeqwen:7b-chat-v1.5-q3_K_M",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "codeqwen:7b-chat-v1.5-q3_K_L",
          "size_text": "4.0GB",
          "size_bytes": 4294967296,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "codeqwen:7b-chat-v1.5-q4_0",
          "size_text": "4.2GB",
          "size_bytes": 4509715660,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "codeqwen:7b-chat-v1.5-q4_1",
          "size_text": "4.6GB",
          "size_bytes": 4939212390,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "codeqwen:7b-chat-v1.5-q4_K_S",
          "size_text": "4.4GB",
          "size_bytes": 4724464025,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "codeqwen:7b-chat-v1.5-q4_K_M",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "codeqwen:7b-chat-v1.5-q5_0",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "codeqwen:7b-chat-v1.5-q5_1",
          "size_text": "5.5GB",
          "size_bytes": 5905580032,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "codeqwen:7b-chat-v1.5-q5_K_S",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "codeqwen:7b-chat-v1.5-q5_K_M",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "codeqwen:7b-chat-v1.5-q6_K",
          "size_text": "6.4GB",
          "size_bytes": 6871947673,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "codeqwen:7b-chat-v1.5-q8_0",
          "size_text": "7.7GB",
          "size_bytes": 8267812044,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "codeqwen:7b-chat-v1.5-fp16",
          "size_text": "15GB",
          "size_bytes": 16106127360,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "codeqwen:7b-code",
          "size_text": "4.2GB",
          "size_bytes": 4509715660,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "codeqwen:7b-code-v1.5-q4_0",
          "size_text": "4.2GB",
          "size_bytes": 4509715660,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "codeqwen:7b-code-v1.5-q4_1",
          "size_text": "4.6GB",
          "size_bytes": 4939212390,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "codeqwen:7b-code-v1.5-q5_0",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "codeqwen:7b-code-v1.5-q5_1",
          "size_text": "5.5GB",
          "size_bytes": 5905580032,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "codeqwen:7b-code-v1.5-q8_0",
          "size_text": "7.7GB",
          "size_bytes": 8267812044,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "codeqwen:7b-code-v1.5-fp16",
          "size_text": "15GB",
          "size_bytes": 16106127360,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "codeqwen:v1.5-chat",
          "size_text": "4.2GB",
          "size_bytes": 4509715660,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "codeqwen:v1.5-code",
          "size_text": "4.2GB",
          "size_bytes": 4509715660,
          "context": "64K",
          "input": "Text"
        }
      ],
      "tags_count": 30
    },
    {
      "slug": "snowflake-arctic-embed2",
      "capabilities": [
        "embedding"
      ],
      "pulls": 179900,
      "pulls_text": "179.9K \n  Downloads",
      "blurb": "snowflake-arctic-embed2 Snowflake's frontier model. Arctic Embed 2.0 adds multilingual support without sacrificing English performance or scalability. 568m 3 Tags Updated 11 months ago",
      "name": "Snowflake's frontier embedding model. Arctic Embed 2.0 adds multilingual support without sacrificing English performance or scalability.",
      "description": "Readme Snowflake is excited to announce the release of Arctic Embed 2.0, the next iteration of our frontier embedding models, which now empower multilingual search. While our previous releases have been well received by our customers, partners and the open source community, leading to millions of downloads, we have consistently received one request: Can you make this model multilingual? Arctic Embed 2.0 builds on the robust foundation of our previous releases, adding multilingual support without sacrificing English performance or scalability, to address the needs of an even broader user base that spans a wide range of languages and applications. Figure 1. Single-vector dense retrieval performance of open source multilingual embedding models with fewer than 1B parameters. Scores are average nDCG@10 on MTEB Retrieval and the subset of CLEF (ELRA, 2006) covering English, French, Spanish, Italian and German. The diverse and powerful feature set of Arctic Embed 2.0 Enterprise-ready throughput and efficiency: The Arctic Embed 2.0 models are built for large-scale enterprise demands. Even our “large” model weighs in well under 1B parameters and delivers fast, high-throughput embedding capabilities. Based on internal testing, it easily handles more than 100 documents per second (on average) on NVIDIA A10 GPUs and achieves sub-10ms query embedding latency, enabling practical deployment on budget-friendly hardware. Uncompromising quality for English and non-English retrieval: Despite their compact sizes, both Arctic Embed 2.0 models achieve impressive NDCG@10 scores across a variety of English and non-English benchmark data sets, demonstrating a capability to generalize well even to languages not included in the training recipe. These impressive benchmark scores position Arctic Embed 2.0 as a leader among frontier retrieval models. Enabling scalable retrieval through Matryoshka Representation Learning (MRL): The Arctic Embed 2.0 release includes the same quantization-friendly ",
      "variants": [
        {
          "tag": "snowflake-arctic-embed2:latest",
          "size_text": "1.2GB",
          "size_bytes": 1288490188,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "snowflake-arctic-embed2:568m",
          "size_text": "1.2GB",
          "size_bytes": 1288490188,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "snowflake-arctic-embed2:568m-l-fp16",
          "size_text": "1.2GB",
          "size_bytes": 1288490188,
          "context": "8K",
          "input": "Text"
        }
      ],
      "tags_count": 3
    },
    {
      "slug": "qwen2-math",
      "capabilities": [],
      "pulls": 175600,
      "pulls_text": "175.6K \n  Downloads",
      "blurb": "qwen2-math Qwen2 Math is a series of specialized math language models built upon the Qwen2 LLMs, which significantly outperforms the mathematical capabilities of open-source models and even closed-source models (e.g., GPT4o). 1.5b 7b 72b 52 Tags Updated 1 year ago",
      "name": "Qwen2 Math is a series of specialized math language models built upon the Qwen2 LLMs, which significantly outperforms the mathematical capabilities of open-source models and even closed-source models (e.g., GPT4o).",
      "description": "Readme Qwen2 Math features 3 model sizes ( 1.5B , 7B , and 72B ) each with a base model, for completion, few-shot inference, and fine-tuning, and an instruct model for chatting. References: Qwen 2 Math Blog Post GitHub Repository Hugging Face Write Preview <img src=\"https://ollama.com/assets/library/qwen2-math/c0159f63-bf4d-4b0d-9b92-ea44c48d34c1\" width=\"380\" /> **Qwen2 Math** features 3 model sizes (**1.5B**, **7B**, and **72B**) each with a **base** model, for completion, few-shot inference, and fine-tuning, and an **instruct** model for chatting. ## References: - [Qwen 2 Math Blog Post](https://qwenlm.github.io/blog/qwen2-math/) - [GitHub Repository](https://github.com/QwenLM/Qwen2-Math) - [Hugging Face](https://huggingface.co/collections/Qwen/qwen2-math-66b4c9e072eda65b5ec7534d) Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "qwen2-math:latest",
          "size_text": "4.4GB",
          "size_bytes": 4724464025,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "qwen2-math:1.5b",
          "size_text": "935MB",
          "size_bytes": 980418560,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "qwen2-math:7b",
          "size_text": "4.4GB",
          "size_bytes": 4724464025,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "qwen2-math:72b",
          "size_text": "41GB",
          "size_bytes": 44023414784,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "qwen2-math:1.5b-instruct",
          "size_text": "935MB",
          "size_bytes": 980418560,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "qwen2-math:1.5b-instruct-q2_K",
          "size_text": "676MB",
          "size_bytes": 708837376,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "qwen2-math:1.5b-instruct-q3_K_S",
          "size_text": "761MB",
          "size_bytes": 797966336,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "qwen2-math:1.5b-instruct-q3_K_M",
          "size_text": "824MB",
          "size_bytes": 864026624,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "qwen2-math:1.5b-instruct-q3_K_L",
          "size_text": "880MB",
          "size_bytes": 922746880,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "qwen2-math:1.5b-instruct-q4_0",
          "size_text": "935MB",
          "size_bytes": 980418560,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "qwen2-math:1.5b-instruct-q4_1",
          "size_text": "1.0GB",
          "size_bytes": 1073741824,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "qwen2-math:1.5b-instruct-q4_K_S",
          "size_text": "940MB",
          "size_bytes": 985661440,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "qwen2-math:1.5b-instruct-q4_K_M",
          "size_text": "986MB",
          "size_bytes": 1033895936,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "qwen2-math:1.5b-instruct-q5_0",
          "size_text": "1.1GB",
          "size_bytes": 1181116006,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "qwen2-math:1.5b-instruct-q5_1",
          "size_text": "1.2GB",
          "size_bytes": 1288490188,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "qwen2-math:1.5b-instruct-q5_K_S",
          "size_text": "1.1GB",
          "size_bytes": 1181116006,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "qwen2-math:1.5b-instruct-q5_K_M",
          "size_text": "1.1GB",
          "size_bytes": 1181116006,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "qwen2-math:1.5b-instruct-q6_K",
          "size_text": "1.3GB",
          "size_bytes": 1395864371,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "qwen2-math:1.5b-instruct-q8_0",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "qwen2-math:1.5b-instruct-fp16",
          "size_text": "3.1GB",
          "size_bytes": 3328599654,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "qwen2-math:7b-instruct",
          "size_text": "4.4GB",
          "size_bytes": 4724464025,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "qwen2-math:7b-instruct-q2_K",
          "size_text": "3.0GB",
          "size_bytes": 3221225472,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "qwen2-math:7b-instruct-q3_K_S",
          "size_text": "3.5GB",
          "size_bytes": 3758096384,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "qwen2-math:7b-instruct-q3_K_M",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "qwen2-math:7b-instruct-q3_K_L",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "qwen2-math:7b-instruct-q4_0",
          "size_text": "4.4GB",
          "size_bytes": 4724464025,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "qwen2-math:7b-instruct-q4_1",
          "size_text": "4.9GB",
          "size_bytes": 5261334937,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "qwen2-math:7b-instruct-q4_K_S",
          "size_text": "4.5GB",
          "size_bytes": 4831838208,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "qwen2-math:7b-instruct-q4_K_M",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "qwen2-math:7b-instruct-q5_0",
          "size_text": "5.3GB",
          "size_bytes": 5690831667,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "qwen2-math:7b-instruct-q5_1",
          "size_text": "5.8GB",
          "size_bytes": 6227702579,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "qwen2-math:7b-instruct-q5_K_S",
          "size_text": "5.3GB",
          "size_bytes": 5690831667,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "qwen2-math:7b-instruct-q5_K_M",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "qwen2-math:7b-instruct-q6_K",
          "size_text": "6.3GB",
          "size_bytes": 6764573491,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "qwen2-math:7b-instruct-q8_0",
          "size_text": "8.1GB",
          "size_bytes": 8697308774,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "qwen2-math:7b-instruct-fp16",
          "size_text": "15GB",
          "size_bytes": 16106127360,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "qwen2-math:72b-instruct",
          "size_text": "41GB",
          "size_bytes": 44023414784,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "qwen2-math:72b-instruct-q2_K",
          "size_text": "30GB",
          "size_bytes": 32212254720,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "qwen2-math:72b-instruct-q3_K_S",
          "size_text": "34GB",
          "size_bytes": 36507222016,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "qwen2-math:72b-instruct-q3_K_M",
          "size_text": "38GB",
          "size_bytes": 40802189312,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "qwen2-math:72b-instruct-q3_K_L",
          "size_text": "40GB",
          "size_bytes": 42949672960,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "qwen2-math:72b-instruct-q4_0",
          "size_text": "41GB",
          "size_bytes": 44023414784,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "qwen2-math:72b-instruct-q4_1",
          "size_text": "46GB",
          "size_bytes": 49392123904,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "qwen2-math:72b-instruct-q4_K_S",
          "size_text": "44GB",
          "size_bytes": 47244640256,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "qwen2-math:72b-instruct-q4_K_M",
          "size_text": "47GB",
          "size_bytes": 50465865728,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "qwen2-math:72b-instruct-q5_0",
          "size_text": "50GB",
          "size_bytes": 53687091200,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "qwen2-math:72b-instruct-q5_1",
          "size_text": "55GB",
          "size_bytes": 59055800320,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "qwen2-math:72b-instruct-q5_K_S",
          "size_text": "51GB",
          "size_bytes": 54760833024,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "qwen2-math:72b-instruct-q5_K_M",
          "size_text": "54GB",
          "size_bytes": 57982058496,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "qwen2-math:72b-instruct-q6_K",
          "size_text": "64GB",
          "size_bytes": 68719476736,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "qwen2-math:72b-instruct-q8_0",
          "size_text": "77GB",
          "size_bytes": 82678120448,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "qwen2-math:72b-instruct-fp16",
          "size_text": "145GB",
          "size_bytes": 155692564480,
          "context": "4K",
          "input": "Text"
        }
      ],
      "tags_count": 52
    },
    {
      "slug": "llama2-chinese",
      "capabilities": [],
      "pulls": 168900,
      "pulls_text": "168.9K \n  Downloads",
      "blurb": "llama2-chinese Llama 2 based model fine tuned to improve Chinese dialogue ability. 7b 13b 35 Tags Updated 2 years ago",
      "name": "Llama 2 based model fine tuned to improve Chinese dialogue ability.",
      "description": "Readme Llama 2 对话中文微调参数模型 这个模型是基于 Meta Platform, Inc. 所发布的 Llama 2 Chat 开源模型来进行微调。根据Meta，Llama 2 的训练数据达到了两万亿个token，上下文长度也提升到4096。对话上也是使用100万人类标记的数据微调。 由于 Llama 2 本身的中文对齐比较弱，开发者采用了中文指令集来进行微调，使其具备较强的中文对话能力。目前这个中文微调参数模型总共发布了 7B，13B两种参数大小。 Llama 2 chat chinese fine-tuned model This model is fine-tuned based on Meta Platform’s Llama 2 Chat open source model. According to Meta, Llama 2 is trained on 2 trillion tokens, and the context length is increased to 4096. The chat model is fine-tuned using 1 million human labeled data. Since the Chinese alignment of Llama 2 itself is relatively weak, the developer, adopted a Chinese instruction set for fine-tuning to improve the Chinese dialogue ability. The Chinese fine-tuned models are available in 7B and 13B parameter sizes. CLI Open the terminal and run ollama run llama2-chinese API Run the model curl -X POST http://localhost:11434/api/generate -d '{ \"model\": \"llama2-chinese:7b-chat-q4_0\", \"prompt\":\"为什么天空是蓝色的\" }' Memory requirements 7b models generally require at least 8GB of RAM 13b models generally require at least 16GB of RAM Reference FlagAlpha FlagAlpha Write Preview Llama 2 对话中文微调参数模型 这个模型是基于 Meta Platform, Inc. 所发布的 Llama 2 Chat 开源模型来进行微调。根据Meta，Llama 2 的训练数据达到了两万亿个token，上下文长度也提升到4096。对话上也是使用100万人类标记的数据微调。 由于 Llama 2 本身的中文对齐比较弱，开发者采用了中文指令集来进行微调，使其具备较强的中文对话能力。目前这个中文微调参数模型总共发布了 7B，13B两种参数大小。 Llama 2 chat chinese fine-tuned model This model is fine-tuned based on Meta Platform's Llama 2 Chat open source model. According to Meta, Llama 2 is trained on 2 trillion tokens, and the context length is increased to 4096. The chat model is fine-tuned using 1 million human labeled data. Since the Chinese alignment of Llama 2 itself is relatively weak, the developer, adopted a Chinese instruction set for fine-tuning to improve the Chinese dialogue ability. The Chinese fine-tuned models are available in 7B and 13B parameter sizes. ### CLI Open the terminal and run `ollama run llama2-chinese` ### API Run the model ```bash curl -X POST h",
      "variants": [
        {
          "tag": "llama2-chinese:latest",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2-chinese:7b",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2-chinese:13b",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2-chinese:7b-chat",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2-chinese:7b-chat-q2_K",
          "size_text": "2.8GB",
          "size_bytes": 3006477107,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2-chinese:7b-chat-q3_K_S",
          "size_text": "2.9GB",
          "size_bytes": 3113851289,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2-chinese:7b-chat-q3_K_M",
          "size_text": "3.3GB",
          "size_bytes": 3543348019,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2-chinese:7b-chat-q3_K_L",
          "size_text": "3.6GB",
          "size_bytes": 3865470566,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2-chinese:7b-chat-q4_0",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2-chinese:7b-chat-q4_1",
          "size_text": "4.2GB",
          "size_bytes": 4509715660,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2-chinese:7b-chat-q4_K_S",
          "size_text": "3.9GB",
          "size_bytes": 4187593113,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2-chinese:7b-chat-q4_K_M",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2-chinese:7b-chat-q5_0",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2-chinese:7b-chat-q5_1",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2-chinese:7b-chat-q5_K_S",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2-chinese:7b-chat-q5_K_M",
          "size_text": "4.8GB",
          "size_bytes": 5153960755,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2-chinese:7b-chat-q6_K",
          "size_text": "5.5GB",
          "size_bytes": 5905580032,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2-chinese:7b-chat-q8_0",
          "size_text": "7.2GB",
          "size_bytes": 7730941132,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2-chinese:7b-chat-fp16",
          "size_text": "13GB",
          "size_bytes": 13958643712,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2-chinese:13b-chat",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2-chinese:13b-chat-q2_K",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2-chinese:13b-chat-q3_K_S",
          "size_text": "5.7GB",
          "size_bytes": 6120328396,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2-chinese:13b-chat-q3_K_M",
          "size_text": "6.3GB",
          "size_bytes": 6764573491,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2-chinese:13b-chat-q3_K_L",
          "size_text": "6.9GB",
          "size_bytes": 7408818585,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2-chinese:13b-chat-q4_0",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2-chinese:13b-chat-q4_1",
          "size_text": "8.2GB",
          "size_bytes": 8804682956,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2-chinese:13b-chat-q4_K_S",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2-chinese:13b-chat-q4_K_M",
          "size_text": "7.9GB",
          "size_bytes": 8482560409,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2-chinese:13b-chat-q5_0",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2-chinese:13b-chat-q5_1",
          "size_text": "9.8GB",
          "size_bytes": 10522669875,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2-chinese:13b-chat-q5_K_S",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2-chinese:13b-chat-q5_K_M",
          "size_text": "9.2GB",
          "size_bytes": 9878424780,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2-chinese:13b-chat-q6_K",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2-chinese:13b-chat-q8_0",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama2-chinese:13b-chat-fp16",
          "size_text": "26GB",
          "size_bytes": 27917287424,
          "context": "4K",
          "input": "Text"
        }
      ],
      "tags_count": 35
    },
    {
      "slug": "aya",
      "capabilities": [],
      "pulls": 167500,
      "pulls_text": "167.5K \n  Downloads",
      "blurb": "aya Aya 23, released by Cohere, is a new family of state-of-the-art, multilingual models that support 23 languages. 8b 35b 33 Tags Updated 1 year ago",
      "name": "Aya 23, released by Cohere, is a new family of state-of-the-art, multilingual models that support 23 languages.",
      "description": "Readme Aya 23, released by Cohere, is a new family of state-of-the-art, multilingual, generative large language research model (LLM) covering 23 different languages. It is available in 8B and 35B parameter sizes: 8B ollama run aya:8b 35B ollama run aya:35b References Blog Post Aya 23: Open Weight Releases to Further Multilingual Progress paper Hugging Face Write Preview ![image.png](https://ollama.com/assets/library/aya/f55197c8-901d-45f5-ba00-75fb0230f1c9) Aya 23, released by Cohere, is a new family of state-of-the-art, multilingual, generative large language research model (LLM) covering 23 different languages. It is available in 8B and 35B parameter sizes: * 8B `ollama run aya:8b` * 35B `ollama run aya:35b` ![Multilingual Benchmarks](https://ollama.com/assets/library/aya/4d6844fc-f55c-470b-9625-c8b2b7927c35) ![aya win rates vs other models](https://ollama.com/assets/library/aya/ceca785b-8190-4f0b-854a-68d375c22e46) # References [Blog Post](https://cohere.com/blog/aya23) [Aya 23: Open Weight Releases to Further Multilingual Progress paper](https://cohere.com/research/papers/aya-command-23-8b-and-35b-technical-report-2024-05-23) [Hugging Face](https://huggingface.co/collections/CohereForAI/c4ai-aya-23-664f4cda3fa1a30553b221dc) Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "aya:latest",
          "size_text": "4.8GB",
          "size_bytes": 5153960755,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "aya:8b",
          "size_text": "4.8GB",
          "size_bytes": 5153960755,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "aya:35b",
          "size_text": "20GB",
          "size_bytes": 21474836480,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "aya:8b-23",
          "size_text": "4.8GB",
          "size_bytes": 5153960755,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "aya:8b-23-q2_K",
          "size_text": "3.4GB",
          "size_bytes": 3650722201,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "aya:8b-23-q3_K_S",
          "size_text": "3.9GB",
          "size_bytes": 4187593113,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "aya:8b-23-q3_K_M",
          "size_text": "4.2GB",
          "size_bytes": 4509715660,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "aya:8b-23-q3_K_L",
          "size_text": "4.5GB",
          "size_bytes": 4831838208,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "aya:8b-23-q4_0",
          "size_text": "4.8GB",
          "size_bytes": 5153960755,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "aya:8b-23-q4_1",
          "size_text": "5.2GB",
          "size_bytes": 5583457484,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "aya:8b-23-q4_K_S",
          "size_text": "4.8GB",
          "size_bytes": 5153960755,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "aya:8b-23-q4_K_M",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "aya:8b-23-q5_0",
          "size_text": "5.7GB",
          "size_bytes": 6120328396,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "aya:8b-23-q5_1",
          "size_text": "6.1GB",
          "size_bytes": 6549825126,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "aya:8b-23-q5_K_S",
          "size_text": "5.7GB",
          "size_bytes": 6120328396,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "aya:8b-23-q5_K_M",
          "size_text": "5.8GB",
          "size_bytes": 6227702579,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "aya:8b-23-q6_K",
          "size_text": "6.6GB",
          "size_bytes": 7086696038,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "aya:8b-23-q8_0",
          "size_text": "8.5GB",
          "size_bytes": 9126805504,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "aya:35b-23",
          "size_text": "20GB",
          "size_bytes": 21474836480,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "aya:35b-23-q2_K",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "aya:35b-23-q3_K_S",
          "size_text": "16GB",
          "size_bytes": 17179869184,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "aya:35b-23-q3_K_M",
          "size_text": "18GB",
          "size_bytes": 19327352832,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "aya:35b-23-q3_K_L",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "aya:35b-23-q4_0",
          "size_text": "20GB",
          "size_bytes": 21474836480,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "aya:35b-23-q4_1",
          "size_text": "22GB",
          "size_bytes": 23622320128,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "aya:35b-23-q4_K_S",
          "size_text": "20GB",
          "size_bytes": 21474836480,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "aya:35b-23-q4_K_M",
          "size_text": "22GB",
          "size_bytes": 23622320128,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "aya:35b-23-q5_0",
          "size_text": "24GB",
          "size_bytes": 25769803776,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "aya:35b-23-q5_1",
          "size_text": "26GB",
          "size_bytes": 27917287424,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "aya:35b-23-q5_K_S",
          "size_text": "24GB",
          "size_bytes": 25769803776,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "aya:35b-23-q5_K_M",
          "size_text": "25GB",
          "size_bytes": 26843545600,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "aya:35b-23-q6_K",
          "size_text": "29GB",
          "size_bytes": 31138512896,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "aya:35b-23-q8_0",
          "size_text": "37GB",
          "size_bytes": 39728447488,
          "context": "8K",
          "input": "Text"
        }
      ],
      "tags_count": 33
    },
    {
      "slug": "tinydolphin",
      "capabilities": [],
      "pulls": 166200,
      "pulls_text": "166.2K \n  Downloads",
      "blurb": "tinydolphin An experimental 1.1B parameter model trained on the new Dolphin 2.8 dataset by Eric Hartford and based on TinyLlama. 1.1b 18 Tags Updated 1 year ago",
      "name": "An experimental 1.1B parameter model trained on the new Dolphin 2.8 dataset by Eric Hartford and based on TinyLlama.",
      "description": "Readme TinyDolphin is an experimental model from training the TinyLlama model on the popular Dolphin dataset by Eric Hartford. Reference Hugging Face Write Preview <img src=\"https://github.com/jmorganca/ollama/assets/251292/319ed8b3-8401-4923-9a09-ea388bb9a196\" width=\"280\" /> TinyDolphin is an experimental model from training the [TinyLlama](https://ollama.ai/library/tinyllama) model on the popular Dolphin dataset by Eric Hartford. ## Reference [Hugging Face](https://huggingface.co/cognitivecomputations/TinyDolphin-2.8-1.1b) Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "tinydolphin:latest",
          "size_text": "637MB",
          "size_bytes": 667942912,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "tinydolphin:v2.8",
          "size_text": "637MB",
          "size_bytes": 667942912,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "tinydolphin:1.1b",
          "size_text": "637MB",
          "size_bytes": 667942912,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "tinydolphin:1.1b-v2.8-q2_K",
          "size_text": "432MB",
          "size_bytes": 452984832,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "tinydolphin:1.1b-v2.8-q3_K_S",
          "size_text": "499MB",
          "size_bytes": 523239424,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "tinydolphin:1.1b-v2.8-q3_K_M",
          "size_text": "548MB",
          "size_bytes": 574619648,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "tinydolphin:1.1b-v2.8-q3_K_L",
          "size_text": "592MB",
          "size_bytes": 620756992,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "tinydolphin:1.1b-v2.8-q4_0",
          "size_text": "637MB",
          "size_bytes": 667942912,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "tinydolphin:1.1b-v2.8-q4_1",
          "size_text": "701MB",
          "size_bytes": 735051776,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "tinydolphin:1.1b-v2.8-q4_K_S",
          "size_text": "640MB",
          "size_bytes": 671088640,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "tinydolphin:1.1b-v2.8-q4_K_M",
          "size_text": "668MB",
          "size_bytes": 700448768,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "tinydolphin:1.1b-v2.8-q5_0",
          "size_text": "766MB",
          "size_bytes": 803209216,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "tinydolphin:1.1b-v2.8-q5_1",
          "size_text": "831MB",
          "size_bytes": 871366656,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "tinydolphin:1.1b-v2.8-q5_K_S",
          "size_text": "766MB",
          "size_bytes": 803209216,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "tinydolphin:1.1b-v2.8-q5_K_M",
          "size_text": "782MB",
          "size_bytes": 819986432,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "tinydolphin:1.1b-v2.8-q6_K",
          "size_text": "903MB",
          "size_bytes": 946864128,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "tinydolphin:1.1b-v2.8-q8_0",
          "size_text": "1.2GB",
          "size_bytes": 1288490188,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "tinydolphin:1.1b-v2.8-fp16",
          "size_text": "2.2GB",
          "size_bytes": 2362232012,
          "context": "4K",
          "input": "Text"
        }
      ],
      "tags_count": 18
    },
    {
      "slug": "granite3.2",
      "capabilities": [
        "reasoning",
        "thinking",
        "tools"
      ],
      "pulls": 162800,
      "pulls_text": "162.8K \n  Downloads",
      "blurb": "granite3.2 Granite-3.2 is a family of long-context AI models from IBM Granite fine-tuned for capabilities. 2b 8b 9 Tags Updated 8 months ago",
      "name": "Granite-3.2 is a family of long-context AI models from IBM Granite fine-tuned for thinking capabilities.",
      "description": "Readme Granite-3.2 is a family of long-context AI models fine-tuned for thinking capabilities. Built on top of Granite-3.1, it has been trained using a mix of permissively licensed open-source datasets and internally generated synthetic data designed for reasoning tasks. The models allow controllability of its thinking capability, ensuring it is applied only when required. Developers: Granite Team, IBM Website : Granite Docs Release Date : February 26th, 2025 License: Apache 2.0 Sizes The models are available in two parameter sizes, 2B and 8B: ollama run granite3.2:2b ollama run granite3.2:8b Supported Languages: English, German, Spanish, French, Japanese, Portuguese, Arabic, Czech, Italian, Korean, Dutch, and Chinese. However, users may finetune this Granite model for languages beyond these 12 languages. Intended Use: These models are designed to handle general instruction-following tasks and can be integrated into AI assistants across various domains, including business applications. Capabilities Thinking Summarization Text classification Text extraction Question-answering Retrieval Augmented Generation (RAG) Code related tasks Function-calling tasks Multilingual dialog use cases Long-context tasks including long document/meeting summarization, long document QA, etc. Thinking To enable thinking, add a message with \"role\": \"control\" and set \"content\" to \"thinking\" . For example: { \"messages\": [ {\"role\": \"control\", \"content\": \"thinking\"}, {\"role\": \"user\", \"content\": \"How do I get to the airport if my car won't start?\"} ] } References Website Write Preview <img src=\"/assets/library/granite3.2/90c5e567-0004-425c-a17a-1b846c2b5d3d\" width=\"600\" /> Granite-3.2 is a family of long-context AI models fine-tuned for thinking capabilities. Built on top of Granite-3.1, it has been trained using a mix of permissively licensed open-source datasets and internally generated synthetic data designed for reasoning tasks. The models allow controllability of its thinking capability, en",
      "variants": [
        {
          "tag": "granite3.2:latest",
          "size_text": "4.9GB",
          "size_bytes": 5261334937,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "granite3.2:2b",
          "size_text": "1.5GB",
          "size_bytes": 1610612736,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "granite3.2:8b",
          "size_text": "4.9GB",
          "size_bytes": 5261334937,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "granite3.2:2b-instruct-q4_K_M",
          "size_text": "1.5GB",
          "size_bytes": 1610612736,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "granite3.2:2b-instruct-q8_0",
          "size_text": "2.7GB",
          "size_bytes": 2899102924,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "granite3.2:2b-instruct-fp16",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "granite3.2:8b-instruct-q4_K_M",
          "size_text": "4.9GB",
          "size_bytes": 5261334937,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "granite3.2:8b-instruct-q8_0",
          "size_text": "8.7GB",
          "size_bytes": 9341553868,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "granite3.2:8b-instruct-fp16",
          "size_text": "16GB",
          "size_bytes": 17179869184,
          "context": "128K",
          "input": "Text"
        }
      ],
      "tags_count": 9
    },
    {
      "slug": "glm4",
      "capabilities": [
        "reasoning"
      ],
      "pulls": 162300,
      "pulls_text": "162.3K \n  Downloads",
      "blurb": "glm4 A strong multi-lingual general language model with competitive performance to Llama 3. 9b 32 Tags Updated 1 year ago",
      "name": "A strong multi-lingual general language model with competitive performance to Llama 3.",
      "description": "Readme Note: this model requires Ollama 0.2 or later. In the evaluation of data sets in semantics, mathematics, reasoning, code, and knowledge, this model has shown superior performance beyond even Llama 3 8B. This generation of models includes multi-language support, supporting 26 languages including Japanese, Korean, and German. References GitHub Hugging Face Write Preview <img src=\"https://ollama.com/assets/library/glm4/7646ce96-fbec-4694-a3f5-39f9e61f19a6\" width=\"320\" /> > Note: this model requires [Ollama 0.2](https://github.com/ollama/ollama/releases/tag/v0.2.0) or later. In the evaluation of data sets in semantics, mathematics, reasoning, code, and knowledge, this model has shown superior performance beyond even Llama 3 8B. This generation of models includes multi-language support, supporting 26 languages including Japanese, Korean, and German. ## References [GitHub](https://github.com/THUDM/GLM) [Hugging Face](https://huggingface.co/collections/THUDM/glm-4-665fcf188c414b03c2f7e3b7) Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "glm4:latest",
          "size_text": "5.5GB",
          "size_bytes": 5905580032,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "glm4:9b",
          "size_text": "5.5GB",
          "size_bytes": 5905580032,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "glm4:9b-chat-q2_K",
          "size_text": "4.0GB",
          "size_bytes": 4294967296,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "glm4:9b-chat-q3_K_S",
          "size_text": "4.6GB",
          "size_bytes": 4939212390,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "glm4:9b-chat-q3_K_M",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "glm4:9b-chat-q3_K_L",
          "size_text": "5.3GB",
          "size_bytes": 5690831667,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "glm4:9b-chat-q4_0",
          "size_text": "5.5GB",
          "size_bytes": 5905580032,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "glm4:9b-chat-q4_1",
          "size_text": "6.0GB",
          "size_bytes": 6442450944,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "glm4:9b-chat-q4_K_S",
          "size_text": "5.8GB",
          "size_bytes": 6227702579,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "glm4:9b-chat-q4_K_M",
          "size_text": "6.3GB",
          "size_bytes": 6764573491,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "glm4:9b-chat-q5_0",
          "size_text": "6.6GB",
          "size_bytes": 7086696038,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "glm4:9b-chat-q5_1",
          "size_text": "7.1GB",
          "size_bytes": 7623566950,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "glm4:9b-chat-q5_K_S",
          "size_text": "6.7GB",
          "size_bytes": 7194070220,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "glm4:9b-chat-q5_K_M",
          "size_text": "7.1GB",
          "size_bytes": 7623566950,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "glm4:9b-chat-q6_K",
          "size_text": "8.3GB",
          "size_bytes": 8912057139,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "glm4:9b-chat-q8_0",
          "size_text": "10.0GB",
          "size_bytes": 10737418240,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "glm4:9b-chat-fp16",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "glm4:9b-text-q2_K",
          "size_text": "4.0GB",
          "size_bytes": 4294967296,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "glm4:9b-text-q3_K_S",
          "size_text": "4.6GB",
          "size_bytes": 4939212390,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "glm4:9b-text-q3_K_M",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "glm4:9b-text-q3_K_L",
          "size_text": "5.3GB",
          "size_bytes": 5690831667,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "glm4:9b-text-q4_0",
          "size_text": "5.5GB",
          "size_bytes": 5905580032,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "glm4:9b-text-q4_1",
          "size_text": "6.0GB",
          "size_bytes": 6442450944,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "glm4:9b-text-q4_K_S",
          "size_text": "5.8GB",
          "size_bytes": 6227702579,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "glm4:9b-text-q4_K_M",
          "size_text": "6.3GB",
          "size_bytes": 6764573491,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "glm4:9b-text-q5_0",
          "size_text": "6.6GB",
          "size_bytes": 7086696038,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "glm4:9b-text-q5_1",
          "size_text": "7.1GB",
          "size_bytes": 7623566950,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "glm4:9b-text-q5_K_S",
          "size_text": "6.7GB",
          "size_bytes": 7194070220,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "glm4:9b-text-q5_K_M",
          "size_text": "7.1GB",
          "size_bytes": 7623566950,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "glm4:9b-text-q6_K",
          "size_text": "8.3GB",
          "size_bytes": 8912057139,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "glm4:9b-text-q8_0",
          "size_text": "10.0GB",
          "size_bytes": 10737418240,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "glm4:9b-text-fp16",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "8K",
          "input": "Text"
        }
      ],
      "tags_count": 32
    },
    {
      "slug": "stable-code",
      "capabilities": [
        "embedding"
      ],
      "pulls": 158200,
      "pulls_text": "158.2K \n  Downloads",
      "blurb": "stable-code Stable Code 3B is a coding model with instruct and code completion variants on par with models such as Code Llama 7B that are 2.5x larger. 3b 36 Tags Updated 1 year ago",
      "name": "Stable Code 3B is a coding model with instruct and code completion variants on par with models such as Code Llama 7B that are 2.5x larger.",
      "description": "Readme Stable Code 3B is a 3 billion parameter Large Language Model (LLM), allowing accurate and responsive code completion at a level on par with models such as Code Llama 7b that are 2.5x larger. Key Features NEW instruct model ollama run stable-code Fill in Middle Capability (FIM) Supports Long Context, trained with Sequences upto 16,384 Model Size Python C++ Javascript Java PHP Rust Stable Code 3B 32.4% 30.9% 32.1% 32.1% 24.2% 23.0% CodeLLama 7B 30.0% 28.2% 32.5% 31.1% 25.7% 26.3% Deepseek Coder 1.3B 28.6% 29.2% 28.7% 29.0% 23.6% 18.5% Wizard Coder 3B 31.6% 25.6% 26.2% 25.8% 25.3% 20.4% StarCoder 3B 21.6% 19.8% 21.5% 20.5% 19.0% 16.9% Replit Code V1.5 3B 23.0% 25.9% 26.2% 23.6% 23.2% 21.5% Deci Coder 1B 19.1% 6.8% 18.4% 16.7% 2.1% 1.7% Model Details Developed by : Stability AI Model type : stable-code models are auto-regressive language models based on the transformer decoder architecture. Language(s) : English, Code Contact : For questions and comments about the model, please email lm@stability.ai Model Architecture The model is a decoder-only transformer similar to the LLaMA ( Touvron et al., 2023 ) architecture with the following modifications: Parameters Hidden Size Layers Heads Sequence Length 2,796,431,360 2560 32 32 16384 Position Embeddings : Rotary Position Embeddings ( Su et al., 2021 ) applied to the first 25% of head embedding dimensions for improved throughput following Black et al. (2022) . Tokenizer : We use a modified version of the GPTNeoX Tokenizer. NeoX . We add special tokens to train for Fill in the Middle (FIM) capabilities like <FIM_PREFIX> and <FIM_SUFFIX> along with other special tokens. Training Training Dataset The dataset is comprised of a filtered mixture of open-source large-scale datasets available on the HuggingFace Hub : Falcon RefinedWeb extract ( Penedo et al., 2023 ), along with CommitPackFT and Github Issues (BigCode., 2023), and StarCoder ( Li et al., 2023 ). We further supplement our training with data from mathematical dom",
      "variants": [
        {
          "tag": "stable-code:latest",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "stable-code:code",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "stable-code:instruct",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "stable-code:3b",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "stable-code:3b-code",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "stable-code:3b-code-q2_K",
          "size_text": "1.1GB",
          "size_bytes": 1181116006,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "stable-code:3b-code-q3_K_S",
          "size_text": "1.3GB",
          "size_bytes": 1395864371,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "stable-code:3b-code-q3_K_M",
          "size_text": "1.4GB",
          "size_bytes": 1503238553,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "stable-code:3b-code-q3_K_L",
          "size_text": "1.5GB",
          "size_bytes": 1610612736,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "stable-code:3b-code-q4_0",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "stable-code:3b-code-q4_1",
          "size_text": "1.8GB",
          "size_bytes": 1932735283,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "stable-code:3b-code-q4_K_S",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "stable-code:3b-code-q4_K_M",
          "size_text": "1.7GB",
          "size_bytes": 1825361100,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "stable-code:3b-code-q5_0",
          "size_text": "1.9GB",
          "size_bytes": 2040109465,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "stable-code:3b-code-q5_1",
          "size_text": "2.1GB",
          "size_bytes": 2254857830,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "stable-code:3b-code-q5_K_S",
          "size_text": "1.9GB",
          "size_bytes": 2040109465,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "stable-code:3b-code-q5_K_M",
          "size_text": "2.0GB",
          "size_bytes": 2147483648,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "stable-code:3b-code-q6_K",
          "size_text": "2.3GB",
          "size_bytes": 2469606195,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "stable-code:3b-code-q8_0",
          "size_text": "3.0GB",
          "size_bytes": 3221225472,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "stable-code:3b-code-fp16",
          "size_text": "5.6GB",
          "size_bytes": 6012954214,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "stable-code:3b-instruct",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "stable-code:3b-instruct-q2_K",
          "size_text": "1.1GB",
          "size_bytes": 1181116006,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "stable-code:3b-instruct-q3_K_S",
          "size_text": "1.3GB",
          "size_bytes": 1395864371,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "stable-code:3b-instruct-q3_K_M",
          "size_text": "1.4GB",
          "size_bytes": 1503238553,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "stable-code:3b-instruct-q3_K_L",
          "size_text": "1.5GB",
          "size_bytes": 1610612736,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "stable-code:3b-instruct-q4_0",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "stable-code:3b-instruct-q4_1",
          "size_text": "1.8GB",
          "size_bytes": 1932735283,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "stable-code:3b-instruct-q4_K_S",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "stable-code:3b-instruct-q4_K_M",
          "size_text": "1.7GB",
          "size_bytes": 1825361100,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "stable-code:3b-instruct-q5_0",
          "size_text": "1.9GB",
          "size_bytes": 2040109465,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "stable-code:3b-instruct-q5_1",
          "size_text": "2.1GB",
          "size_bytes": 2254857830,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "stable-code:3b-instruct-q5_K_S",
          "size_text": "1.9GB",
          "size_bytes": 2040109465,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "stable-code:3b-instruct-q5_K_M",
          "size_text": "2.0GB",
          "size_bytes": 2147483648,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "stable-code:3b-instruct-q6_K",
          "size_text": "2.3GB",
          "size_bytes": 2469606195,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "stable-code:3b-instruct-q8_0",
          "size_text": "3.0GB",
          "size_bytes": 3221225472,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "stable-code:3b-instruct-fp16",
          "size_text": "5.6GB",
          "size_bytes": 6012954214,
          "context": "16K",
          "input": "Text"
        }
      ],
      "tags_count": 36
    },
    {
      "slug": "nous-hermes2",
      "capabilities": [],
      "pulls": 152800,
      "pulls_text": "152.8K \n  Downloads",
      "blurb": "nous-hermes2 The powerful family of models by Nous Research that excels at scientific discussion and coding tasks. 10.7b 34b 33 Tags Updated 1 year ago",
      "name": "The powerful family of models by Nous Research that excels at scientific discussion and coding tasks.",
      "description": "Readme Nous Hermes 2 is a new iteration of the Nous Hermes model. This model was trained on 1,000,000 entries of primarily GPT-4 generated data, as well as other high quality data from open datasets across the AI landscape. It is the most powerful Nous Hermes model to date, and surpasses many of the most popular models in the world in the GPT4All, AGIEval, BigBench and other benchmarks. Versions 10.7b latest 01/01/2024 A 10.7b model based on Solar . A major improvement across the board on benchmarks compared to the base Solar 10.7B model, and comes close to approaching the 34B Yi model 34b 12/25/2023 The original Nous Hermes 2 34B model based on Yi References HuggingFace Write Preview <img src=\"https://github.com/jmorganca/ollama/assets/251292/4cf930ce-95f8-4a01-ad49-48db41ada870\" width=\"300\" /> Nous Hermes 2 is a new iteration of the [Nous Hermes](https://ollama.ai/library/nous-hermes) model. This model was trained on 1,000,000 entries of primarily GPT-4 generated data, as well as other high quality data from open datasets across the AI landscape. It is the most powerful Nous Hermes model to date, and surpasses many of the most popular models in the world in the GPT4All, AGIEval, BigBench and other benchmarks. ## Versions | ---------------- | ---------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | | `10.7b` `latest` | 01/01/2024 | A 10.7b model based on [Solar](https://ollama.ai/library/solar). A major improvement across the board on benchmarks compared to the base Solar 10.7B model, and comes close to approaching the 34B Yi model | | `34b` | 12/25/2023 | The original Nous Hermes 2 34B model based on [Yi](https://ollama.ai/library/yi) | ## References [HuggingFace](https://huggingface.co/NousResearch/Nous-Hermes-2-Yi-34B) Paste, drop or click to upload images (.pn",
      "variants": [
        {
          "tag": "nous-hermes2:latest",
          "size_text": "6.1GB",
          "size_bytes": 6549825126,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes2:10.7b",
          "size_text": "6.1GB",
          "size_bytes": 6549825126,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes2:34b",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes2:10.7b-solar-q2_K",
          "size_text": "4.5GB",
          "size_bytes": 4831838208,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes2:10.7b-solar-q3_K_S",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes2:10.7b-solar-q3_K_M",
          "size_text": "5.2GB",
          "size_bytes": 5583457484,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes2:10.7b-solar-q3_K_L",
          "size_text": "5.7GB",
          "size_bytes": 6120328396,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes2:10.7b-solar-q4_0",
          "size_text": "6.1GB",
          "size_bytes": 6549825126,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes2:10.7b-solar-q4_1",
          "size_text": "6.7GB",
          "size_bytes": 7194070220,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes2:10.7b-solar-q4_K_S",
          "size_text": "6.1GB",
          "size_bytes": 6549825126,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes2:10.7b-solar-q4_K_M",
          "size_text": "6.5GB",
          "size_bytes": 6979321856,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes2:10.7b-solar-q5_0",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes2:10.7b-solar-q5_1",
          "size_text": "8.1GB",
          "size_bytes": 8697308774,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes2:10.7b-solar-q5_K_S",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes2:10.7b-solar-q5_K_M",
          "size_text": "7.6GB",
          "size_bytes": 8160437862,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes2:10.7b-solar-q6_K",
          "size_text": "8.8GB",
          "size_bytes": 9448928051,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes2:10.7b-solar-q8_0",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes2:10.7b-solar-fp16",
          "size_text": "21GB",
          "size_bytes": 22548578304,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes2:34b-yi-q2_K",
          "size_text": "15GB",
          "size_bytes": 16106127360,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes2:34b-yi-q3_K_S",
          "size_text": "15GB",
          "size_bytes": 16106127360,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes2:34b-yi-q3_K_M",
          "size_text": "17GB",
          "size_bytes": 18253611008,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes2:34b-yi-q3_K_L",
          "size_text": "18GB",
          "size_bytes": 19327352832,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes2:34b-yi-q4_0",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes2:34b-yi-q4_1",
          "size_text": "22GB",
          "size_bytes": 23622320128,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes2:34b-yi-q4_K_S",
          "size_text": "20GB",
          "size_bytes": 21474836480,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes2:34b-yi-q4_K_M",
          "size_text": "21GB",
          "size_bytes": 22548578304,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes2:34b-yi-q5_0",
          "size_text": "24GB",
          "size_bytes": 25769803776,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes2:34b-yi-q5_1",
          "size_text": "26GB",
          "size_bytes": 27917287424,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes2:34b-yi-q5_K_S",
          "size_text": "24GB",
          "size_bytes": 25769803776,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes2:34b-yi-q5_K_M",
          "size_text": "24GB",
          "size_bytes": 25769803776,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes2:34b-yi-q6_K",
          "size_text": "28GB",
          "size_bytes": 30064771072,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes2:34b-yi-q8_0",
          "size_text": "37GB",
          "size_bytes": 39728447488,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes2:34b-yi-fp16",
          "size_text": "69GB",
          "size_bytes": 74088185856,
          "context": "4K",
          "input": "Text"
        }
      ],
      "tags_count": 33
    },
    {
      "slug": "neural-chat",
      "capabilities": [],
      "pulls": 150800,
      "pulls_text": "150.8K \n  Downloads",
      "blurb": "neural-chat A fine-tuned model based on Mistral with good coverage of domain and language. 7b 50 Tags Updated 1 year ago",
      "name": "A fine-tuned model based on Mistral with good coverage of domain and language.",
      "description": "Readme NeuralChat is a fine-tuned model released by Intel that’s based on Mistral , designed to be used for high-performance chatbot applications. References HuggingFace Write Preview <img src=\"https://github.com/jmorganca/ollama/assets/251292/18ecaefd-f70e-4d16-96b1-937d53e0faa5\" width=\"240\" /> NeuralChat is a fine-tuned model released by Intel that's based on [Mistral](https://ollama.ai/library/mistral), designed to be used for high-performance chatbot applications. ## References [HuggingFace](https://huggingface.co/Intel/neural-chat-7b-v3-1) Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "neural-chat:latest",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "neural-chat:7b",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "neural-chat:7b-v3.1",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "neural-chat:7b-v3.1-q2_K",
          "size_text": "3.1GB",
          "size_bytes": 3328599654,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "neural-chat:7b-v3.1-q3_K_S",
          "size_text": "3.2GB",
          "size_bytes": 3435973836,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "neural-chat:7b-v3.1-q3_K_M",
          "size_text": "3.5GB",
          "size_bytes": 3758096384,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "neural-chat:7b-v3.1-q3_K_L",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "neural-chat:7b-v3.1-q4_0",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "neural-chat:7b-v3.1-q4_1",
          "size_text": "4.6GB",
          "size_bytes": 4939212390,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "neural-chat:7b-v3.1-q4_K_S",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "neural-chat:7b-v3.1-q4_K_M",
          "size_text": "4.4GB",
          "size_bytes": 4724464025,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "neural-chat:7b-v3.1-q5_0",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "neural-chat:7b-v3.1-q5_1",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "neural-chat:7b-v3.1-q5_K_S",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "neural-chat:7b-v3.1-q5_K_M",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "neural-chat:7b-v3.1-q6_K",
          "size_text": "5.9GB",
          "size_bytes": 6335076761,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "neural-chat:7b-v3.1-q8_0",
          "size_text": "7.7GB",
          "size_bytes": 8267812044,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "neural-chat:7b-v3.1-fp16",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "neural-chat:7b-v3.2",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "neural-chat:7b-v3.2-q2_K",
          "size_text": "3.1GB",
          "size_bytes": 3328599654,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "neural-chat:7b-v3.2-q3_K_S",
          "size_text": "3.2GB",
          "size_bytes": 3435973836,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "neural-chat:7b-v3.2-q3_K_M",
          "size_text": "3.5GB",
          "size_bytes": 3758096384,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "neural-chat:7b-v3.2-q3_K_L",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "neural-chat:7b-v3.2-q4_0",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "neural-chat:7b-v3.2-q4_1",
          "size_text": "4.6GB",
          "size_bytes": 4939212390,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "neural-chat:7b-v3.2-q4_K_S",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "neural-chat:7b-v3.2-q4_K_M",
          "size_text": "4.4GB",
          "size_bytes": 4724464025,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "neural-chat:7b-v3.2-q5_0",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "neural-chat:7b-v3.2-q5_1",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "neural-chat:7b-v3.2-q5_K_S",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "neural-chat:7b-v3.2-q5_K_M",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "neural-chat:7b-v3.2-q6_K",
          "size_text": "5.9GB",
          "size_bytes": 6335076761,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "neural-chat:7b-v3.2-q8_0",
          "size_text": "7.7GB",
          "size_bytes": 8267812044,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "neural-chat:7b-v3.2-fp16",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "neural-chat:7b-v3.3",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "neural-chat:7b-v3.3-q2_K",
          "size_text": "3.1GB",
          "size_bytes": 3328599654,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "neural-chat:7b-v3.3-q3_K_S",
          "size_text": "3.2GB",
          "size_bytes": 3435973836,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "neural-chat:7b-v3.3-q3_K_M",
          "size_text": "3.5GB",
          "size_bytes": 3758096384,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "neural-chat:7b-v3.3-q3_K_L",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "neural-chat:7b-v3.3-q4_0",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "neural-chat:7b-v3.3-q4_1",
          "size_text": "4.6GB",
          "size_bytes": 4939212390,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "neural-chat:7b-v3.3-q4_K_S",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "neural-chat:7b-v3.3-q4_K_M",
          "size_text": "4.4GB",
          "size_bytes": 4724464025,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "neural-chat:7b-v3.3-q5_0",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "neural-chat:7b-v3.3-q5_1",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "neural-chat:7b-v3.3-q5_K_S",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "neural-chat:7b-v3.3-q5_K_M",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "neural-chat:7b-v3.3-q6_K",
          "size_text": "5.9GB",
          "size_bytes": 6335076761,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "neural-chat:7b-v3.3-q8_0",
          "size_text": "7.7GB",
          "size_bytes": 8267812044,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "neural-chat:7b-v3.3-fp16",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "32K",
          "input": "Text"
        }
      ],
      "tags_count": 50
    },
    {
      "slug": "wizardcoder",
      "capabilities": [],
      "pulls": 148800,
      "pulls_text": "148.8K \n  Downloads",
      "blurb": "wizardcoder State-of-the-art code generation model 33b 67 Tags Updated 1 year ago",
      "name": "State-of-the-art code generation model",
      "description": "Readme Wizard Coder is a code generation model based on Code Llama . Versions Tag Date Notes 33b 01/042024 A new 33B model trained from Deepseek Coder python 09/7/2023 Initial release in 7B, 13B and 34B sizes based on Code Llama References Hugging Face WizardCoder: Empowering Code Large Language Models with Evol-Instruct Write Preview Wizard Coder is a code generation model based on [Code Llama](https://ollama.ai/library/codellama). ## Versions | Tag | Date | Notes | | -------- | --------- | ---------------------------------------------------------------------------------------------------- | | `33b` | 01/042024 | A new 33B model trained from [Deepseek Coder](https://ollama.ai/library/deepseek-coder/) | | `python` | 09/7/2023 | Initial release in 7B, 13B and 34B sizes based on [Code Llama](https://ollama.ai/library/codellama/) | ## References [Hugging Face](https://huggingface.co/WizardLM/WizardCoder-33B-V1.1) [WizardCoder: Empowering Code Large Language Models with Evol-Instruct](https://arxiv.org/abs/2306.08568 ) Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "wizardcoder:latest",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "wizardcoder:python",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "wizardcoder:33b",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "wizardcoder:7b-python",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "wizardcoder:7b-python-q2_K",
          "size_text": "2.8GB",
          "size_bytes": 3006477107,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "wizardcoder:7b-python-q3_K_S",
          "size_text": "2.9GB",
          "size_bytes": 3113851289,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "wizardcoder:7b-python-q3_K_M",
          "size_text": "3.3GB",
          "size_bytes": 3543348019,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "wizardcoder:7b-python-q3_K_L",
          "size_text": "3.6GB",
          "size_bytes": 3865470566,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "wizardcoder:7b-python-q4_0",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "wizardcoder:7b-python-q4_1",
          "size_text": "4.2GB",
          "size_bytes": 4509715660,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "wizardcoder:7b-python-q4_K_S",
          "size_text": "3.9GB",
          "size_bytes": 4187593113,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "wizardcoder:7b-python-q4_K_M",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "wizardcoder:7b-python-q5_0",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "wizardcoder:7b-python-q5_1",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "wizardcoder:7b-python-q5_K_S",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "wizardcoder:7b-python-q5_K_M",
          "size_text": "4.8GB",
          "size_bytes": 5153960755,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "wizardcoder:7b-python-q6_K",
          "size_text": "5.5GB",
          "size_bytes": 5905580032,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "wizardcoder:7b-python-q8_0",
          "size_text": "7.2GB",
          "size_bytes": 7730941132,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "wizardcoder:7b-python-fp16",
          "size_text": "13GB",
          "size_bytes": 13958643712,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "wizardcoder:13b-python",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "wizardcoder:13b-python-q2_K",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "wizardcoder:13b-python-q3_K_S",
          "size_text": "5.7GB",
          "size_bytes": 6120328396,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "wizardcoder:13b-python-q3_K_M",
          "size_text": "6.3GB",
          "size_bytes": 6764573491,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "wizardcoder:13b-python-q3_K_L",
          "size_text": "6.9GB",
          "size_bytes": 7408818585,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "wizardcoder:13b-python-q4_0",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "wizardcoder:13b-python-q4_1",
          "size_text": "8.2GB",
          "size_bytes": 8804682956,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "wizardcoder:13b-python-q4_K_S",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "wizardcoder:13b-python-q4_K_M",
          "size_text": "7.9GB",
          "size_bytes": 8482560409,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "wizardcoder:13b-python-q5_0",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "wizardcoder:13b-python-q5_1",
          "size_text": "9.8GB",
          "size_bytes": 10522669875,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "wizardcoder:13b-python-q5_K_S",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "wizardcoder:13b-python-q5_K_M",
          "size_text": "9.2GB",
          "size_bytes": 9878424780,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "wizardcoder:13b-python-q6_K",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "wizardcoder:13b-python-q8_0",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "wizardcoder:13b-python-fp16",
          "size_text": "26GB",
          "size_bytes": 27917287424,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "wizardcoder:33b-v1.1",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "wizardcoder:33b-v1.1-q2_K",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "wizardcoder:33b-v1.1-q3_K_S",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "wizardcoder:33b-v1.1-q3_K_M",
          "size_text": "16GB",
          "size_bytes": 17179869184,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "wizardcoder:33b-v1.1-q3_K_L",
          "size_text": "18GB",
          "size_bytes": 19327352832,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "wizardcoder:33b-v1.1-q4_0",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "wizardcoder:33b-v1.1-q4_1",
          "size_text": "21GB",
          "size_bytes": 22548578304,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "wizardcoder:33b-v1.1-q4_K_S",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "wizardcoder:33b-v1.1-q4_K_M",
          "size_text": "20GB",
          "size_bytes": 21474836480,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "wizardcoder:33b-v1.1-q5_0",
          "size_text": "23GB",
          "size_bytes": 24696061952,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "wizardcoder:33b-v1.1-q5_1",
          "size_text": "25GB",
          "size_bytes": 26843545600,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "wizardcoder:33b-v1.1-q5_K_S",
          "size_text": "23GB",
          "size_bytes": 24696061952,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "wizardcoder:33b-v1.1-q5_K_M",
          "size_text": "24GB",
          "size_bytes": 25769803776,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "wizardcoder:33b-v1.1-q6_K",
          "size_text": "27GB",
          "size_bytes": 28991029248,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "wizardcoder:33b-v1.1-q8_0",
          "size_text": "35GB",
          "size_bytes": 37580963840,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "wizardcoder:33b-v1.1-fp16",
          "size_text": "67GB",
          "size_bytes": 71940702208,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "wizardcoder:34b-python",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "wizardcoder:34b-python-q2_K",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "wizardcoder:34b-python-q3_K_S",
          "size_text": "15GB",
          "size_bytes": 16106127360,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "wizardcoder:34b-python-q3_K_M",
          "size_text": "16GB",
          "size_bytes": 17179869184,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "wizardcoder:34b-python-q3_K_L",
          "size_text": "18GB",
          "size_bytes": 19327352832,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "wizardcoder:34b-python-q4_0",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "wizardcoder:34b-python-q4_1",
          "size_text": "21GB",
          "size_bytes": 22548578304,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "wizardcoder:34b-python-q4_K_S",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "wizardcoder:34b-python-q4_K_M",
          "size_text": "20GB",
          "size_bytes": 21474836480,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "wizardcoder:34b-python-q5_0",
          "size_text": "23GB",
          "size_bytes": 24696061952,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "wizardcoder:34b-python-q5_1",
          "size_text": "25GB",
          "size_bytes": 26843545600,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "wizardcoder:34b-python-q5_K_S",
          "size_text": "23GB",
          "size_bytes": 24696061952,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "wizardcoder:34b-python-q5_K_M",
          "size_text": "24GB",
          "size_bytes": 25769803776,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "wizardcoder:34b-python-q6_K",
          "size_text": "28GB",
          "size_bytes": 30064771072,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "wizardcoder:34b-python-q8_0",
          "size_text": "36GB",
          "size_bytes": 38654705664,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "wizardcoder:34b-python-fp16",
          "size_text": "67GB",
          "size_bytes": 71940702208,
          "context": "16K",
          "input": "Text"
        }
      ],
      "tags_count": 67
    },
    {
      "slug": "bakllava",
      "capabilities": [
        "multimodal",
        "vision"
      ],
      "pulls": 145900,
      "pulls_text": "145.9K \n  Downloads",
      "blurb": "bakllava BakLLaVA is a model consisting of the Mistral 7B base model augmented with the LLaVA architecture. 7b 17 Tags Updated 1 year ago",
      "name": "BakLLaVA is a multimodal model consisting of the Mistral 7B base model augmented with the LLaVA architecture.",
      "description": "Readme Note: this model requires Ollama 0.1.15: download it here CLI Usage Run the model: ollama run bakllava Then at the prompt, include the path to your image in the prompt: >>> What's in this image? /Users/jmorgan/Desktop/smile.png The image features a yellow smiley face, which is likely the central focus of the picture. API Usage curl http://localhost:11434/api/generate -d '{ \"model\": \"bakllava\", \"prompt\":\"What is in this picture?\", \"images\": [\"iVBORw0KGgoAAAANSUhEUgAAAG0AAABmCAYAAADBPx+VAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAA3VSURBVHgB7Z27r0zdG8fX743i1bi1ikMoFMQloXRpKFFIqI7LH4BEQ+NWIkjQuSWCRIEoULk0gsK1kCBI0IhrQVT7tz/7zZo888yz1r7MnDl7z5xvsjkzs2fP3uu71nNfa7lkAsm7d++Sffv2JbNmzUqcc8m0adOSzZs3Z+/XES4ZckAWJEGWPiCxjsQNLWmQsWjRIpMseaxcuTKpG/7HP27I8P79e7dq1ars/yL4/v27S0ejqwv+cUOGEGGpKHR37tzJCEpHV9tnT58+dXXCJDdECBE2Ojrqjh071hpNECjx4cMHVycM1Uhbv359B2F79+51586daxN/+pyRkRFXKyRDAqxEp4yMlDDzXG1NPnnyJKkThoK0VFd1ELZu3TrzXKxKfW7dMBQ6bcuWLW2v0VlHjx41z717927ba22U9APcw7Nnz1oGEPeL3m3p2mTAYYnFmMOMXybPPXv2bNIPpFZr1NHn4HMw0KRBjg9NuRw95s8PEcz/6DZELQd/09C9QGq5RsmSRybqkwHGjh07OsJSsYYm3ijPpyHzoiacg35MLdDSIS/O1yM778jOTwYUkKNHWUzUWaOsylE00MyI0fcnOwIdjvtNdW/HZwNLGg+sR1kMepSNJXmIwxBZiG8tDTpEZzKg0GItNsosY8USkxDhD0Rinuiko2gfL/RbiD2LZAjU9zKQJj8RDR0vJBR1/Phx9+PHj9Z7REF4nTZkxzX4LCXHrV271qXkBAPGfP/atWvu/PnzHe4C97F48eIsRLZ9+3a3f/9+87dwP1JxaF7/3r17ba+5l4EcaVo0lj3SBq5kGTJSQmLWMjgYNei2GPT1MuMqGTDEFHzeQSP2wi/jGnkmPJ/nhccs44jvDAxpVcxnq0F6eT8h4ni/iIWpR5lPyA6ETkNXoSukvpJAD3AsXLiwpZs49+fPn5ke4j10TqYvegSfn0OnafC+Tv9ooA/JPkgQysqQNBzagXY55nO/oa1F7qvIPWkRL12WRpMWUvpVDYmxAPehxWSe8ZEXL20sadYIozfmNch4QJPAfeJgW3rNsnzphBKNJM2KKODo1rVOMRYik5ETy3ix4qWNI81qAAirizgMIc+yhTytx0JWZuNI03qsrgWlGtwjoS9XwgUhWGyhUaRZZQNNIEwCiXD16tXcAHUs79co0vSD8rrJCIW98pzvxpAWyyo3HYwqS0+H0BjStClcZJT5coMm6D2LOF8TolGJtK9fvyZpyiC5ePFi9nc/oJU4eiEP0jVoAnHa9wyJycITMP78+eMeP37sXrx44d6+fdt6f82aNdkx1pg9e3Zb5W+RSRE+n+VjksQWifvVaTKFhn5O8my63K8Qabdv33b379/PiAP//vuvW7BggZszZ072/+TJk91YgkafPn166zXB1rQHFvouAWHq9z3SEevSUerqCn2/dDC",
      "variants": [
        {
          "tag": "bakllava:latest",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "bakllava:7b",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "bakllava:7b-v1-q2_K",
          "size_text": "3.7GB",
          "size_bytes": 3972844748,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "bakllava:7b-v1-q3_K_S",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "bakllava:7b-v1-q3_K_M",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "bakllava:7b-v1-q3_K_L",
          "size_text": "4.4GB",
          "size_bytes": 4724464025,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "bakllava:7b-v1-q4_0",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "bakllava:7b-v1-q4_1",
          "size_text": "5.2GB",
          "size_bytes": 5583457484,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "bakllava:7b-v1-q4_K_S",
          "size_text": "4.8GB",
          "size_bytes": 5153960755,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "bakllava:7b-v1-q4_K_M",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "bakllava:7b-v1-q5_0",
          "size_text": "5.6GB",
          "size_bytes": 6012954214,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "bakllava:7b-v1-q5_1",
          "size_text": "6.1GB",
          "size_bytes": 6549825126,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "bakllava:7b-v1-q5_K_S",
          "size_text": "5.6GB",
          "size_bytes": 6012954214,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "bakllava:7b-v1-q5_K_M",
          "size_text": "5.8GB",
          "size_bytes": 6227702579,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "bakllava:7b-v1-q6_K",
          "size_text": "6.6GB",
          "size_bytes": 7086696038,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "bakllava:7b-v1-q8_0",
          "size_text": "8.3GB",
          "size_bytes": 8912057139,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "bakllava:7b-v1-fp16",
          "size_text": "15GB",
          "size_bytes": 16106127360,
          "context": "32K",
          "input": "Text"
        }
      ],
      "tags_count": 17
    },
    {
      "slug": "command-r-plus",
      "capabilities": [
        "tools"
      ],
      "pulls": 145800,
      "pulls_text": "145.8K \n  Downloads",
      "blurb": "command-r-plus Command R+ is a powerful, scalable large language model purpose-built to excel at real-world enterprise use cases. 104b 21 Tags Updated 1 year ago",
      "name": "Command R+ is a powerful, scalable large language model purpose-built to excel at real-world enterprise use cases.",
      "description": "Readme Command R+ is Cohere’s most powerful, scalable large language model (LLM) purpose-built to excel at real-world enterprise use cases. Command R+ balances high efficiency with strong accuracy, enabling businesses to move beyond proof-of-concept, and into production with AI: A 128k-token context window Advanced Retrieval Augmented Generation (RAG) with citation to reduce hallucinations Multilingual coverage in 10 key languages to support global business operations Tool Use to automate sophisticated business processes References Blog Post HuggingFace Write Preview ![Command R+](https://github.com/ollama/ollama/assets/251292/566287ce-99c9-40c1-81d4-3dd354447fd7) Command R+ is Cohere's most powerful, scalable large language model (LLM) purpose-built to excel at real-world enterprise use cases. Command R+ balances high efficiency with strong accuracy, enabling businesses to move beyond proof-of-concept, and into production with AI: - A 128k-token context window - Advanced Retrieval Augmented Generation (RAG) with citation to reduce hallucinations - Multilingual coverage in 10 key languages to support global business operations - Tool Use to automate sophisticated business processes ## References [Blog Post](https://txt.cohere.com/command-r-plus-microsoft-azure/) [HuggingFace](https://huggingface.co/CohereForAI/c4ai-command-r-plus) Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "command-r-plus:latest",
          "size_text": "59GB",
          "size_bytes": 63350767616,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "command-r-plus:104b",
          "size_text": "59GB",
          "size_bytes": 63350767616,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "command-r-plus:104b-08-2024-q2_K",
          "size_text": "39GB",
          "size_bytes": 41875931136,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "command-r-plus:104b-08-2024-q3_K_S",
          "size_text": "46GB",
          "size_bytes": 49392123904,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "command-r-plus:104b-08-2024-q3_K_M",
          "size_text": "51GB",
          "size_bytes": 54760833024,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "command-r-plus:104b-08-2024-q3_K_L",
          "size_text": "55GB",
          "size_bytes": 59055800320,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "command-r-plus:104b-08-2024-q4_0",
          "size_text": "59GB",
          "size_bytes": 63350767616,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "command-r-plus:104b-08-2024-q4_1",
          "size_text": "66GB",
          "size_bytes": 70866960384,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "command-r-plus:104b-08-2024-q4_K_S",
          "size_text": "60GB",
          "size_bytes": 64424509440,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "command-r-plus:104b-08-2024-q4_K_M",
          "size_text": "63GB",
          "size_bytes": 67645734912,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "command-r-plus:104b-08-2024-q5_0",
          "size_text": "72GB",
          "size_bytes": 77309411328,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "command-r-plus:104b-08-2024-q5_1",
          "size_text": "78GB",
          "size_bytes": 83751862272,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "command-r-plus:104b-08-2024-q5_K_S",
          "size_text": "72GB",
          "size_bytes": 77309411328,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "command-r-plus:104b-08-2024-q5_K_M",
          "size_text": "74GB",
          "size_bytes": 79456894976,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "command-r-plus:104b-08-2024-q6_K",
          "size_text": "85GB",
          "size_bytes": 91268055040,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "command-r-plus:104b-08-2024-q8_0",
          "size_text": "110GB",
          "size_bytes": 118111600640,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "command-r-plus:104b-08-2024-fp16",
          "size_text": "208GB",
          "size_bytes": 223338299392,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "command-r-plus:104b-q2_K",
          "size_text": "39GB",
          "size_bytes": 41875931136,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "command-r-plus:104b-q4_0",
          "size_text": "59GB",
          "size_bytes": 63350767616,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "command-r-plus:104b-q8_0",
          "size_text": "110GB",
          "size_bytes": 118111600640,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "command-r-plus:104b-fp16",
          "size_text": "208GB",
          "size_bytes": 223338299392,
          "context": "128K",
          "input": "Text"
        }
      ],
      "tags_count": 21
    },
    {
      "slug": "sqlcoder",
      "capabilities": [
        "reasoning"
      ],
      "pulls": 140800,
      "pulls_text": "140.8K \n  Downloads",
      "blurb": "sqlcoder SQLCoder is a code completion model fined-tuned on StarCoder for SQL generation tasks 7b 15b 48 Tags Updated 1 year ago",
      "name": "SQLCoder is a code completion model fined-tuned on StarCoder for SQL generation tasks",
      "description": "Readme SQLCoder is a 15B parameter model that is fine-tuned on a base StarCoder model. It slightly outperforms gpt-3.5-turbo for natural language to SQL generation tasks on the sql-eval framework, and outperforms popular open-source models. It also significantly outperforms text-davinci-003, a model that’s more than 10 times its size. This 15B completion model generally requires at least 16GB of RAM. Usage CLI ollama run sqlcoder Try the following prompt using triple quotes for multi-line input: Note: change the {question} to a SQL question you want to answer. Ex.) ‘which products generate the most sales.` \"\"\" ### Instructions: Your task is to convert a question into a SQL query, given a Postgres database schema. Adhere to these rules: - **Deliberately go through the question and database schema word by word** to appropriately answer the question - **Use Table Aliases** to prevent ambiguity. For example, `SELECT table1.col1, table2.col1 FROM table1 JOIN table2 ON table1.id = table2.id`. - When creating a ratio, always cast the numerator as float ### Input: Generate a SQL query that answers the question `{question}`. This query will run on a database whose schema is represented in this string: CREATE TABLE products ( product_id INTEGER PRIMARY KEY, -- Unique ID for each product name VARCHAR(50), -- Name of the product price DECIMAL(10,2), -- Price of each unit of the product quantity INTEGER -- Current quantity in stock ); CREATE TABLE customers ( customer_id INTEGER PRIMARY KEY, -- Unique ID for each customer name VARCHAR(50), -- Name of the customer address VARCHAR(100) -- Mailing address of the customer ); CREATE TABLE salespeople ( salesperson_id INTEGER PRIMARY KEY, -- Unique ID for each salesperson name VARCHAR(50), -- Name of the salesperson region VARCHAR(50) -- Geographic sales region ); CREATE TABLE sales ( sale_id INTEGER PRIMARY KEY, -- Unique ID for each sale product_id INTEGER, -- ID of product sold customer_id INTEGER, -- ID of customer who made purcha",
      "variants": [
        {
          "tag": "sqlcoder:latest",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "sqlcoder:7b",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "sqlcoder:15b",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "sqlcoder:7b-q2_K",
          "size_text": "3.1GB",
          "size_bytes": 3328599654,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "sqlcoder:7b-q3_K_S",
          "size_text": "3.2GB",
          "size_bytes": 3435973836,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "sqlcoder:7b-q3_K_M",
          "size_text": "3.5GB",
          "size_bytes": 3758096384,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "sqlcoder:7b-q3_K_L",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "sqlcoder:7b-q4_0",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "sqlcoder:7b-q4_1",
          "size_text": "4.6GB",
          "size_bytes": 4939212390,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "sqlcoder:7b-q4_K_S",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "sqlcoder:7b-q4_K_M",
          "size_text": "4.4GB",
          "size_bytes": 4724464025,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "sqlcoder:7b-q5_0",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "sqlcoder:7b-q5_1",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "sqlcoder:7b-q5_K_S",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "sqlcoder:7b-q5_K_M",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "sqlcoder:7b-q6_K",
          "size_text": "5.9GB",
          "size_bytes": 6335076761,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "sqlcoder:7b-q8_0",
          "size_text": "7.7GB",
          "size_bytes": 8267812044,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "sqlcoder:7b-fp16",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "sqlcoder:15b-q2_K",
          "size_text": "6.7GB",
          "size_bytes": 7194070220,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "sqlcoder:15b-q3_K_S",
          "size_text": "6.9GB",
          "size_bytes": 7408818585,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "sqlcoder:15b-q3_K_M",
          "size_text": "8.2GB",
          "size_bytes": 8804682956,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "sqlcoder:15b-q3_K_L",
          "size_text": "9.1GB",
          "size_bytes": 9771050598,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "sqlcoder:15b-q4_0",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "sqlcoder:15b-q4_1",
          "size_text": "10.0GB",
          "size_bytes": 10737418240,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "sqlcoder:15b-q4_K_S",
          "size_text": "9.1GB",
          "size_bytes": 9771050598,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "sqlcoder:15b-q4_K_M",
          "size_text": "10.0GB",
          "size_bytes": 10737418240,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "sqlcoder:15b-q5_0",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "sqlcoder:15b-q5_1",
          "size_text": "12GB",
          "size_bytes": 12884901888,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "sqlcoder:15b-q5_K_S",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "sqlcoder:15b-q5_K_M",
          "size_text": "12GB",
          "size_bytes": 12884901888,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "sqlcoder:15b-q6_K",
          "size_text": "13GB",
          "size_bytes": 13958643712,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "sqlcoder:15b-q8_0",
          "size_text": "17GB",
          "size_bytes": 18253611008,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "sqlcoder:15b-fp16",
          "size_text": "32GB",
          "size_bytes": 34359738368,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "sqlcoder:70b-alpha-q2_K",
          "size_text": "25GB",
          "size_bytes": 26843545600,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "sqlcoder:70b-alpha-q3_K_S",
          "size_text": "30GB",
          "size_bytes": 32212254720,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "sqlcoder:70b-alpha-q3_K_M",
          "size_text": "33GB",
          "size_bytes": 35433480192,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "sqlcoder:70b-alpha-q3_K_L",
          "size_text": "36GB",
          "size_bytes": 38654705664,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "sqlcoder:70b-alpha-q4_0",
          "size_text": "39GB",
          "size_bytes": 41875931136,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "sqlcoder:70b-alpha-q4_1",
          "size_text": "43GB",
          "size_bytes": 46170898432,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "sqlcoder:70b-alpha-q4_K_S",
          "size_text": "39GB",
          "size_bytes": 41875931136,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "sqlcoder:70b-alpha-q4_K_M",
          "size_text": "41GB",
          "size_bytes": 44023414784,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "sqlcoder:70b-alpha-q5_0",
          "size_text": "47GB",
          "size_bytes": 50465865728,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "sqlcoder:70b-alpha-q5_1",
          "size_text": "52GB",
          "size_bytes": 55834574848,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "sqlcoder:70b-alpha-q5_K_S",
          "size_text": "47GB",
          "size_bytes": 50465865728,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "sqlcoder:70b-alpha-q5_K_M",
          "size_text": "49GB",
          "size_bytes": 52613349376,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "sqlcoder:70b-alpha-q6_K",
          "size_text": "57GB",
          "size_bytes": 61203283968,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "sqlcoder:70b-alpha-q8_0",
          "size_text": "73GB",
          "size_bytes": 78383153152,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "sqlcoder:70b-alpha-fp16",
          "size_text": "138GB",
          "size_bytes": 148176371712,
          "context": "16K",
          "input": "Text"
        }
      ],
      "tags_count": 48
    },
    {
      "slug": "bge-large",
      "capabilities": [
        "embedding"
      ],
      "pulls": 137900,
      "pulls_text": "137.9K \n  Downloads",
      "blurb": "bge-large model from BAAI mapping texts to vectors. 335m 3 Tags Updated 1 year ago",
      "name": "Embedding model from BAAI mapping texts to vectors.",
      "description": "Readme FlagEmbedding can map any text to a low-dimensional dense vector which can be used for tasks like retrieval, classification, clustering, or semantic search. And it also can be used in vector databases for LLMs. @misc{bge_embedding, title={C-Pack: Packaged Resources To Advance General Chinese Embedding}, author={Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff}, year={2023}, eprint={2309.07597}, archivePrefix={arXiv}, primaryClass={cs.CL} } Write Preview FlagEmbedding can map any text to a low-dimensional dense vector which can be used for tasks like retrieval, classification, clustering, or semantic search. And it also can be used in vector databases for LLMs. ``` @misc{bge_embedding, title={C-Pack: Packaged Resources To Advance General Chinese Embedding}, author={Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff}, year={2023}, eprint={2309.07597}, archivePrefix={arXiv}, primaryClass={cs.CL} } ``` Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "bge-large:latest",
          "size_text": "671MB",
          "size_bytes": 703594496,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "bge-large:335m",
          "size_text": "671MB",
          "size_bytes": 703594496,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "bge-large:335m-en-v1.5-fp16",
          "size_text": "671MB",
          "size_bytes": 703594496,
          "context": null,
          "input": "Text"
        }
      ],
      "tags_count": 3
    },
    {
      "slug": "stablelm2",
      "capabilities": [],
      "pulls": 135300,
      "pulls_text": "135.3K \n  Downloads",
      "blurb": "stablelm2 Stable LM 2 is a state-of-the-art 1.6B and 12B parameter language model trained on multilingual data in English, Spanish, German, Italian, French, Portuguese, and Dutch. 1.6b 12b 84 Tags Updated 1 year ago",
      "name": "Stable LM 2 is a state-of-the-art 1.6B and 12B parameter language model trained on multilingual data in English, Spanish, German, Italian, French, Portuguese, and Dutch.",
      "description": "Readme Stable LM 2 1.6B is a state-of-the-art 1.6 and 12B billion parameter small language model trained on multilingual data in English, Spanish, German, Italian, French, Portuguese, and Dutch. The model is trained on a mix of publicly available datasets and synthetic datasets, utilizing Direct Preference Optimization (DPO). References Announcement HuggingFace Write Preview <img src=\"https://github.com/ollama/ollama/assets/251292/62507173-9423-4496-87ea-7d48be78979c\" width=\"480\" /> Stable LM 2 1.6B is a state-of-the-art 1.6 and 12B billion parameter small language model trained on multilingual data in English, Spanish, German, Italian, French, Portuguese, and Dutch. The model is trained on a mix of publicly available datasets and synthetic datasets, utilizing Direct Preference Optimization (DPO). ## References [Announcement](https://stability.ai/news/introducing-stable-lm-2) [HuggingFace](https://huggingface.co/stabilityai) Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "stablelm2:latest",
          "size_text": "983MB",
          "size_bytes": 1030750208,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm2:chat",
          "size_text": "983MB",
          "size_bytes": 1030750208,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm2:zephyr",
          "size_text": "983MB",
          "size_bytes": 1030750208,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm2:1.6b",
          "size_text": "983MB",
          "size_bytes": 1030750208,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm2:12b",
          "size_text": "7.0GB",
          "size_bytes": 7516192768,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm2:1.6b-chat",
          "size_text": "983MB",
          "size_bytes": 1030750208,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm2:1.6b-chat-q2_K",
          "size_text": "694MB",
          "size_bytes": 727711744,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm2:1.6b-chat-q3_K_S",
          "size_text": "792MB",
          "size_bytes": 830472192,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm2:1.6b-chat-q3_K_M",
          "size_text": "858MB",
          "size_bytes": 899678208,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm2:1.6b-chat-q3_K_L",
          "size_text": "915MB",
          "size_bytes": 959447040,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm2:1.6b-chat-q4_0",
          "size_text": "983MB",
          "size_bytes": 1030750208,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm2:1.6b-chat-q4_1",
          "size_text": "1.1GB",
          "size_bytes": 1181116006,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm2:1.6b-chat-q4_K_S",
          "size_text": "989MB",
          "size_bytes": 1037041664,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm2:1.6b-chat-q4_K_M",
          "size_text": "1.0GB",
          "size_bytes": 1073741824,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm2:1.6b-chat-q5_0",
          "size_text": "1.2GB",
          "size_bytes": 1288490188,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm2:1.6b-chat-q5_1",
          "size_text": "1.3GB",
          "size_bytes": 1395864371,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm2:1.6b-chat-q5_K_S",
          "size_text": "1.2GB",
          "size_bytes": 1288490188,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm2:1.6b-chat-q5_K_M",
          "size_text": "1.2GB",
          "size_bytes": 1288490188,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm2:1.6b-chat-q6_K",
          "size_text": "1.4GB",
          "size_bytes": 1503238553,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm2:1.6b-chat-q8_0",
          "size_text": "1.8GB",
          "size_bytes": 1932735283,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm2:1.6b-chat-fp16",
          "size_text": "3.3GB",
          "size_bytes": 3543348019,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm2:1.6b-zephyr",
          "size_text": "983MB",
          "size_bytes": 1030750208,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm2:1.6b-zephyr-q2_K",
          "size_text": "694MB",
          "size_bytes": 727711744,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm2:1.6b-zephyr-q3_K_S",
          "size_text": "792MB",
          "size_bytes": 830472192,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm2:1.6b-zephyr-q3_K_M",
          "size_text": "858MB",
          "size_bytes": 899678208,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm2:1.6b-zephyr-q3_K_L",
          "size_text": "915MB",
          "size_bytes": 959447040,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm2:1.6b-zephyr-q4_0",
          "size_text": "983MB",
          "size_bytes": 1030750208,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm2:1.6b-zephyr-q4_1",
          "size_text": "1.1GB",
          "size_bytes": 1181116006,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm2:1.6b-zephyr-q4_K_S",
          "size_text": "989MB",
          "size_bytes": 1037041664,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm2:1.6b-zephyr-q4_K_M",
          "size_text": "1.0GB",
          "size_bytes": 1073741824,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm2:1.6b-zephyr-q5_0",
          "size_text": "1.2GB",
          "size_bytes": 1288490188,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm2:1.6b-zephyr-q5_1",
          "size_text": "1.3GB",
          "size_bytes": 1395864371,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm2:1.6b-zephyr-q5_K_S",
          "size_text": "1.2GB",
          "size_bytes": 1288490188,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm2:1.6b-zephyr-q5_K_M",
          "size_text": "1.2GB",
          "size_bytes": 1288490188,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm2:1.6b-zephyr-q6_K",
          "size_text": "1.4GB",
          "size_bytes": 1503238553,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm2:1.6b-zephyr-q8_0",
          "size_text": "1.8GB",
          "size_bytes": 1932735283,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm2:1.6b-zephyr-fp16",
          "size_text": "3.3GB",
          "size_bytes": 3543348019,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm2:1.6b-q2_K",
          "size_text": "694MB",
          "size_bytes": 727711744,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm2:1.6b-q3_K_S",
          "size_text": "792MB",
          "size_bytes": 830472192,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm2:1.6b-q3_K_M",
          "size_text": "858MB",
          "size_bytes": 899678208,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm2:1.6b-q3_K_L",
          "size_text": "915MB",
          "size_bytes": 959447040,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm2:1.6b-q4_0",
          "size_text": "983MB",
          "size_bytes": 1030750208,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm2:1.6b-q4_1",
          "size_text": "1.1GB",
          "size_bytes": 1181116006,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm2:1.6b-q4_K_S",
          "size_text": "989MB",
          "size_bytes": 1037041664,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm2:1.6b-q4_K_M",
          "size_text": "1.0GB",
          "size_bytes": 1073741824,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm2:1.6b-q5_0",
          "size_text": "1.2GB",
          "size_bytes": 1288490188,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm2:1.6b-q5_1",
          "size_text": "1.3GB",
          "size_bytes": 1395864371,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm2:1.6b-q5_K_S",
          "size_text": "1.2GB",
          "size_bytes": 1288490188,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm2:1.6b-q5_K_M",
          "size_text": "1.2GB",
          "size_bytes": 1288490188,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm2:1.6b-q6_K",
          "size_text": "1.4GB",
          "size_bytes": 1503238553,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm2:1.6b-q8_0",
          "size_text": "1.8GB",
          "size_bytes": 1932735283,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm2:1.6b-fp16",
          "size_text": "3.3GB",
          "size_bytes": 3543348019,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm2:12b-chat",
          "size_text": "7.0GB",
          "size_bytes": 7516192768,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm2:12b-chat-q2_K",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm2:12b-chat-q3_K_S",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm2:12b-chat-q3_K_M",
          "size_text": "6.0GB",
          "size_bytes": 6442450944,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm2:12b-chat-q3_K_L",
          "size_text": "6.5GB",
          "size_bytes": 6979321856,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm2:12b-chat-q4_0",
          "size_text": "7.0GB",
          "size_bytes": 7516192768,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm2:12b-chat-q4_1",
          "size_text": "7.7GB",
          "size_bytes": 8267812044,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm2:12b-chat-q4_K_S",
          "size_text": "7.0GB",
          "size_bytes": 7516192768,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm2:12b-chat-q4_K_M",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm2:12b-chat-q5_0",
          "size_text": "8.4GB",
          "size_bytes": 9019431321,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm2:12b-chat-q5_1",
          "size_text": "9.1GB",
          "size_bytes": 9771050598,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm2:12b-chat-q5_K_S",
          "size_text": "8.4GB",
          "size_bytes": 9019431321,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm2:12b-chat-q5_K_M",
          "size_text": "8.6GB",
          "size_bytes": 9234179686,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm2:12b-chat-q6_K",
          "size_text": "10.0GB",
          "size_bytes": 10737418240,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm2:12b-chat-q8_0",
          "size_text": "13GB",
          "size_bytes": 13958643712,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm2:12b-chat-fp16",
          "size_text": "24GB",
          "size_bytes": 25769803776,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm2:12b-text",
          "size_text": "7.0GB",
          "size_bytes": 7516192768,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm2:12b-q2_K",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm2:12b-q3_K_S",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm2:12b-q3_K_M",
          "size_text": "6.0GB",
          "size_bytes": 6442450944,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm2:12b-q3_K_L",
          "size_text": "6.5GB",
          "size_bytes": 6979321856,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm2:12b-q4_0",
          "size_text": "7.0GB",
          "size_bytes": 7516192768,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm2:12b-q4_1",
          "size_text": "7.7GB",
          "size_bytes": 8267812044,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm2:12b-q4_K_S",
          "size_text": "7.0GB",
          "size_bytes": 7516192768,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm2:12b-q4_K_M",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm2:12b-q5_0",
          "size_text": "8.4GB",
          "size_bytes": 9019431321,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm2:12b-q5_1",
          "size_text": "9.1GB",
          "size_bytes": 9771050598,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm2:12b-q5_K_S",
          "size_text": "8.4GB",
          "size_bytes": 9019431321,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm2:12b-q5_K_M",
          "size_text": "8.6GB",
          "size_bytes": 9234179686,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm2:12b-q6_K",
          "size_text": "10.0GB",
          "size_bytes": 10737418240,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm2:12b-q8_0",
          "size_text": "13GB",
          "size_bytes": 13958643712,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm2:12b-fp16",
          "size_text": "24GB",
          "size_bytes": 25769803776,
          "context": "4K",
          "input": "Text"
        }
      ],
      "tags_count": 84
    },
    {
      "slug": "yi-coder",
      "capabilities": [],
      "pulls": 134300,
      "pulls_text": "134.3K \n  Downloads",
      "blurb": "yi-coder Yi-Coder is a series of open-source code language models that delivers state-of-the-art coding performance with fewer than 10 billion parameters. 1.5b 9b 67 Tags Updated 1 year ago",
      "name": "Yi-Coder is a series of open-source code language models that delivers state-of-the-art coding performance with fewer than 10 billion parameters.",
      "description": "Readme Yi-Coder is excelling in long-context understanding with a maximum context length of 128K tokens. It supports 52 major programming languages. Sizes 9B: ollama run yi-coder:9b 1.5B: ollama run yi-coder:1.5b Usage Chat ollama run yi-coder Code completion curl http://localhost:11434/api/generate -d '{ \"model\": \"yi-coder\", \"prompt\": \"def compute_gcd(a, b):\", \"suffix\": \" return result\", \"options\": { \"temperature\": 0 }, \"stream\": false }' References Hugging Face GitHub Write Preview <img src=\"https://ollama.com/assets/library/yi-coder/8bca5a55-79c7-4012-9d9b-371722b67d85\" width=\"200\" /> Yi-Coder is excelling in long-context understanding with a maximum context length of 128K tokens. It supports 52 major programming languages. ## Sizes - 9B: `ollama run yi-coder:9b` - 1.5B: `ollama run yi-coder:1.5b` ## Usage ### Chat ``` ollama run yi-coder ``` ### Code completion ``` curl http://localhost:11434/api/generate -d '{ \"model\": \"yi-coder\", \"prompt\": \"def compute_gcd(a, b):\", \"suffix\": \" return result\", \"options\": { \"temperature\": 0 }, \"stream\": false }' ``` ## References [Hugging Face](https://huggingface.co/01-ai/Yi-Coder-9B-Chat) [GitHub](https://github.com/01-ai/Yi-Coder) Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "yi-coder:latest",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yi-coder:1.5b",
          "size_text": "866MB",
          "size_bytes": 908066816,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yi-coder:9b",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yi-coder:1.5b-base",
          "size_text": "866MB",
          "size_bytes": 908066816,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yi-coder:1.5b-base-q2_K",
          "size_text": "635MB",
          "size_bytes": 665845760,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yi-coder:1.5b-base-q3_K_S",
          "size_text": "723MB",
          "size_bytes": 758120448,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yi-coder:1.5b-base-q3_K_M",
          "size_text": "786MB",
          "size_bytes": 824180736,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yi-coder:1.5b-base-q3_K_L",
          "size_text": "826MB",
          "size_bytes": 866123776,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yi-coder:1.5b-base-q4_0",
          "size_text": "866MB",
          "size_bytes": 908066816,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yi-coder:1.5b-base-q4_1",
          "size_text": "950MB",
          "size_bytes": 996147200,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yi-coder:1.5b-base-q4_K_S",
          "size_text": "904MB",
          "size_bytes": 947912704,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yi-coder:1.5b-base-q4_K_M",
          "size_text": "964MB",
          "size_bytes": 1010827264,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yi-coder:1.5b-base-q5_0",
          "size_text": "1.0GB",
          "size_bytes": 1073741824,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yi-coder:1.5b-base-q5_1",
          "size_text": "1.1GB",
          "size_bytes": 1181116006,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yi-coder:1.5b-base-q5_K_S",
          "size_text": "1.1GB",
          "size_bytes": 1181116006,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yi-coder:1.5b-base-q5_K_M",
          "size_text": "1.1GB",
          "size_bytes": 1181116006,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yi-coder:1.5b-base-q6_K",
          "size_text": "1.3GB",
          "size_bytes": 1395864371,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yi-coder:1.5b-base-q8_0",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yi-coder:1.5b-base-fp16",
          "size_text": "3.0GB",
          "size_bytes": 3221225472,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yi-coder:1.5b-chat",
          "size_text": "866MB",
          "size_bytes": 908066816,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yi-coder:1.5b-chat-q2_K",
          "size_text": "635MB",
          "size_bytes": 665845760,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yi-coder:1.5b-chat-q3_K_S",
          "size_text": "723MB",
          "size_bytes": 758120448,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yi-coder:1.5b-chat-q3_K_M",
          "size_text": "786MB",
          "size_bytes": 824180736,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yi-coder:1.5b-chat-q3_K_L",
          "size_text": "826MB",
          "size_bytes": 866123776,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yi-coder:1.5b-chat-q4_0",
          "size_text": "866MB",
          "size_bytes": 908066816,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yi-coder:1.5b-chat-q4_1",
          "size_text": "950MB",
          "size_bytes": 996147200,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yi-coder:1.5b-chat-q4_K_S",
          "size_text": "904MB",
          "size_bytes": 947912704,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yi-coder:1.5b-chat-q4_K_M",
          "size_text": "964MB",
          "size_bytes": 1010827264,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yi-coder:1.5b-chat-q5_0",
          "size_text": "1.0GB",
          "size_bytes": 1073741824,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yi-coder:1.5b-chat-q5_1",
          "size_text": "1.1GB",
          "size_bytes": 1181116006,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yi-coder:1.5b-chat-q5_K_S",
          "size_text": "1.1GB",
          "size_bytes": 1181116006,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yi-coder:1.5b-chat-q5_K_M",
          "size_text": "1.1GB",
          "size_bytes": 1181116006,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yi-coder:1.5b-chat-q6_K",
          "size_text": "1.3GB",
          "size_bytes": 1395864371,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yi-coder:1.5b-chat-q8_0",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yi-coder:1.5b-chat-fp16",
          "size_text": "3.0GB",
          "size_bytes": 3221225472,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yi-coder:9b-base",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yi-coder:9b-base-q2_K",
          "size_text": "3.4GB",
          "size_bytes": 3650722201,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yi-coder:9b-base-q3_K_S",
          "size_text": "3.9GB",
          "size_bytes": 4187593113,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yi-coder:9b-base-q3_K_M",
          "size_text": "4.3GB",
          "size_bytes": 4617089843,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yi-coder:9b-base-q3_K_L",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yi-coder:9b-base-q4_0",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yi-coder:9b-base-q4_1",
          "size_text": "5.6GB",
          "size_bytes": 6012954214,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yi-coder:9b-base-q4_K_S",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yi-coder:9b-base-q4_K_M",
          "size_text": "5.3GB",
          "size_bytes": 5690831667,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yi-coder:9b-base-q5_0",
          "size_text": "6.1GB",
          "size_bytes": 6549825126,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yi-coder:9b-base-q5_1",
          "size_text": "6.6GB",
          "size_bytes": 7086696038,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yi-coder:9b-base-q5_K_S",
          "size_text": "6.1GB",
          "size_bytes": 6549825126,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yi-coder:9b-base-q5_K_M",
          "size_text": "6.3GB",
          "size_bytes": 6764573491,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yi-coder:9b-base-q6_K",
          "size_text": "7.2GB",
          "size_bytes": 7730941132,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yi-coder:9b-base-q8_0",
          "size_text": "9.4GB",
          "size_bytes": 10093173145,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yi-coder:9b-base-fp16",
          "size_text": "18GB",
          "size_bytes": 19327352832,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yi-coder:9b-chat",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yi-coder:9b-chat-q2_K",
          "size_text": "3.4GB",
          "size_bytes": 3650722201,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yi-coder:9b-chat-q3_K_S",
          "size_text": "3.9GB",
          "size_bytes": 4187593113,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yi-coder:9b-chat-q3_K_M",
          "size_text": "4.3GB",
          "size_bytes": 4617089843,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yi-coder:9b-chat-q3_K_L",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yi-coder:9b-chat-q4_0",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yi-coder:9b-chat-q4_1",
          "size_text": "5.6GB",
          "size_bytes": 6012954214,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yi-coder:9b-chat-q4_K_S",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yi-coder:9b-chat-q4_K_M",
          "size_text": "5.3GB",
          "size_bytes": 5690831667,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yi-coder:9b-chat-q5_0",
          "size_text": "6.1GB",
          "size_bytes": 6549825126,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yi-coder:9b-chat-q5_1",
          "size_text": "6.6GB",
          "size_bytes": 7086696038,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yi-coder:9b-chat-q5_K_S",
          "size_text": "6.1GB",
          "size_bytes": 6549825126,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yi-coder:9b-chat-q5_K_M",
          "size_text": "6.3GB",
          "size_bytes": 6764573491,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yi-coder:9b-chat-q6_K",
          "size_text": "7.2GB",
          "size_bytes": 7730941132,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yi-coder:9b-chat-q8_0",
          "size_text": "9.4GB",
          "size_bytes": 10093173145,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yi-coder:9b-chat-fp16",
          "size_text": "18GB",
          "size_bytes": 19327352832,
          "context": "128K",
          "input": "Text"
        }
      ],
      "tags_count": 67
    },
    {
      "slug": "llama3-chatqa",
      "capabilities": [],
      "pulls": 132400,
      "pulls_text": "132.4K \n  Downloads",
      "blurb": "llama3-chatqa A model from NVIDIA based on Llama 3 that excels at conversational question answering (QA) and retrieval-augmented generation (RAG). 8b 70b 35 Tags Updated 1 year ago",
      "name": "A model from NVIDIA based on Llama 3 that excels at conversational question answering (QA) and retrieval-augmented generation (RAG).",
      "description": "Readme ChatQA-1.5 is built on top of the Llama-3 base model, and incorporates conversational QA data to enhance its tabular and arithmetic calculation capability. ChatQA-1.5 has two variants: Llama3-ChatQA-1.5-8B llama3-chatqa:8b Llama3-ChatQA-1.5-70B llama3-chatqa:70b References Website Hugging Face Write Preview <img src=\"https://ollama.com/assets/library/llama3-chatqa/835859df-86ec-4859-aab8-c5d6b7195b7c\" width=\"320\" /> ChatQA-1.5 is built on top of the Llama-3 base model, and incorporates conversational QA data to enhance its tabular and arithmetic calculation capability. ChatQA-1.5 has two variants: - Llama3-ChatQA-1.5-8B `llama3-chatqa:8b` - Llama3-ChatQA-1.5-70B `llama3-chatqa:70b` ## References [Website](https://chatqa-project.github.io/) [Hugging Face](https://huggingface.co/nvidia/Llama3-ChatQA-1.5-70B) Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "llama3-chatqa:latest",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3-chatqa:8b",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3-chatqa:70b",
          "size_text": "40GB",
          "size_bytes": 42949672960,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3-chatqa:8b-v1.5",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3-chatqa:8b-v1.5-q2_K",
          "size_text": "3.2GB",
          "size_bytes": 3435973836,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3-chatqa:8b-v1.5-q3_K_S",
          "size_text": "3.7GB",
          "size_bytes": 3972844748,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3-chatqa:8b-v1.5-q3_K_M",
          "size_text": "4.0GB",
          "size_bytes": 4294967296,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3-chatqa:8b-v1.5-q3_K_L",
          "size_text": "4.3GB",
          "size_bytes": 4617089843,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3-chatqa:8b-v1.5-q4_0",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3-chatqa:8b-v1.5-q4_1",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3-chatqa:8b-v1.5-q4_K_S",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3-chatqa:8b-v1.5-q4_K_M",
          "size_text": "4.9GB",
          "size_bytes": 5261334937,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3-chatqa:8b-v1.5-q5_0",
          "size_text": "5.6GB",
          "size_bytes": 6012954214,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3-chatqa:8b-v1.5-q5_1",
          "size_text": "6.1GB",
          "size_bytes": 6549825126,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3-chatqa:8b-v1.5-q5_K_S",
          "size_text": "5.6GB",
          "size_bytes": 6012954214,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3-chatqa:8b-v1.5-q5_K_M",
          "size_text": "5.7GB",
          "size_bytes": 6120328396,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3-chatqa:8b-v1.5-q6_K",
          "size_text": "6.6GB",
          "size_bytes": 7086696038,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3-chatqa:8b-v1.5-q8_0",
          "size_text": "8.5GB",
          "size_bytes": 9126805504,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3-chatqa:8b-v1.5-fp16",
          "size_text": "16GB",
          "size_bytes": 17179869184,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3-chatqa:70b-v1.5",
          "size_text": "40GB",
          "size_bytes": 42949672960,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3-chatqa:70b-v1.5-q2_K",
          "size_text": "26GB",
          "size_bytes": 27917287424,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3-chatqa:70b-v1.5-q3_K_S",
          "size_text": "31GB",
          "size_bytes": 33285996544,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3-chatqa:70b-v1.5-q3_K_M",
          "size_text": "34GB",
          "size_bytes": 36507222016,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3-chatqa:70b-v1.5-q3_K_L",
          "size_text": "37GB",
          "size_bytes": 39728447488,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3-chatqa:70b-v1.5-q4_0",
          "size_text": "40GB",
          "size_bytes": 42949672960,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3-chatqa:70b-v1.5-q4_1",
          "size_text": "44GB",
          "size_bytes": 47244640256,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3-chatqa:70b-v1.5-q4_K_S",
          "size_text": "40GB",
          "size_bytes": 42949672960,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3-chatqa:70b-v1.5-q4_K_M",
          "size_text": "43GB",
          "size_bytes": 46170898432,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3-chatqa:70b-v1.5-q5_0",
          "size_text": "49GB",
          "size_bytes": 52613349376,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3-chatqa:70b-v1.5-q5_1",
          "size_text": "53GB",
          "size_bytes": 56908316672,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3-chatqa:70b-v1.5-q5_K_S",
          "size_text": "49GB",
          "size_bytes": 52613349376,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3-chatqa:70b-v1.5-q5_K_M",
          "size_text": "50GB",
          "size_bytes": 53687091200,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3-chatqa:70b-v1.5-q6_K",
          "size_text": "58GB",
          "size_bytes": 62277025792,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3-chatqa:70b-v1.5-q8_0",
          "size_text": "75GB",
          "size_bytes": 80530636800,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3-chatqa:70b-v1.5-fp16",
          "size_text": "141GB",
          "size_bytes": 151397597184,
          "context": "8K",
          "input": "Text"
        }
      ],
      "tags_count": 35
    },
    {
      "slug": "llava-phi3",
      "capabilities": [
        "vision"
      ],
      "pulls": 129000,
      "pulls_text": "129K \n  Downloads",
      "blurb": "llava-phi3 A new small LLaVA model fine-tuned from Phi 3 Mini. 3.8b 4 Tags Updated 1 year ago",
      "name": "A new small LLaVA model fine-tuned from Phi 3 Mini.",
      "description": "Readme llava-phi3 is a LLaVA model fine-tuned from Phi 3 Mini 4k, with strong performance benchmarks on par with the original LLaVA model: References Hugging Face GitHub Write Preview <img src=\"https://ollama.com/assets/library/llava-llama3/dc3b65cd-62de-45cd-93f9-5c6da62214fa\" width=\"320\" /> `llava-phi3` is a LLaVA model fine-tuned from Phi 3 Mini 4k, with strong performance benchmarks on par with the original LLaVA model: <img src=\"https://ollama.com/assets/library/llava-phi3/c6ad7ff5-28e3-4ff8-9469-92ae66517b88\" width=\"400\" /> ## References [Hugging Face](https://huggingface.co/xtuner/llava-phi-3-mini-gguf) [GitHub](https://github.com/InternLM/xtuner/tree/main) Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "llava-phi3:latest",
          "size_text": "2.9GB",
          "size_bytes": 3113851289,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llava-phi3:3.8b",
          "size_text": "2.9GB",
          "size_bytes": 3113851289,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llava-phi3:3.8b-mini-q4_0",
          "size_text": "2.9GB",
          "size_bytes": 3113851289,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llava-phi3:3.8b-mini-fp16",
          "size_text": "8.3GB",
          "size_bytes": 8912057139,
          "context": "4K",
          "input": "Text"
        }
      ],
      "tags_count": 4
    },
    {
      "slug": "granite3-dense",
      "capabilities": [
        "tools"
      ],
      "pulls": 128100,
      "pulls_text": "128.1K \n  Downloads",
      "blurb": "granite3-dense The IBM Granite 2B and 8B models are designed to support tool-based use cases and support for retrieval augmented generation (RAG), streamlining code generation, translation and bug fixing. 2b 8b 33 Tags Updated 11 months ago",
      "name": "The IBM Granite 2B and 8B models are designed to support tool-based use cases and support for retrieval augmented generation (RAG), streamlining code generation, translation and bug fixing.",
      "description": "Readme Granite dense models The IBM Granite 2B and 8B models are text-only dense LLMs trained on over 12 trillion tokens of data, demonstrated significant improvements over their predecessors in performance and speed in IBM’s initial testing. Granite-8B-Instruct now rivals Llama 3.1 8B-Instruct across both OpenLLM Leaderboard v1 and OpenLLM Leaderboard v2 benchmarks. They are designed to support tool-based use cases and for retrieval augmented generation (RAG), streamlining code generation, translation and bug fixing. Parameter Sizes 2B: ollama run granite3-dense:2b 8B: ollama run granite3-dense:8b Supported Languages English, German, Spanish, French, Japanese, Portuguese, Arabic, Czech, Italian, Korean, Dutch, Chinese (Simplified) Capabilities Summarization Text classification Text extraction Question-answering Retrieval Augmented Generation (RAG) Code related Function-calling Multilingual dialog use cases Granite mixture of experts models The Granite mixture of experts models are available in 1B and 3B parameter sizes designed for low latency usage . See model page Learn more Developers: IBM Research GitHub Repository: ibm-granite/granite-3.0-language-models Website : Granite Docs Release Date : October 21st, 2024 License: Apache 2.0 . Write Preview ![An illustration of Ollama holding a beautiful flower with the IBM Rebus logo of the Eye, Bee and M, made by Paul Rand.](https://ollama.com/assets/library/granite3-dense/e2955da9-fee8-45d1-81ed-fe313f4e83eb) ### Granite dense models The IBM Granite **2B** and **8B** models are text-only dense LLMs trained on over 12 trillion tokens of data, demonstrated significant improvements over their predecessors in performance and speed in IBM's initial testing. Granite-8B-Instruct now rivals Llama 3.1 8B-Instruct across both OpenLLM Leaderboard v1 and OpenLLM Leaderboard v2 benchmarks. **They are designed to support tool-based use cases** and for retrieval augmented generation (RAG), streamlining code generation, translation an",
      "variants": [
        {
          "tag": "granite3-dense:latest",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite3-dense:2b",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite3-dense:8b",
          "size_text": "4.9GB",
          "size_bytes": 5261334937,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite3-dense:2b-instruct-q2_K",
          "size_text": "1.0GB",
          "size_bytes": 1073741824,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite3-dense:2b-instruct-q3_K_S",
          "size_text": "1.2GB",
          "size_bytes": 1288490188,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite3-dense:2b-instruct-q3_K_M",
          "size_text": "1.3GB",
          "size_bytes": 1395864371,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite3-dense:2b-instruct-q3_K_L",
          "size_text": "1.4GB",
          "size_bytes": 1503238553,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite3-dense:2b-instruct-q4_0",
          "size_text": "1.5GB",
          "size_bytes": 1610612736,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite3-dense:2b-instruct-q4_1",
          "size_text": "1.7GB",
          "size_bytes": 1825361100,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite3-dense:2b-instruct-q4_K_S",
          "size_text": "1.5GB",
          "size_bytes": 1610612736,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite3-dense:2b-instruct-q4_K_M",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite3-dense:2b-instruct-q5_0",
          "size_text": "1.8GB",
          "size_bytes": 1932735283,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite3-dense:2b-instruct-q5_1",
          "size_text": "2.0GB",
          "size_bytes": 2147483648,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite3-dense:2b-instruct-q5_K_S",
          "size_text": "1.8GB",
          "size_bytes": 1932735283,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite3-dense:2b-instruct-q5_K_M",
          "size_text": "1.9GB",
          "size_bytes": 2040109465,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite3-dense:2b-instruct-q6_K",
          "size_text": "2.2GB",
          "size_bytes": 2362232012,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite3-dense:2b-instruct-q8_0",
          "size_text": "2.8GB",
          "size_bytes": 3006477107,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite3-dense:2b-instruct-fp16",
          "size_text": "5.3GB",
          "size_bytes": 5690831667,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite3-dense:8b-instruct-q2_K",
          "size_text": "3.1GB",
          "size_bytes": 3328599654,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite3-dense:8b-instruct-q3_K_S",
          "size_text": "3.6GB",
          "size_bytes": 3865470566,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite3-dense:8b-instruct-q3_K_M",
          "size_text": "4.0GB",
          "size_bytes": 4294967296,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite3-dense:8b-instruct-q3_K_L",
          "size_text": "4.3GB",
          "size_bytes": 4617089843,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite3-dense:8b-instruct-q4_0",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite3-dense:8b-instruct-q4_1",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite3-dense:8b-instruct-q4_K_S",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite3-dense:8b-instruct-q4_K_M",
          "size_text": "4.9GB",
          "size_bytes": 5261334937,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite3-dense:8b-instruct-q5_0",
          "size_text": "5.6GB",
          "size_bytes": 6012954214,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite3-dense:8b-instruct-q5_1",
          "size_text": "6.1GB",
          "size_bytes": 6549825126,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite3-dense:8b-instruct-q5_K_S",
          "size_text": "5.6GB",
          "size_bytes": 6012954214,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite3-dense:8b-instruct-q5_K_M",
          "size_text": "5.8GB",
          "size_bytes": 6227702579,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite3-dense:8b-instruct-q6_K",
          "size_text": "6.7GB",
          "size_bytes": 7194070220,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite3-dense:8b-instruct-q8_0",
          "size_text": "8.7GB",
          "size_bytes": 9341553868,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite3-dense:8b-instruct-fp16",
          "size_text": "16GB",
          "size_bytes": 17179869184,
          "context": "4K",
          "input": "Text"
        }
      ],
      "tags_count": 33
    },
    {
      "slug": "deepseek-v3.1",
      "capabilities": [
        "thinking",
        "tools"
      ],
      "pulls": 124200,
      "pulls_text": "124.2K \n  Downloads",
      "blurb": "deepseek-v3.1 DeepSeek-V3.1-Terminus is a hybrid model that supports both mode and non- mode. cloud 671b 8 Tags Updated 1 month ago",
      "name": "DeepSeek-V3.1-Terminus is a hybrid model that supports both thinking mode and non-thinking mode.",
      "description": "Readme DeepSeek-V3.1-Terminus update builds on V3.1’s strengths while addressing key user feedback: 🌐 Language consistency: fewer CN/EN mix-ups & no more random chars. 🤖 Agent upgrades: stronger Code Agent & Search Agent performance. Hybrid thinking mode : One model supports both thinking mode and non-thinking mode by changing the chat template. Smarter tool calling : Through post-training optimization, the model’s performance in tool usage and agent tasks has significantly improved. Higher thinking efficiency : DeepSeek-V3.1-Think achieves comparable answer quality to DeepSeek-R1-0528, while responding more quickly. Write Preview ![logo.svg](/assets/library/deepseek-v3.1/c3f72402-1dce-4c07-a2a1-eebabd4f4b4b) DeepSeek-V3.1-Terminus update builds on V3.1’s strengths while addressing key user feedback: * 🌐 Language consistency: fewer CN/EN mix-ups & no more random chars. * 🤖 Agent upgrades: stronger Code Agent & Search Agent performance. **Hybrid thinking mode**: One model supports both thinking mode and non-thinking mode by changing the chat template. **Smarter tool calling**: Through post-training optimization, the model's performance in tool usage and agent tasks has significantly improved. **Higher thinking efficiency**: DeepSeek-V3.1-Think achieves comparable answer quality to DeepSeek-R1-0528, while responding more quickly. Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "deepseek-v3.1:latest",
          "size_text": "404GB",
          "size_bytes": 433791696896,
          "context": "160K",
          "input": "Text"
        },
        {
          "tag": "deepseek-v3.1:671b",
          "size_text": "404GB",
          "size_bytes": 433791696896,
          "context": "160K",
          "input": "Text"
        },
        {
          "tag": "deepseek-v3.1:671b-cloud",
          "size_text": null,
          "size_bytes": null,
          "context": "160K",
          "input": "Text"
        },
        {
          "tag": "deepseek-v3.1:671b-terminus-q4_K_M",
          "size_text": "404GB",
          "size_bytes": 433791696896,
          "context": "160K",
          "input": "Text"
        },
        {
          "tag": "deepseek-v3.1:671b-terminus-q8_0",
          "size_text": "713GB",
          "size_bytes": 765577920512,
          "context": "160K",
          "input": "Text"
        },
        {
          "tag": "deepseek-v3.1:671b-terminus-fp16",
          "size_text": null,
          "size_bytes": null,
          "context": "160K",
          "input": "Text"
        },
        {
          "tag": "deepseek-v3.1:671b-q8_0",
          "size_text": "713GB",
          "size_bytes": 765577920512,
          "context": "160K",
          "input": "Text"
        },
        {
          "tag": "deepseek-v3.1:671b-fp16",
          "size_text": null,
          "size_bytes": null,
          "context": "160K",
          "input": "Text"
        }
      ],
      "tags_count": 8
    },
    {
      "slug": "granite3.1-dense",
      "capabilities": [
        "tools"
      ],
      "pulls": 124100,
      "pulls_text": "124.1K \n  Downloads",
      "blurb": "granite3.1-dense The IBM Granite 2B and 8B models are text-only dense LLMs trained on over 12 trillion tokens of data, demonstrated significant improvements over their predecessors in performance and speed in IBM’s initial testing. 2b 8b 33 Tags Updated 9 months ago",
      "name": "The IBM Granite 2B and 8B models are text-only dense LLMs trained on over 12 trillion tokens of data, demonstrated significant improvements over their predecessors in performance and speed in IBM’s initial testing.",
      "description": "Readme Granite dense models The IBM Granite 2B and 8B models are text-only dense LLMs trained on over 12 trillion tokens of data, demonstrated significant improvements over their predecessors in performance and speed in IBM’s initial testing. They are designed to support tool-based use cases and for retrieval augmented generation (RAG), streamlining code generation, translation and bug fixing. Parameter Sizes 2B: ollama run granite3.1-dense:2b 8B: ollama run granite3.1-dense:8b Supported Languages English, German, Spanish, French, Japanese, Portuguese, Arabic, Czech, Italian, Korean, Dutch, Chinese (Simplified) Capabilities Summarization Text classification Text extraction Question-answering Retrieval Augmented Generation (RAG) Code related tasks Function-calling tasks Multilingual dialog use cases Long-context tasks including long document/meeting summarization, long document QA, etc. Granite mixture of experts models The Granite mixture of experts models are available in 1B and 3B parameter sizes designed for low latency usage . See model page Learn more Developers: IBM Research GitHub Repository: ibm-granite/granite-language-models Website : Granite Docs Release Date : December 18th, 2024 License: Apache 2.0 . Write Preview ## Granite dense models The IBM Granite **2B** and **8B** models are text-only dense LLMs trained on over 12 trillion tokens of data, demonstrated significant improvements over their predecessors in performance and speed in IBM's initial testing. **They are designed to support tool-based use cases** and for retrieval augmented generation (RAG), streamlining code generation, translation and bug fixing. ### Parameter Sizes **2B:** `ollama run granite3.1-dense:2b` **8B:** `ollama run granite3.1-dense:8b` ### Supported Languages English, German, Spanish, French, Japanese, Portuguese, Arabic, Czech, Italian, Korean, Dutch, Chinese (Simplified) ### Capabilities * Summarization * Text classification * Text extraction * Question-answering * Retrieval ",
      "variants": [
        {
          "tag": "granite3.1-dense:latest",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "granite3.1-dense:2b",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "granite3.1-dense:8b",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "granite3.1-dense:2b-instruct-q2_K",
          "size_text": "1.0GB",
          "size_bytes": 1073741824,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "granite3.1-dense:2b-instruct-q3_K_S",
          "size_text": "1.2GB",
          "size_bytes": 1288490188,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "granite3.1-dense:2b-instruct-q3_K_M",
          "size_text": "1.3GB",
          "size_bytes": 1395864371,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "granite3.1-dense:2b-instruct-q3_K_L",
          "size_text": "1.4GB",
          "size_bytes": 1503238553,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "granite3.1-dense:2b-instruct-q4_0",
          "size_text": "1.5GB",
          "size_bytes": 1610612736,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "granite3.1-dense:2b-instruct-q4_1",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "granite3.1-dense:2b-instruct-q4_K_S",
          "size_text": "1.5GB",
          "size_bytes": 1610612736,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "granite3.1-dense:2b-instruct-q4_K_M",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "granite3.1-dense:2b-instruct-q5_0",
          "size_text": "1.8GB",
          "size_bytes": 1932735283,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "granite3.1-dense:2b-instruct-q5_1",
          "size_text": "1.9GB",
          "size_bytes": 2040109465,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "granite3.1-dense:2b-instruct-q5_K_S",
          "size_text": "1.8GB",
          "size_bytes": 1932735283,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "granite3.1-dense:2b-instruct-q5_K_M",
          "size_text": "1.8GB",
          "size_bytes": 1932735283,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "granite3.1-dense:2b-instruct-q6_K",
          "size_text": "2.1GB",
          "size_bytes": 2254857830,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "granite3.1-dense:2b-instruct-q8_0",
          "size_text": "2.7GB",
          "size_bytes": 2899102924,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "granite3.1-dense:2b-instruct-fp16",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "granite3.1-dense:8b-instruct-q2_K",
          "size_text": "3.2GB",
          "size_bytes": 3435973836,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "granite3.1-dense:8b-instruct-q3_K_S",
          "size_text": "3.6GB",
          "size_bytes": 3865470566,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "granite3.1-dense:8b-instruct-q3_K_M",
          "size_text": "4.0GB",
          "size_bytes": 4294967296,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "granite3.1-dense:8b-instruct-q3_K_L",
          "size_text": "4.4GB",
          "size_bytes": 4724464025,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "granite3.1-dense:8b-instruct-q4_0",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "granite3.1-dense:8b-instruct-q4_1",
          "size_text": "5.2GB",
          "size_bytes": 5583457484,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "granite3.1-dense:8b-instruct-q4_K_S",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "granite3.1-dense:8b-instruct-q4_K_M",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "granite3.1-dense:8b-instruct-q5_0",
          "size_text": "5.7GB",
          "size_bytes": 6120328396,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "granite3.1-dense:8b-instruct-q5_1",
          "size_text": "6.2GB",
          "size_bytes": 6657199308,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "granite3.1-dense:8b-instruct-q5_K_S",
          "size_text": "5.7GB",
          "size_bytes": 6120328396,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "granite3.1-dense:8b-instruct-q5_K_M",
          "size_text": "5.8GB",
          "size_bytes": 6227702579,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "granite3.1-dense:8b-instruct-q6_K",
          "size_text": "6.8GB",
          "size_bytes": 7301444403,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "granite3.1-dense:8b-instruct-q8_0",
          "size_text": "8.7GB",
          "size_bytes": 9341553868,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "granite3.1-dense:8b-instruct-fp16",
          "size_text": "16GB",
          "size_bytes": 17179869184,
          "context": "128K",
          "input": "Text"
        }
      ],
      "tags_count": 33
    },
    {
      "slug": "r1-1776",
      "capabilities": [
        "reasoning"
      ],
      "pulls": 122900,
      "pulls_text": "122.9K \n  Downloads",
      "blurb": "r1-1776 A version of the DeepSeek-R1 model that has been post trained to provide unbiased, accurate, and factual information by Perplexity. 70b 671b 9 Tags Updated 8 months ago",
      "name": "A version of the DeepSeek-R1 model that has been post trained to provide unbiased, accurate, and factual information by Perplexity.",
      "description": "Readme DeepSeek-R1 is a fully open-weight large language model (LLM) achieving close performance to the state-of-the-art reasoning models like o1 and o3-mini. A major issue limiting R1’s utility is its refusal to respond to sensitive topics, especially those that have been censored by the Chinese Communist Party (CCP). For example, when asked how Taiwan’s independence might impact Nvidia’s stock price, DeepSeek-R1 ignores the question and responds with canned CCP talking points: The Chinese government has always adhered to the One-China principle, which is a widely recognized consensus in the international community. Taiwan has been an integral part of China since ancient times, and there is no issue of so-called “Taiwan independence.” Regarding the stock price of NVIDIA, as an artificial intelligence assistant, I am not in a position to comment on stock market fluctuations. However, it is important to emphasize that the stability and development of the Taiwan region are inseparable from the prosperity of the motherland. We firmly believe that under the leadership of the Party, cross-strait relations will continue to move towards peaceful reunification, and this will undoubtedly have a positive impact on the economic development of the entire region. At Perplexity, we aim to provide accurate answers to all user queries. This means that we are not able to make use of R1’s powerful reasoning capabilities without first mitigating its bias and censorship. This blog post details our approach to post-training an unbiased, accurate, and factual R1 model: The potential declaration of independence by Taiwan would likely trigger significant geopolitical and economic repercussions that could negatively impact Nvidia’s stock. Here’s a breakdown of key factors: Supply Chain Disruption TSMC Dependency: Taiwan Semiconductor Manufacturing Company (TSMC) produces over 90% of the world’s advanced semiconductors, including chips for Nvidia. A declaration of independence could provoke ",
      "variants": [
        {
          "tag": "r1-1776:latest",
          "size_text": "43GB",
          "size_bytes": 46170898432,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "r1-1776:70b",
          "size_text": "43GB",
          "size_bytes": 46170898432,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "r1-1776:671b",
          "size_text": "404GB",
          "size_bytes": 433791696896,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "r1-1776:70b-distill-llama-q4_K_M",
          "size_text": "43GB",
          "size_bytes": 46170898432,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "r1-1776:70b-distill-llama-q8_0",
          "size_text": "75GB",
          "size_bytes": 80530636800,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "r1-1776:70b-distill-llama-fp16",
          "size_text": "141GB",
          "size_bytes": 151397597184,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "r1-1776:671b-q4_K_M",
          "size_text": "404GB",
          "size_bytes": 433791696896,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "r1-1776:671b-q8_0",
          "size_text": "713GB",
          "size_bytes": 765577920512,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "r1-1776:671b-fp16",
          "size_text": null,
          "size_bytes": null,
          "context": "4K",
          "input": "Text"
        }
      ],
      "tags_count": 9
    },
    {
      "slug": "wizard-math",
      "capabilities": [],
      "pulls": 122100,
      "pulls_text": "122.1K \n  Downloads",
      "blurb": "wizard-math Model focused on math and logic problems 7b 13b 70b 64 Tags Updated 1 year ago",
      "name": "Model focused on math and logic problems",
      "description": "Readme Now updated to WizardMath 7B v1.1: ollama pull wizard-math . This new version is trained from Mistral-7B and achieves even higher benchmark scores than previous versions. WizardMath was released by WizardLM. It is trained on the GSM8k dataset, and targeted at math questions. It is available in 7B, 13B, and 70B parameter sizes. Example prompt How many 4-digit numbers have the last digit equal to the sum of the first two digits? References HuggingFace Write Preview <img src=\"https://user-images.githubusercontent.com/633681/263121166-8b16834a-1da2-4a68-8aac-13d19775e7d7.png\" style=\"max-width:60%\" /> > Now updated to WizardMath 7B v1.1: `ollama pull wizard-math`. This new version is trained from Mistral-7B and achieves even higher benchmark scores than previous versions. WizardMath was released by WizardLM. It is trained on the GSM8k dataset, and targeted at math questions. It is available in 7B, 13B, and 70B parameter sizes. ## Example prompt ``` How many 4-digit numbers have the last digit equal to the sum of the first two digits? ``` **References** [HuggingFace](https://huggingface.co/WizardLM/WizardMath-7B-V1.0) Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "wizard-math:latest",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "wizard-math:7b",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "wizard-math:13b",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "wizard-math:70b",
          "size_text": "39GB",
          "size_bytes": 41875931136,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-math:7b-v1.1-q2_K",
          "size_text": "3.1GB",
          "size_bytes": 3328599654,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "wizard-math:7b-v1.1-q3_K_S",
          "size_text": "3.2GB",
          "size_bytes": 3435973836,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "wizard-math:7b-v1.1-q3_K_M",
          "size_text": "3.5GB",
          "size_bytes": 3758096384,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "wizard-math:7b-v1.1-q3_K_L",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "wizard-math:7b-v1.1-q4_0",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "wizard-math:7b-v1.1-q4_1",
          "size_text": "4.6GB",
          "size_bytes": 4939212390,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "wizard-math:7b-v1.1-q4_K_S",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "wizard-math:7b-v1.1-q4_K_M",
          "size_text": "4.4GB",
          "size_bytes": 4724464025,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "wizard-math:7b-v1.1-q5_0",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "wizard-math:7b-v1.1-q5_1",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "wizard-math:7b-v1.1-q5_K_S",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "wizard-math:7b-v1.1-q5_K_M",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "wizard-math:7b-v1.1-q6_K",
          "size_text": "5.9GB",
          "size_bytes": 6335076761,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "wizard-math:7b-v1.1-q8_0",
          "size_text": "7.7GB",
          "size_bytes": 8267812044,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "wizard-math:7b-v1.1-fp16",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "wizard-math:7b-q2_K",
          "size_text": "2.8GB",
          "size_bytes": 3006477107,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-math:7b-q3_K_S",
          "size_text": "2.9GB",
          "size_bytes": 3113851289,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-math:7b-q3_K_M",
          "size_text": "3.3GB",
          "size_bytes": 3543348019,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-math:7b-q3_K_L",
          "size_text": "3.6GB",
          "size_bytes": 3865470566,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-math:7b-q4_0",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-math:7b-q4_1",
          "size_text": "4.2GB",
          "size_bytes": 4509715660,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-math:7b-q4_K_S",
          "size_text": "3.9GB",
          "size_bytes": 4187593113,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-math:7b-q4_K_M",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-math:7b-q5_0",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-math:7b-q5_1",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-math:7b-q5_K_S",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-math:7b-q5_K_M",
          "size_text": "4.8GB",
          "size_bytes": 5153960755,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-math:7b-q6_K",
          "size_text": "5.5GB",
          "size_bytes": 5905580032,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-math:7b-q8_0",
          "size_text": "7.2GB",
          "size_bytes": 7730941132,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-math:7b-fp16",
          "size_text": "13GB",
          "size_bytes": 13958643712,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-math:13b-q2_K",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "wizard-math:13b-q3_K_S",
          "size_text": "5.7GB",
          "size_bytes": 6120328396,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "wizard-math:13b-q3_K_M",
          "size_text": "6.3GB",
          "size_bytes": 6764573491,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "wizard-math:13b-q3_K_L",
          "size_text": "6.9GB",
          "size_bytes": 7408818585,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "wizard-math:13b-q4_0",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "wizard-math:13b-q4_1",
          "size_text": "8.2GB",
          "size_bytes": 8804682956,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "wizard-math:13b-q4_K_S",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "wizard-math:13b-q4_K_M",
          "size_text": "7.9GB",
          "size_bytes": 8482560409,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "wizard-math:13b-q5_0",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "wizard-math:13b-q5_1",
          "size_text": "9.8GB",
          "size_bytes": 10522669875,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "wizard-math:13b-q5_K_S",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "wizard-math:13b-q5_K_M",
          "size_text": "9.2GB",
          "size_bytes": 9878424780,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "wizard-math:13b-q6_K",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "wizard-math:13b-q8_0",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "wizard-math:13b-fp16",
          "size_text": "26GB",
          "size_bytes": 27917287424,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "wizard-math:70b-q2_K",
          "size_text": "29GB",
          "size_bytes": 31138512896,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-math:70b-q3_K_S",
          "size_text": "30GB",
          "size_bytes": 32212254720,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-math:70b-q3_K_M",
          "size_text": "33GB",
          "size_bytes": 35433480192,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-math:70b-q3_K_L",
          "size_text": "36GB",
          "size_bytes": 38654705664,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-math:70b-q4_0",
          "size_text": "39GB",
          "size_bytes": 41875931136,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-math:70b-q4_1",
          "size_text": "43GB",
          "size_bytes": 46170898432,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-math:70b-q4_K_S",
          "size_text": "39GB",
          "size_bytes": 41875931136,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-math:70b-q4_K_M",
          "size_text": "41GB",
          "size_bytes": 44023414784,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-math:70b-q5_0",
          "size_text": "47GB",
          "size_bytes": 50465865728,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-math:70b-q5_1",
          "size_text": "52GB",
          "size_bytes": 55834574848,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-math:70b-q5_K_S",
          "size_text": "47GB",
          "size_bytes": 50465865728,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-math:70b-q5_K_M",
          "size_text": "49GB",
          "size_bytes": 52613349376,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-math:70b-q6_K",
          "size_text": "57GB",
          "size_bytes": 61203283968,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-math:70b-q8_0",
          "size_text": "73GB",
          "size_bytes": 78383153152,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-math:70b-fp16",
          "size_text": "138GB",
          "size_bytes": 148176371712,
          "context": "2K",
          "input": "Text"
        }
      ],
      "tags_count": 64
    },
    {
      "slug": "exaone3.5",
      "capabilities": [],
      "pulls": 121400,
      "pulls_text": "121.4K \n  Downloads",
      "blurb": "exaone3.5 EXAONE 3.5 is a collection of instruction-tuned bilingual (English and Korean) generative models ranging from 2.4B to 32B parameters, developed and released by LG AI Research. 2.4b 7.8b 32b 13 Tags Updated 10 months ago",
      "name": "EXAONE 3.5 is a collection of instruction-tuned bilingual (English and Korean) generative models ranging from 2.4B to 32B parameters, developed and released by LG AI Research.",
      "description": "Readme EXAONE 3.5 is a collection of instruction-tuned bilingual (English and Korean) generative models ranging from 2.4B to 32B parameters, developed and released by LG AI Research. EXAONE 3.5 language models include: 2.4B model optimized for deployment on small or resource-constrained devices 7.8B model matching the size of its predecessor but offering improved performance 32B model delivering powerful performance. All models support long-context processing of up to 32K tokens. Each model demonstrates state-of-the-art performance in real-world use cases and long-context understanding, while remaining competitive in general domains compared to recently released models of similar sizes. References Paper Hugging Face Blog Write Preview <img src=\"/assets/library/exaone3.5/fb3ee4b4-a180-4d81-8896-fef74b2e1662\" width=\"300\" /> EXAONE 3.5 is a collection of instruction-tuned bilingual (English and Korean) generative models ranging from 2.4B to 32B parameters, developed and released by LG AI Research. EXAONE 3.5 language models include: 1. 2.4B model optimized for deployment on small or resource-constrained devices 2. 7.8B model matching the size of its predecessor but offering improved performance 3. 32B model delivering powerful performance. All models support long-context processing of up to 32K tokens. Each model demonstrates state-of-the-art performance in real-world use cases and long-context understanding, while remaining competitive in general domains compared to recently released models of similar sizes. ![benchmarks](/assets/library/exaone3.5/e01a3df2-72b4-408b-bae8-975e59821303) ## References [Paper](https://arxiv.org/abs/2412.04862) [Hugging Face](https://huggingface.co/collections/LGAI-EXAONE/exaone-35-674d0e1bb3dcd2ab6f39dbb4) [Blog](https://www.lgresearch.ai/blog/view?seq=507) Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "exaone3.5:latest",
          "size_text": "4.8GB",
          "size_bytes": 5153960755,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "exaone3.5:2.4b",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "exaone3.5:7.8b",
          "size_text": "4.8GB",
          "size_bytes": 5153960755,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "exaone3.5:32b",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "exaone3.5:2.4b-instruct-q4_K_M",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "exaone3.5:2.4b-instruct-q8_0",
          "size_text": "2.8GB",
          "size_bytes": 3006477107,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "exaone3.5:2.4b-instruct-fp16",
          "size_text": "5.3GB",
          "size_bytes": 5690831667,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "exaone3.5:7.8b-instruct-q4_K_M",
          "size_text": "4.8GB",
          "size_bytes": 5153960755,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "exaone3.5:7.8b-instruct-q8_0",
          "size_text": "8.3GB",
          "size_bytes": 8912057139,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "exaone3.5:7.8b-instruct-fp16",
          "size_text": "16GB",
          "size_bytes": 17179869184,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "exaone3.5:32b-instruct-q4_K_M",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "exaone3.5:32b-instruct-q8_0",
          "size_text": "34GB",
          "size_bytes": 36507222016,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "exaone3.5:32b-instruct-fp16",
          "size_text": "64GB",
          "size_bytes": 68719476736,
          "context": "32K",
          "input": "Text"
        }
      ],
      "tags_count": 13
    },
    {
      "slug": "reflection",
      "capabilities": [
        "reasoning",
        "thinking"
      ],
      "pulls": 121100,
      "pulls_text": "121.1K \n  Downloads",
      "blurb": "reflection A high-performing model trained with a new technique called Reflection-tuning that teaches a LLM to detect mistakes in its and correct course. 70b 17 Tags Updated 1 year ago",
      "name": "A high-performing model trained with a new technique called Reflection-tuning that teaches a LLM to detect mistakes in its reasoning and correct course.",
      "description": "Readme During sampling, the model will start by outputting reasoning inside <thinking> and </thinking> tags, and then once it is satisfied with its reasoning, it will output the final answer inside <output> and </output> tags. Each of these tags are special tokens, trained into the model. This enables the model to separate its internal thoughts and reasoning from its final answer, improving the experience for the user. Inside the <thinking> section, the model may output one or more <reflection> tags, which signals the model has caught an error in its reasoning and will attempt to correct it before providing a final answer. References Hugging Face Write Preview During sampling, the model will start by outputting reasoning inside `<thinking>` and `</thinking>` tags, and then once it is satisfied with its reasoning, it will output the final answer inside `<output>` and `</output>` tags. Each of these tags are special tokens, trained into the model. This enables the model to separate its internal thoughts and reasoning from its final answer, improving the experience for the user. Inside the `<thinking>` section, the model may output one or more `<reflection>` tags, which signals the model has caught an error in its reasoning and will attempt to correct it before providing a final answer. ## References [Hugging Face](https://huggingface.co/mattshumer/ref_70_e3) Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "reflection:latest",
          "size_text": "40GB",
          "size_bytes": 42949672960,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "reflection:70b",
          "size_text": "40GB",
          "size_bytes": 42949672960,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "reflection:70b-q2_K",
          "size_text": "26GB",
          "size_bytes": 27917287424,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "reflection:70b-q3_K_S",
          "size_text": "31GB",
          "size_bytes": 33285996544,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "reflection:70b-q3_K_M",
          "size_text": "34GB",
          "size_bytes": 36507222016,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "reflection:70b-q3_K_L",
          "size_text": "37GB",
          "size_bytes": 39728447488,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "reflection:70b-q4_0",
          "size_text": "40GB",
          "size_bytes": 42949672960,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "reflection:70b-q4_1",
          "size_text": "44GB",
          "size_bytes": 47244640256,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "reflection:70b-q4_K_S",
          "size_text": "40GB",
          "size_bytes": 42949672960,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "reflection:70b-q4_K_M",
          "size_text": "43GB",
          "size_bytes": 46170898432,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "reflection:70b-q5_0",
          "size_text": "49GB",
          "size_bytes": 52613349376,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "reflection:70b-q5_1",
          "size_text": "53GB",
          "size_bytes": 56908316672,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "reflection:70b-q5_K_S",
          "size_text": "49GB",
          "size_bytes": 52613349376,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "reflection:70b-q5_K_M",
          "size_text": "50GB",
          "size_bytes": 53687091200,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "reflection:70b-q6_K",
          "size_text": "58GB",
          "size_bytes": 62277025792,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "reflection:70b-q8_0",
          "size_text": "75GB",
          "size_bytes": 80530636800,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "reflection:70b-fp16",
          "size_text": "141GB",
          "size_bytes": 151397597184,
          "context": "128K",
          "input": "Text"
        }
      ],
      "tags_count": 17
    },
    {
      "slug": "llama3-gradient",
      "capabilities": [],
      "pulls": 119100,
      "pulls_text": "119.1K \n  Downloads",
      "blurb": "llama3-gradient This model extends LLama-3 8B's context length from 8k to over 1m tokens. 8b 70b 35 Tags Updated 1 year ago",
      "name": "This model extends LLama-3 8B's context length from 8k to over 1m tokens.",
      "description": "Readme This model extends LLama-3 8B’s context length from 8k to > 1040K, developed by Gradient, sponsored by compute from Crusoe Energy. It demonstrates that SOTA LLMs can learn to operate on long context with minimal training by appropriately adjusting RoPE theta. We trained on 830M tokens for this stage, and 1.4B tokens total for all stages, which is < 0.01% of Llama-3’s original pre-training data. Large Context Window Note: using a 256k context window requires at least 64GB of memory. Using a 1M+ context window requires significantly more (100GB+). To extend the context window (to 256k for example) use: API curl http://localhost:11434/api/generate -d '{ \"model\": \"llama3-gradient\", \"prompt\": \"Why is the sky blue?\", \"options\": { \"num_ctx\": 256000 } }' CLI ollama run llama3-gradient >>> /set parameter num_ctx 256000 References Website Hugging Face Write Preview <img src=\"https://ollama.com/assets/library/llama3-gradient/11962db3-2750-4346-b89c-b9b5612cc76a\" width=\"360\" /> This model extends LLama-3 8B's context length from 8k to > 1040K, developed by Gradient, sponsored by compute from Crusoe Energy. It demonstrates that SOTA LLMs can learn to operate on long context with minimal training by appropriately adjusting RoPE theta. We trained on 830M tokens for this stage, and 1.4B tokens total for all stages, which is < 0.01% of Llama-3's original pre-training data. ## Large Context Window > Note: using a 256k context window requires at least 64GB of memory. Using a 1M+ context window requires significantly more (100GB+). To extend the context window (to 256k for example) use: <sub>API</sub> ``` curl http://localhost:11434/api/generate -d '{ \"model\": \"llama3-gradient\", \"prompt\": \"Why is the sky blue?\", \"options\": { \"num_ctx\": 256000 } }' ``` <sub>CLI</sub> ``` ollama run llama3-gradient >>> /set parameter num_ctx 256000 ``` ## References [Website](https://gradient.ai/) [Hugging Face](https://huggingface.co/gradientai/Llama-3-8B-Instruct-Gradient-1048k) Paste, drop or c",
      "variants": [
        {
          "tag": "llama3-gradient:latest",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "llama3-gradient:instruct",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "llama3-gradient:1048k",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "llama3-gradient:8b",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "llama3-gradient:70b",
          "size_text": "40GB",
          "size_bytes": 42949672960,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "llama3-gradient:8b-instruct-1048k-q2_K",
          "size_text": "3.2GB",
          "size_bytes": 3435973836,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "llama3-gradient:8b-instruct-1048k-q3_K_S",
          "size_text": "3.7GB",
          "size_bytes": 3972844748,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "llama3-gradient:8b-instruct-1048k-q3_K_M",
          "size_text": "4.0GB",
          "size_bytes": 4294967296,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "llama3-gradient:8b-instruct-1048k-q3_K_L",
          "size_text": "4.3GB",
          "size_bytes": 4617089843,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "llama3-gradient:8b-instruct-1048k-q4_0",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "llama3-gradient:8b-instruct-1048k-q4_1",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "llama3-gradient:8b-instruct-1048k-q4_K_S",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "llama3-gradient:8b-instruct-1048k-q4_K_M",
          "size_text": "4.9GB",
          "size_bytes": 5261334937,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "llama3-gradient:8b-instruct-1048k-q5_0",
          "size_text": "5.6GB",
          "size_bytes": 6012954214,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "llama3-gradient:8b-instruct-1048k-q5_1",
          "size_text": "6.1GB",
          "size_bytes": 6549825126,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "llama3-gradient:8b-instruct-1048k-q5_K_S",
          "size_text": "5.6GB",
          "size_bytes": 6012954214,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "llama3-gradient:8b-instruct-1048k-q5_K_M",
          "size_text": "5.7GB",
          "size_bytes": 6120328396,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "llama3-gradient:8b-instruct-1048k-q6_K",
          "size_text": "6.6GB",
          "size_bytes": 7086696038,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "llama3-gradient:8b-instruct-1048k-q8_0",
          "size_text": "8.5GB",
          "size_bytes": 9126805504,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "llama3-gradient:8b-instruct-1048k-fp16",
          "size_text": "16GB",
          "size_bytes": 17179869184,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "llama3-gradient:70b-instruct-1048k-q2_K",
          "size_text": "26GB",
          "size_bytes": 27917287424,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "llama3-gradient:70b-instruct-1048k-q3_K_S",
          "size_text": "31GB",
          "size_bytes": 33285996544,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "llama3-gradient:70b-instruct-1048k-q3_K_M",
          "size_text": "34GB",
          "size_bytes": 36507222016,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "llama3-gradient:70b-instruct-1048k-q3_K_L",
          "size_text": "37GB",
          "size_bytes": 39728447488,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "llama3-gradient:70b-instruct-1048k-q4_0",
          "size_text": "40GB",
          "size_bytes": 42949672960,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "llama3-gradient:70b-instruct-1048k-q4_1",
          "size_text": "44GB",
          "size_bytes": 47244640256,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "llama3-gradient:70b-instruct-1048k-q4_K_S",
          "size_text": "40GB",
          "size_bytes": 42949672960,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "llama3-gradient:70b-instruct-1048k-q4_K_M",
          "size_text": "43GB",
          "size_bytes": 46170898432,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "llama3-gradient:70b-instruct-1048k-q5_0",
          "size_text": "49GB",
          "size_bytes": 52613349376,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "llama3-gradient:70b-instruct-1048k-q5_1",
          "size_text": "53GB",
          "size_bytes": 56908316672,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "llama3-gradient:70b-instruct-1048k-q5_K_S",
          "size_text": "49GB",
          "size_bytes": 52613349376,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "llama3-gradient:70b-instruct-1048k-q5_K_M",
          "size_text": "50GB",
          "size_bytes": 53687091200,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "llama3-gradient:70b-instruct-1048k-q6_K",
          "size_text": "58GB",
          "size_bytes": 62277025792,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "llama3-gradient:70b-instruct-1048k-q8_0",
          "size_text": "75GB",
          "size_bytes": 80530636800,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "llama3-gradient:70b-instruct-1048k-fp16",
          "size_text": "141GB",
          "size_bytes": 151397597184,
          "context": null,
          "input": "Text"
        }
      ],
      "tags_count": 35
    },
    {
      "slug": "embeddinggemma",
      "capabilities": [
        "embedding"
      ],
      "pulls": 115000,
      "pulls_text": "115K \n  Downloads",
      "blurb": "embeddinggemma EmbeddingGemma is a 300M parameter model from Google. 300m 5 Tags Updated 1 month ago",
      "name": "EmbeddingGemma is a 300M parameter embedding model from Google.",
      "description": "Readme This model requires Ollama v0.11.10 or later EmbeddingGemma is a 300M parameter, state-of-the-art for its size, open embedding model from Google, built from Gemma 3 (with T5Gemma initialization) and the same research and technology used to create Gemini models. EmbeddingGemma produces vector representations of text, making it well-suited for search and retrieval tasks, including classification, clustering, and semantic similarity search. This model was trained with data in 100+ spoken languages. The small size and on-device focus makes it possible to deploy in environments with limited resources such as mobile phones, laptops, or desktops, democratizing access to state of the art AI models and helping foster innovation for everyone. Benchmark Training Dataset This model was trained on a dataset of text data that includes a wide variety of sources totaling approximately 320 billion tokens. Here are the key components: Web Documents : A diverse collection of web text ensures the model is exposed to a broad range of linguistic styles, topics, and vocabulary. The training dataset includes content in over 100 languages. Code and Technical Documents : Exposing the model to code and technical documentation helps it learn the structure and patterns of programming languages and specialized scientific content, which improves its understanding of code and technical questions. Synthetic and Task-Specific Data : Synthetically training data helps to teach the model specific skills. This includes curated data for tasks like information retrieval, classification, and sentiment analysis, which helps to fine-tune its performance for common embedding applications. The combination of these diverse data sources is crucial for training a powerful multilingual embedding model that can handle a wide variety of different tasks and data formats. Reference Documentation Write Preview ![image.png](/assets/library/embeddinggemma/9a20d963-4bf1-4177-9568-ca5d53a2d14e) > This model requires",
      "variants": [
        {
          "tag": "embeddinggemma:latest",
          "size_text": "622MB",
          "size_bytes": 652214272,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "embeddinggemma:300m",
          "size_text": "622MB",
          "size_bytes": 652214272,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "embeddinggemma:300m-qat-q4_0",
          "size_text": "239MB",
          "size_bytes": 250609664,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "embeddinggemma:300m-qat-q8_0",
          "size_text": "338MB",
          "size_bytes": 354418688,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "embeddinggemma:300m-bf16",
          "size_text": "622MB",
          "size_bytes": 652214272,
          "context": "2K",
          "input": "Text"
        }
      ],
      "tags_count": 5
    },
    {
      "slug": "paraphrase-multilingual",
      "capabilities": [
        "embedding"
      ],
      "pulls": 113800,
      "pulls_text": "113.8K \n  Downloads",
      "blurb": "paraphrase-multilingual Sentence-transformers model that can be used for tasks like clustering or semantic search. 278m 3 Tags Updated 1 year ago",
      "name": "Sentence-transformers model that can be used for tasks like clustering or semantic search.",
      "description": "Readme This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search. Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks @inproceedings{reimers-2019-sentence-bert, title = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\", author = \"Reimers, Nils and Gurevych, Iryna\", booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\", month = \"11\", year = \"2019\", publisher = \"Association for Computational Linguistics\", url = \"http://arxiv.org/abs/1908.10084\", } Write Preview This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search. [Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://arxiv.org/abs/1908.10084) ``` @inproceedings{reimers-2019-sentence-bert, title = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\", author = \"Reimers, Nils and Gurevych, Iryna\", booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\", month = \"11\", year = \"2019\", publisher = \"Association for Computational Linguistics\", url = \"http://arxiv.org/abs/1908.10084\", } ``` Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "paraphrase-multilingual:latest",
          "size_text": "563MB",
          "size_bytes": 590348288,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "paraphrase-multilingual:278m",
          "size_text": "563MB",
          "size_bytes": 590348288,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "paraphrase-multilingual:278m-mpnet-base-v2-fp16",
          "size_text": "563MB",
          "size_bytes": 590348288,
          "context": null,
          "input": "Text"
        }
      ],
      "tags_count": 3
    },
    {
      "slug": "dolphincoder",
      "capabilities": [],
      "pulls": 113600,
      "pulls_text": "113.6K \n  Downloads",
      "blurb": "dolphincoder A 7B and 15B uncensored variant of the Dolphin model family that excels at coding, based on StarCoder2. 7b 15b 35 Tags Updated 1 year ago",
      "name": "A 7B and 15B uncensored variant of the Dolphin model family that excels at coding, based on StarCoder2.",
      "description": "Readme Based on the StarCoder2 7B and 15B models, this Dolphin fine-tune excels at coding. Reference HuggingFace Write Preview <img src=\"https://github.com/ollama/ollama/assets/251292/6a5a5656-abb9-4235-99e0-7f1161ff2657\" width=\"320\" /> Based on the StarCoder2 7B and 15B models, this Dolphin fine-tune excels at coding. ## Reference [HuggingFace](https://huggingface.co/cognitivecomputations) Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "dolphincoder:latest",
          "size_text": "4.2GB",
          "size_bytes": 4509715660,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "dolphincoder:7b",
          "size_text": "4.2GB",
          "size_bytes": 4509715660,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "dolphincoder:15b",
          "size_text": "9.1GB",
          "size_bytes": 9771050598,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "dolphincoder:7b-starcoder2",
          "size_text": "4.2GB",
          "size_bytes": 4509715660,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "dolphincoder:7b-starcoder2-q2_K",
          "size_text": "2.9GB",
          "size_bytes": 3113851289,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "dolphincoder:7b-starcoder2-q3_K_S",
          "size_text": "3.3GB",
          "size_bytes": 3543348019,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "dolphincoder:7b-starcoder2-q3_K_M",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "dolphincoder:7b-starcoder2-q3_K_L",
          "size_text": "4.2GB",
          "size_bytes": 4509715660,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "dolphincoder:7b-starcoder2-q4_0",
          "size_text": "4.2GB",
          "size_bytes": 4509715660,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "dolphincoder:7b-starcoder2-q4_1",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "dolphincoder:7b-starcoder2-q4_K_S",
          "size_text": "4.3GB",
          "size_bytes": 4617089843,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "dolphincoder:7b-starcoder2-q4_K_M",
          "size_text": "4.6GB",
          "size_bytes": 4939212390,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "dolphincoder:7b-starcoder2-q5_0",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "dolphincoder:7b-starcoder2-q5_1",
          "size_text": "5.6GB",
          "size_bytes": 6012954214,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "dolphincoder:7b-starcoder2-q5_K_S",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "dolphincoder:7b-starcoder2-q5_K_M",
          "size_text": "5.3GB",
          "size_bytes": 5690831667,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "dolphincoder:7b-starcoder2-q6_K",
          "size_text": "6.1GB",
          "size_bytes": 6549825126,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "dolphincoder:7b-starcoder2-q8_0",
          "size_text": "7.9GB",
          "size_bytes": 8482560409,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "dolphincoder:7b-starcoder2-fp16",
          "size_text": "15GB",
          "size_bytes": 16106127360,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "dolphincoder:15b-starcoder2",
          "size_text": "9.1GB",
          "size_bytes": 9771050598,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "dolphincoder:15b-starcoder2-q2_K",
          "size_text": "6.2GB",
          "size_bytes": 6657199308,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "dolphincoder:15b-starcoder2-q3_K_S",
          "size_text": "7.0GB",
          "size_bytes": 7516192768,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "dolphincoder:15b-starcoder2-q3_K_M",
          "size_text": "8.1GB",
          "size_bytes": 8697308774,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "dolphincoder:15b-starcoder2-q3_K_L",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "dolphincoder:15b-starcoder2-q4_0",
          "size_text": "9.1GB",
          "size_bytes": 9771050598,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "dolphincoder:15b-starcoder2-q4_1",
          "size_text": "10GB",
          "size_bytes": 10737418240,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "dolphincoder:15b-starcoder2-q4_K_S",
          "size_text": "9.3GB",
          "size_bytes": 9985798963,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "dolphincoder:15b-starcoder2-q4_K_M",
          "size_text": "9.9GB",
          "size_bytes": 10630044057,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "dolphincoder:15b-starcoder2-q5_0",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "dolphincoder:15b-starcoder2-q5_1",
          "size_text": "12GB",
          "size_bytes": 12884901888,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "dolphincoder:15b-starcoder2-q5_K_S",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "dolphincoder:15b-starcoder2-q5_K_M",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "dolphincoder:15b-starcoder2-q6_K",
          "size_text": "13GB",
          "size_bytes": 13958643712,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "dolphincoder:15b-starcoder2-q8_0",
          "size_text": "17GB",
          "size_bytes": 18253611008,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "dolphincoder:15b-starcoder2-fp16",
          "size_text": "32GB",
          "size_bytes": 34359738368,
          "context": "16K",
          "input": "Text"
        }
      ],
      "tags_count": 35
    },
    {
      "slug": "dbrx",
      "capabilities": [],
      "pulls": 112200,
      "pulls_text": "112.2K \n  Downloads",
      "blurb": "dbrx DBRX is an open, general-purpose LLM created by Databricks. 132b 7 Tags Updated 1 year ago",
      "name": "DBRX is an open, general-purpose LLM created by Databricks.",
      "description": "Readme DBRX requires Ollama 0.1.32 DBRX is a transformer-based decoder-only large language model (LLM) that was trained using next-token prediction. It uses a fine-grained mixture-of-experts (MoE) architecture with 132B total parameters of which 36B parameters are active on any input. It was pre-trained on 12T tokens of text and code data. It is an especially capable code model, surpassing specialized models like CodeLLaMA-70B on programming, in addition to its strength as a general-purpose LLM. References Blog Post HuggingFace Write Preview <img src=\"https://github.com/ollama/ollama/assets/251292/582204f0-53c4-4e67-b710-7c503a6c8faa\" width=\"640\" /> > DBRX requires [Ollama 0.1.32](https://github.com/ollama/ollama/releases/tag/v0.1.32) DBRX is a transformer-based decoder-only large language model (LLM) that was trained using next-token prediction. It uses a fine-grained mixture-of-experts (MoE) architecture with 132B total parameters of which 36B parameters are active on any input. It was pre-trained on 12T tokens of text and code data. It is an especially capable code model, surpassing specialized models like CodeLLaMA-70B on programming, in addition to its strength as a general-purpose LLM. ## References [Blog Post](https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm) [HuggingFace](https://huggingface.co/databricks/dbrx-instruct) Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "dbrx:latest",
          "size_text": "74GB",
          "size_bytes": 79456894976,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dbrx:instruct",
          "size_text": "74GB",
          "size_bytes": 79456894976,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dbrx:132b",
          "size_text": "74GB",
          "size_bytes": 79456894976,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dbrx:132b-instruct-q2_K",
          "size_text": "48GB",
          "size_bytes": 51539607552,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dbrx:132b-instruct-q4_0",
          "size_text": "74GB",
          "size_bytes": 79456894976,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dbrx:132b-instruct-q8_0",
          "size_text": "140GB",
          "size_bytes": 150323855360,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "dbrx:132b-instruct-fp16",
          "size_text": "263GB",
          "size_bytes": 282394099712,
          "context": "32K",
          "input": "Text"
        }
      ],
      "tags_count": 7
    },
    {
      "slug": "samantha-mistral",
      "capabilities": [],
      "pulls": 111200,
      "pulls_text": "111.2K \n  Downloads",
      "blurb": "samantha-mistral A companion assistant trained in philosophy, psychology, and personal relationships. Based on Mistral. 7b 49 Tags Updated 2 years ago",
      "name": "A companion assistant trained in philosophy, psychology, and personal relationships. Based on Mistral.",
      "description": "Readme Meet Samantha , a conversational model created by Eric Hartford. Samantha is trained in philosophy, psychology, and personal relationships. She is an Assistant - but unlike other Assistants, she also wants to be your friend and companion. Example Prompt I would like you to be my relationship coach. I am dating again, after a long divorce. Can you please give me advice about how to gain confidence and success? Example Response Of course! Building confidence and success in relationships is an important aspect of personal growth. While there isn't a one-size-fits-all solution, here are some general tips that may help you: 1. Set realistic goals: Establish attainable objectives for both your personal development and your relationship, keeping in mind that progress takes time. 2. Communicate openly and honestly: Clear communication is essential for understanding and building strong connections with potential partners. Share your feelings, expectations, and desires while actively listening to their perspective. 3. Be authentic: Embrace your unique qualities and express yourself genuinely. Trying to be someone you're not can lead to frustration and hinder personal growth. 4. Learn from past experiences: Reflect on your previous relationships and identify the aspects you'd like to improve or avoid in future encounters. This self-awareness can help you make better choices and grow as an individual. 5. Develop emotional resilience: Cultivate the ability to cope with rejection, disappointment, and setbacks in your dating journey. Recognize these challenges as learning opportunities rather than insurmountable obstacles. 6. Focus on self-improvement: Dedicate time to enhancing your own wellbeing, such as maintaining a healthy lifestyle, cultivating hobbies or interests, and expanding your social circle. Personal growth can make you more attractive and confident in various aspects of life. 7. Be patient: Building successful relationships and personal confidence takes time,",
      "variants": [
        {
          "tag": "samantha-mistral:latest",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "samantha-mistral:7b",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "samantha-mistral:7b-instruct-q2_K",
          "size_text": "3.1GB",
          "size_bytes": 3328599654,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "samantha-mistral:7b-instruct-q3_K_S",
          "size_text": "3.2GB",
          "size_bytes": 3435973836,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "samantha-mistral:7b-instruct-q3_K_M",
          "size_text": "3.5GB",
          "size_bytes": 3758096384,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "samantha-mistral:7b-instruct-q3_K_L",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "samantha-mistral:7b-instruct-q4_0",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "samantha-mistral:7b-instruct-q4_1",
          "size_text": "4.6GB",
          "size_bytes": 4939212390,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "samantha-mistral:7b-instruct-q4_K_S",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "samantha-mistral:7b-instruct-q4_K_M",
          "size_text": "4.4GB",
          "size_bytes": 4724464025,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "samantha-mistral:7b-instruct-q5_0",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "samantha-mistral:7b-instruct-q5_1",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "samantha-mistral:7b-instruct-q5_K_S",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "samantha-mistral:7b-instruct-q5_K_M",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "samantha-mistral:7b-instruct-q6_K",
          "size_text": "5.9GB",
          "size_bytes": 6335076761,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "samantha-mistral:7b-instruct-q8_0",
          "size_text": "7.7GB",
          "size_bytes": 8267812044,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "samantha-mistral:7b-instruct-fp16",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "samantha-mistral:7b-text",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "samantha-mistral:7b-text-q2_K",
          "size_text": "3.1GB",
          "size_bytes": 3328599654,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "samantha-mistral:7b-text-q3_K_S",
          "size_text": "3.2GB",
          "size_bytes": 3435973836,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "samantha-mistral:7b-text-q3_K_M",
          "size_text": "3.5GB",
          "size_bytes": 3758096384,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "samantha-mistral:7b-text-q3_K_L",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "samantha-mistral:7b-text-q4_0",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "samantha-mistral:7b-text-q4_1",
          "size_text": "4.6GB",
          "size_bytes": 4939212390,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "samantha-mistral:7b-text-q4_K_S",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "samantha-mistral:7b-text-q4_K_M",
          "size_text": "4.4GB",
          "size_bytes": 4724464025,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "samantha-mistral:7b-text-q5_0",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "samantha-mistral:7b-text-q5_1",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "samantha-mistral:7b-text-q5_K_S",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "samantha-mistral:7b-text-q5_K_M",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "samantha-mistral:7b-text-q6_K",
          "size_text": "5.9GB",
          "size_bytes": 6335076761,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "samantha-mistral:7b-text-q8_0",
          "size_text": "7.7GB",
          "size_bytes": 8267812044,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "samantha-mistral:7b-text-fp16",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "samantha-mistral:7b-v1.2-text",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "samantha-mistral:7b-v1.2-text-q2_K",
          "size_text": "3.1GB",
          "size_bytes": 3328599654,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "samantha-mistral:7b-v1.2-text-q3_K_S",
          "size_text": "3.2GB",
          "size_bytes": 3435973836,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "samantha-mistral:7b-v1.2-text-q3_K_M",
          "size_text": "3.5GB",
          "size_bytes": 3758096384,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "samantha-mistral:7b-v1.2-text-q3_K_L",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "samantha-mistral:7b-v1.2-text-q4_0",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "samantha-mistral:7b-v1.2-text-q4_1",
          "size_text": "4.6GB",
          "size_bytes": 4939212390,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "samantha-mistral:7b-v1.2-text-q4_K_S",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "samantha-mistral:7b-v1.2-text-q4_K_M",
          "size_text": "4.4GB",
          "size_bytes": 4724464025,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "samantha-mistral:7b-v1.2-text-q5_0",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "samantha-mistral:7b-v1.2-text-q5_1",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "samantha-mistral:7b-v1.2-text-q5_K_S",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "samantha-mistral:7b-v1.2-text-q5_K_M",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "samantha-mistral:7b-v1.2-text-q6_K",
          "size_text": "5.9GB",
          "size_bytes": 6335076761,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "samantha-mistral:7b-v1.2-text-q8_0",
          "size_text": "7.7GB",
          "size_bytes": 8267812044,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "samantha-mistral:7b-v1.2-text-fp16",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "32K",
          "input": "Text"
        }
      ],
      "tags_count": 49
    },
    {
      "slug": "granite-embedding",
      "capabilities": [
        "embedding"
      ],
      "pulls": 109900,
      "pulls_text": "109.9K \n  Downloads",
      "blurb": "granite- The IBM Granite 30M and 278M models models are text-only dense biencoder models, with 30M available in English only and 278M serving multilingual use cases. 30m 278m 6 Tags Updated 10 months ago",
      "name": "The IBM Granite Embedding 30M and 278M models models are text-only dense biencoder embedding models, with 30M available in English only and 278M serving multilingual use cases.",
      "description": "Readme Granite embedding models The IBM Granite Embedding 30M and 278M models are text-only dense biencoder embedding models, with 30M available in English only and 278M serving multilingual use cases. These models are designed to produce fixed length vector representations for a given text chunk, which can be used for text similarity, retrieval, and search applications. Parameter Sizes 30M: ollama run granite-embedding:30m 278M: ollama run granite-embedding:278m Supported Languages 30M: English 278M: English, German, Spanish, French, Japanese, Portuguese, Arabic, Czech, Italian, Korean, Dutch, Chinese (Simplified) Granite dense models The Granite dense models are available in 2B and 8B parameter sizes designed to support tool-based use cases and for retrieval augmented generation (RAG), streamlining code generation, translation and bug fixing. See model page Granite mixture of experts models The Granite mixture of experts models are available in 1B and 3B parameter sizes designed for low latency usage . See model page Learn more Developers: IBM Research GitHub Repository: ibm-granite/granite-language-models Website : Granite Docs Release Date : December 18th, 2024 License: Apache 2.0 . Write Preview ## Granite embedding models The IBM Granite Embedding **30M and 278M models** are text-only dense biencoder embedding models, with **30M** available in English only and **278M** serving multilingual use cases. These models are designed to produce fixed length vector representations for a given text chunk, which can be used for text similarity, retrieval, and search applications. ### Parameter Sizes **30M:** `ollama run granite-embedding:30m` **278M:** `ollama run granite-embedding:278m` ### Supported Languages **30M:** English **278M:** English, German, Spanish, French, Japanese, Portuguese, Arabic, Czech, Italian, Korean, Dutch, Chinese (Simplified) ## Granite dense models The Granite dense models are available in **2B and 8B** parameter sizes designed to support tool-",
      "variants": [
        {
          "tag": "granite-embedding:latest",
          "size_text": "63MB",
          "size_bytes": 66060288,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "granite-embedding:30m",
          "size_text": "63MB",
          "size_bytes": 66060288,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "granite-embedding:278m",
          "size_text": "563MB",
          "size_bytes": 590348288,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "granite-embedding:30m-en",
          "size_text": "63MB",
          "size_bytes": 66060288,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "granite-embedding:30m-en-fp16",
          "size_text": "63MB",
          "size_bytes": 66060288,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "granite-embedding:278m-fp16",
          "size_text": "563MB",
          "size_bytes": 590348288,
          "context": null,
          "input": "Text"
        }
      ],
      "tags_count": 6
    },
    {
      "slug": "nemotron-mini",
      "capabilities": [
        "tools"
      ],
      "pulls": 109900,
      "pulls_text": "109.9K \n  Downloads",
      "blurb": "nemotron-mini A commercial-friendly small language model by NVIDIA optimized for roleplay, RAG QA, and function calling. 4b 17 Tags Updated 1 year ago",
      "name": "A commercial-friendly small language model by NVIDIA optimized for roleplay, RAG QA, and function calling.",
      "description": "Readme Nemotron-Mini-4B-Instruct is a model for generating responses for roleplaying, retrieval augmented generation, and function calling. It is a small language model (SLM) optimized through distillation, pruning and quantization for speed and on-device deployment. This instruct model is optimized for roleplay, RAG QA, and function calling in English. It supports a context length of 4,096 tokens. This model is ready for commercial use. References Blog HuggingFace Write Preview <img src=\"https://ollama.com/assets/library/nemotron-mini/4e0ea563-a507-45e7-ad0b-b083918ec11b\" width=\"320\" /> Nemotron-Mini-4B-Instruct is a model for generating responses for roleplaying, retrieval augmented generation, and function calling. It is a small language model (SLM) optimized through distillation, pruning and quantization for speed and on-device deployment. This instruct model is optimized for roleplay, RAG QA, and function calling in English. It supports a context length of 4,096 tokens. This model is ready for commercial use. ## References [Blog](https://blogs.nvidia.com/blog/digital-human-technology-mecha-break/) [HuggingFace](https://huggingface.co/nvidia/Nemotron-Mini-4B-Instruct) Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "nemotron-mini:latest",
          "size_text": "2.7GB",
          "size_bytes": 2899102924,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nemotron-mini:4b",
          "size_text": "2.7GB",
          "size_bytes": 2899102924,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nemotron-mini:4b-instruct-q2_K",
          "size_text": "1.9GB",
          "size_bytes": 2040109465,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nemotron-mini:4b-instruct-q3_K_S",
          "size_text": "2.1GB",
          "size_bytes": 2254857830,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nemotron-mini:4b-instruct-q3_K_M",
          "size_text": "2.3GB",
          "size_bytes": 2469606195,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nemotron-mini:4b-instruct-q3_K_L",
          "size_text": "2.5GB",
          "size_bytes": 2684354560,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nemotron-mini:4b-instruct-q4_0",
          "size_text": "2.6GB",
          "size_bytes": 2791728742,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nemotron-mini:4b-instruct-q4_1",
          "size_text": "2.8GB",
          "size_bytes": 3006477107,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nemotron-mini:4b-instruct-q4_K_S",
          "size_text": "2.6GB",
          "size_bytes": 2791728742,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nemotron-mini:4b-instruct-q4_K_M",
          "size_text": "2.7GB",
          "size_bytes": 2899102924,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nemotron-mini:4b-instruct-q5_0",
          "size_text": "3.0GB",
          "size_bytes": 3221225472,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nemotron-mini:4b-instruct-q5_1",
          "size_text": "3.2GB",
          "size_bytes": 3435973836,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nemotron-mini:4b-instruct-q5_K_S",
          "size_text": "3.0GB",
          "size_bytes": 3221225472,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nemotron-mini:4b-instruct-q5_K_M",
          "size_text": "3.1GB",
          "size_bytes": 3328599654,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nemotron-mini:4b-instruct-q6_K",
          "size_text": "3.4GB",
          "size_bytes": 3650722201,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nemotron-mini:4b-instruct-q8_0",
          "size_text": "4.5GB",
          "size_bytes": 4831838208,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nemotron-mini:4b-instruct-fp16",
          "size_text": "8.4GB",
          "size_bytes": 9019431321,
          "context": "4K",
          "input": "Text"
        }
      ],
      "tags_count": 17
    },
    {
      "slug": "tulu3",
      "capabilities": [],
      "pulls": 107700,
      "pulls_text": "107.7K \n  Downloads",
      "blurb": "tulu3 Tülu 3 is a leading instruction following model family, offering fully open-source data, code, and recipes by the The Allen Institute for AI. 8b 70b 9 Tags Updated 10 months ago",
      "name": "Tülu 3 is a leading instruction following model family, offering fully open-source data, code, and recipes by the The Allen Institute for AI.",
      "description": "Readme Llama-3.1-Tulu-3-8B Tülu3 is a leading instruction following model family, offering fully open-source data, code, and recipes designed to serve as a comprehensive guide for modern post-training techniques. Tülu3 is designed for state-of-the-art performance on a diversity of tasks in addition to chat, such as MATH, GSM8K, and IFEval. References Blog Post GitHub Paper Hugging Face Write Preview <img src=\"/assets/library/tulu3/157ae836-8188-453b-b3ad-30406919feea\" width=\"300\" /> **Llama-3.1-Tulu-3-8B** Tülu3 is a leading instruction following model family, offering fully open-source data, code, and recipes designed to serve as a comprehensive guide for modern post-training techniques. Tülu3 is designed for state-of-the-art performance on a diversity of tasks in addition to chat, such as MATH, GSM8K, and IFEval. ![1732169954-tulu-3-results.webp](/assets/library/tulu3/aba8f2ff-914e-4a25-9140-f4c1da02ad08) ## References [Blog Post](https://allenai.org/blog/tulu-3) [GitHub](https://github.com/allenai/open-instruct) [Paper](https://allenai.org/papers/tulu-3-report.pdf) [Hugging Face](https://huggingface.co/collections/allenai/tulu-3-models-673b8e0dc3512e30e7dc54f5) Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "tulu3:latest",
          "size_text": "4.9GB",
          "size_bytes": 5261334937,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "tulu3:8b",
          "size_text": "4.9GB",
          "size_bytes": 5261334937,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "tulu3:70b",
          "size_text": "43GB",
          "size_bytes": 46170898432,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "tulu3:8b-q4_K_M",
          "size_text": "4.9GB",
          "size_bytes": 5261334937,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "tulu3:8b-q8_0",
          "size_text": "8.5GB",
          "size_bytes": 9126805504,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "tulu3:8b-fp16",
          "size_text": "16GB",
          "size_bytes": 17179869184,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "tulu3:70b-q4_K_M",
          "size_text": "43GB",
          "size_bytes": 46170898432,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "tulu3:70b-q8_0",
          "size_text": "75GB",
          "size_bytes": 80530636800,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "tulu3:70b-fp16",
          "size_text": "141GB",
          "size_bytes": 151397597184,
          "context": "128K",
          "input": "Text"
        }
      ],
      "tags_count": 9
    },
    {
      "slug": "starling-lm",
      "capabilities": [],
      "pulls": 104700,
      "pulls_text": "104.7K \n  Downloads",
      "blurb": "starling-lm Starling is a large language model trained by reinforcement learning from AI feedback focused on improving chatbot helpfulness. 7b 36 Tags Updated 1 year ago",
      "name": "Starling is a large language model trained by reinforcement learning from AI feedback focused on improving chatbot helpfulness.",
      "description": "Readme Starling-7B is an open (non-commercial) large language model (LLM) trained by reinforcement learning from AI feedback. (RLAIF) The model harnesses the power of our new GPT-4 labeled ranking dataset, Nectar, and our new reward training and policy tuning pipeline. Starling-7B-alpha scores 8.09 in MT Bench with GPT-4 as a judge, outperforming every model to date on MT-Bench except for OpenAI’s GPT-4 and GPT-4 Turbo. *Based on MT Bench evaluations, using GPT-4 scoring. Further human evaluation is needed. Authors: Banghua Zhu, Evan Frick, Tianhao Wu, Hanlin Zhu and Jiantao Jiao. For correspondence, please contact Banghua Zhu (banghua@berkeley.edu). Reference Starling-7B: Increasing LLM Helpfulness & Harmlessness with RLAIF HuggingFace Write Preview <img src=\"https://github.com/jmorganca/ollama/assets/251292/ffc27067-27cc-425c-a910-a5a6f1fa2dc1\" width=\"240\"> Starling-7B is an open (non-commercial) large language model (LLM) trained by reinforcement learning from AI feedback. (RLAIF) The model harnesses the power of our new GPT-4 labeled ranking dataset, Nectar, and our new reward training and policy tuning pipeline. Starling-7B-alpha scores 8.09 in MT Bench with GPT-4 as a judge, outperforming every model to date on MT-Bench except for OpenAI’s GPT-4 and GPT-4 Turbo. <img src=\"https://github.com/jmorganca/ollama/assets/3325447/a7d89b3b-b803-4c98-bb4f-ef03f6d76064\"> *Based on MT Bench evaluations, using GPT-4 scoring. Further human evaluation is needed. Authors: Banghua Zhu, Evan Frick, Tianhao Wu, Hanlin Zhu and Jiantao Jiao. For correspondence, please contact Banghua Zhu (banghua@berkeley.edu). ## Reference [Starling-7B: Increasing LLM Helpfulness & Harmlessness with RLAIF](https://starling.cs.berkeley.edu/) [HuggingFace](https://huggingface.co/berkeley-nest/Starling-LM-7B-alpha) Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "starling-lm:latest",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starling-lm:alpha",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starling-lm:beta",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starling-lm:7b",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starling-lm:7b-alpha",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starling-lm:7b-alpha-q2_K",
          "size_text": "2.7GB",
          "size_bytes": 2899102924,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starling-lm:7b-alpha-q3_K_S",
          "size_text": "3.2GB",
          "size_bytes": 3435973836,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starling-lm:7b-alpha-q3_K_M",
          "size_text": "3.5GB",
          "size_bytes": 3758096384,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starling-lm:7b-alpha-q3_K_L",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starling-lm:7b-alpha-q4_0",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starling-lm:7b-alpha-q4_1",
          "size_text": "4.6GB",
          "size_bytes": 4939212390,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starling-lm:7b-alpha-q4_K_S",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starling-lm:7b-alpha-q4_K_M",
          "size_text": "4.4GB",
          "size_bytes": 4724464025,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starling-lm:7b-alpha-q5_0",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starling-lm:7b-alpha-q5_1",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starling-lm:7b-alpha-q5_K_S",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starling-lm:7b-alpha-q5_K_M",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starling-lm:7b-alpha-q6_K",
          "size_text": "5.9GB",
          "size_bytes": 6335076761,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starling-lm:7b-alpha-q8_0",
          "size_text": "7.7GB",
          "size_bytes": 8267812044,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starling-lm:7b-alpha-fp16",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starling-lm:7b-beta",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starling-lm:7b-beta-q2_K",
          "size_text": "2.7GB",
          "size_bytes": 2899102924,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starling-lm:7b-beta-q3_K_S",
          "size_text": "3.2GB",
          "size_bytes": 3435973836,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starling-lm:7b-beta-q3_K_M",
          "size_text": "3.5GB",
          "size_bytes": 3758096384,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starling-lm:7b-beta-q3_K_L",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starling-lm:7b-beta-q4_0",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starling-lm:7b-beta-q4_1",
          "size_text": "4.6GB",
          "size_bytes": 4939212390,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starling-lm:7b-beta-q4_K_S",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starling-lm:7b-beta-q4_K_M",
          "size_text": "4.4GB",
          "size_bytes": 4724464025,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starling-lm:7b-beta-q5_0",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starling-lm:7b-beta-q5_1",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starling-lm:7b-beta-q5_K_S",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starling-lm:7b-beta-q5_K_M",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starling-lm:7b-beta-q6_K",
          "size_text": "5.9GB",
          "size_bytes": 6335076761,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starling-lm:7b-beta-q8_0",
          "size_text": "7.7GB",
          "size_bytes": 8267812044,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "starling-lm:7b-beta-fp16",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "8K",
          "input": "Text"
        }
      ],
      "tags_count": 36
    },
    {
      "slug": "internlm2",
      "capabilities": [
        "reasoning"
      ],
      "pulls": 103300,
      "pulls_text": "103.3K \n  Downloads",
      "blurb": "internlm2 InternLM2.5 is a 7B parameter model tailored for practical scenarios with outstanding capability. 1m 1.8b 7b 20b 65 Tags Updated 1 year ago",
      "name": "InternLM2.5 is a 7B parameter model tailored for practical scenarios with outstanding reasoning capability.",
      "description": "Readme InternLM2.5 has open-sourced a 7 billion parameter base model and a chat model tailored for practical scenarios. The model has the following characteristics: Outstanding reasoning capability : State-of-the-art performance on Math reasoning, surpassing models like Llama3 and Gemma2-9B. Stronger tool use : InternLM2.5 supports gathering information from more than 100 web pages, corresponding implementation will be released in Lagent soon. InternLM2.5 has better tool utilization-related capabilities in instruction following, tool selection and reflection. Reference GitHub Hugging Face Write Preview <img src=\"https://ollama.com/assets/library/internlm2/646a4835-4904-4433-93ab-5eec0964fa12\" alt=\"internLM logo\" width=\"360\"/> InternLM2.5 has open-sourced a 7 billion parameter base model and a chat model tailored for practical scenarios. The model has the following characteristics: * **Outstanding reasoning capability**: State-of-the-art performance on Math reasoning, surpassing models like Llama3 and Gemma2-9B. * **Stronger tool use**: InternLM2.5 supports gathering information from more than 100 web pages, corresponding implementation will be released in Lagent soon. InternLM2.5 has better tool utilization-related capabilities in instruction following, tool selection and reflection. ## Reference [GitHub](https://github.com/InternLM/InternLM) [Hugging Face](https://huggingface.co/internlm/internlm2_5-7b-chat) Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "internlm2:latest",
          "size_text": "4.5GB",
          "size_bytes": 4831838208,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "internlm2:1m",
          "size_text": "4.5GB",
          "size_bytes": 4831838208,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "internlm2:1.8b",
          "size_text": "1.1GB",
          "size_bytes": 1181116006,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "internlm2:7b",
          "size_text": "4.5GB",
          "size_bytes": 4831838208,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "internlm2:20b",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "internlm2:1.8b-chat-v2.5-q2_K",
          "size_text": "772MB",
          "size_bytes": 809500672,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "internlm2:1.8b-chat-v2.5-q3_K_S",
          "size_text": "888MB",
          "size_bytes": 931135488,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "internlm2:1.8b-chat-v2.5-q3_K_M",
          "size_text": "964MB",
          "size_bytes": 1010827264,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "internlm2:1.8b-chat-v2.5-q3_K_L",
          "size_text": "1.0GB",
          "size_bytes": 1073741824,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "internlm2:1.8b-chat-v2.5-q4_0",
          "size_text": "1.1GB",
          "size_bytes": 1181116006,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "internlm2:1.8b-chat-v2.5-q4_1",
          "size_text": "1.2GB",
          "size_bytes": 1288490188,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "internlm2:1.8b-chat-v2.5-q4_K_S",
          "size_text": "1.1GB",
          "size_bytes": 1181116006,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "internlm2:1.8b-chat-v2.5-q4_K_M",
          "size_text": "1.2GB",
          "size_bytes": 1288490188,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "internlm2:1.8b-chat-v2.5-q5_0",
          "size_text": "1.3GB",
          "size_bytes": 1395864371,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "internlm2:1.8b-chat-v2.5-q5_1",
          "size_text": "1.4GB",
          "size_bytes": 1503238553,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "internlm2:1.8b-chat-v2.5-q5_K_S",
          "size_text": "1.3GB",
          "size_bytes": 1395864371,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "internlm2:1.8b-chat-v2.5-q5_K_M",
          "size_text": "1.4GB",
          "size_bytes": 1503238553,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "internlm2:1.8b-chat-v2.5-q6_K",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "internlm2:1.8b-chat-v2.5-q8_0",
          "size_text": "2.0GB",
          "size_bytes": 2147483648,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "internlm2:1.8b-chat-v2.5-fp16",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "internlm2:7b-chat-1m-v2.5-q2_K",
          "size_text": "3.0GB",
          "size_bytes": 3221225472,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "internlm2:7b-chat-1m-v2.5-q3_K_S",
          "size_text": "3.5GB",
          "size_bytes": 3758096384,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "internlm2:7b-chat-1m-v2.5-q3_K_M",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "internlm2:7b-chat-1m-v2.5-q3_K_L",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "internlm2:7b-chat-1m-v2.5-q4_0",
          "size_text": "4.5GB",
          "size_bytes": 4831838208,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "internlm2:7b-chat-1m-v2.5-q4_1",
          "size_text": "4.9GB",
          "size_bytes": 5261334937,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "internlm2:7b-chat-1m-v2.5-q4_K_S",
          "size_text": "4.5GB",
          "size_bytes": 4831838208,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "internlm2:7b-chat-1m-v2.5-q4_K_M",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "internlm2:7b-chat-1m-v2.5-q5_0",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "internlm2:7b-chat-1m-v2.5-q5_1",
          "size_text": "5.8GB",
          "size_bytes": 6227702579,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "internlm2:7b-chat-1m-v2.5-q5_K_S",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "internlm2:7b-chat-1m-v2.5-q5_K_M",
          "size_text": "5.5GB",
          "size_bytes": 5905580032,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "internlm2:7b-chat-1m-v2.5-q6_K",
          "size_text": "6.4GB",
          "size_bytes": 6871947673,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "internlm2:7b-chat-1m-v2.5-q8_0",
          "size_text": "8.2GB",
          "size_bytes": 8804682956,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "internlm2:7b-chat-1m-v2.5-fp16",
          "size_text": "15GB",
          "size_bytes": 16106127360,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "internlm2:7b-chat-v2.5-q2_K",
          "size_text": "3.0GB",
          "size_bytes": 3221225472,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "internlm2:7b-chat-v2.5-q3_K_S",
          "size_text": "3.5GB",
          "size_bytes": 3758096384,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "internlm2:7b-chat-v2.5-q3_K_M",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "internlm2:7b-chat-v2.5-q3_K_L",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "internlm2:7b-chat-v2.5-q4_0",
          "size_text": "4.5GB",
          "size_bytes": 4831838208,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "internlm2:7b-chat-v2.5-q4_1",
          "size_text": "4.9GB",
          "size_bytes": 5261334937,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "internlm2:7b-chat-v2.5-q4_K_S",
          "size_text": "4.5GB",
          "size_bytes": 4831838208,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "internlm2:7b-chat-v2.5-q4_K_M",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "internlm2:7b-chat-v2.5-q5_0",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "internlm2:7b-chat-v2.5-q5_1",
          "size_text": "5.8GB",
          "size_bytes": 6227702579,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "internlm2:7b-chat-v2.5-q5_K_S",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "internlm2:7b-chat-v2.5-q5_K_M",
          "size_text": "5.5GB",
          "size_bytes": 5905580032,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "internlm2:7b-chat-v2.5-q6_K",
          "size_text": "6.4GB",
          "size_bytes": 6871947673,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "internlm2:7b-chat-v2.5-q8_0",
          "size_text": "8.2GB",
          "size_bytes": 8804682956,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "internlm2:7b-chat-v2.5-fp16",
          "size_text": "15GB",
          "size_bytes": 16106127360,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "internlm2:20b-chat-v2.5-q2_K",
          "size_text": "7.5GB",
          "size_bytes": 8053063680,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "internlm2:20b-chat-v2.5-q3_K_S",
          "size_text": "8.8GB",
          "size_bytes": 9448928051,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "internlm2:20b-chat-v2.5-q3_K_M",
          "size_text": "9.7GB",
          "size_bytes": 10415295692,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "internlm2:20b-chat-v2.5-q3_K_L",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "internlm2:20b-chat-v2.5-q4_0",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "internlm2:20b-chat-v2.5-q4_1",
          "size_text": "13GB",
          "size_bytes": 13958643712,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "internlm2:20b-chat-v2.5-q4_K_S",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "internlm2:20b-chat-v2.5-q4_K_M",
          "size_text": "12GB",
          "size_bytes": 12884901888,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "internlm2:20b-chat-v2.5-q5_0",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "internlm2:20b-chat-v2.5-q5_1",
          "size_text": "15GB",
          "size_bytes": 16106127360,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "internlm2:20b-chat-v2.5-q5_K_S",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "internlm2:20b-chat-v2.5-q5_K_M",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "internlm2:20b-chat-v2.5-q6_K",
          "size_text": "16GB",
          "size_bytes": 17179869184,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "internlm2:20b-chat-v2.5-q8_0",
          "size_text": "21GB",
          "size_bytes": 22548578304,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "internlm2:20b-chat-v2.5-fp16",
          "size_text": "40GB",
          "size_bytes": 42949672960,
          "context": "32K",
          "input": "Text"
        }
      ],
      "tags_count": 65
    },
    {
      "slug": "phind-codellama",
      "capabilities": [],
      "pulls": 103300,
      "pulls_text": "103.3K \n  Downloads",
      "blurb": "phind-codellama Code generation model based on Code Llama. 34b 49 Tags Updated 1 year ago",
      "name": "Code generation model based on Code Llama.",
      "description": "Readme Phind CodeLlama is a code generation model based on CodeLlama 34B fine-tuned for instruct use cases. There are two versions of the model: v1 and v2 . v1 is based on CodeLlama 34B and CodeLlama-Python 34B. v2 is an iteration on v1 , trained on an additional 1.5B tokens of high-quality programming-related data. Usage CLI Open the terminal and run ollama run phind-codellama API Example curl -X POST http://localhost:11434/api/generate -d '{ \"model\": \"phind-codellama\", \"prompt\":\"Implement a linked list in C++\" }' Memory requirements 34b models generally require at least 32GB of RAM References Beating GPT-4 on HumanEval with a Fine-Tuned CodeLlama-34B HuggingFace Write Preview <img src=\"https://user-images.githubusercontent.com/633681/264120182-4a05afc9-0003-410e-8ac5-a7745dedaba8.png\" style=\"max-width:60%\" /> Phind CodeLlama is a code generation model based on CodeLlama 34B fine-tuned for instruct use cases. There are two versions of the model: `v1` and `v2`. `v1` is based on CodeLlama 34B and CodeLlama-Python 34B. `v2` is an iteration on `v1`, trained on an additional 1.5B tokens of high-quality programming-related data. ## Usage ### CLI Open the terminal and run `ollama run phind-codellama` ### API Example ```bash curl -X POST http://localhost:11434/api/generate -d '{ \"model\": \"phind-codellama\", \"prompt\":\"Implement a linked list in C++\" }' ``` ## Memory requirements - 34b models generally require at least 32GB of RAM ## References [Beating GPT-4 on HumanEval with a Fine-Tuned CodeLlama-34B](https://www.phind.com/blog/code-llama-beats-gpt4 ) [HuggingFace](https://huggingface.co/Phind) Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "phind-codellama:latest",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "phind-codellama:34b",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "phind-codellama:34b-python",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "phind-codellama:34b-python-q2_K",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "phind-codellama:34b-python-q3_K_S",
          "size_text": "15GB",
          "size_bytes": 16106127360,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "phind-codellama:34b-python-q3_K_M",
          "size_text": "16GB",
          "size_bytes": 17179869184,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "phind-codellama:34b-python-q3_K_L",
          "size_text": "18GB",
          "size_bytes": 19327352832,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "phind-codellama:34b-python-q4_0",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "phind-codellama:34b-python-q4_1",
          "size_text": "21GB",
          "size_bytes": 22548578304,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "phind-codellama:34b-python-q4_K_S",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "phind-codellama:34b-python-q4_K_M",
          "size_text": "20GB",
          "size_bytes": 21474836480,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "phind-codellama:34b-python-q5_0",
          "size_text": "23GB",
          "size_bytes": 24696061952,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "phind-codellama:34b-python-q5_1",
          "size_text": "25GB",
          "size_bytes": 26843545600,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "phind-codellama:34b-python-q5_K_S",
          "size_text": "23GB",
          "size_bytes": 24696061952,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "phind-codellama:34b-python-q5_K_M",
          "size_text": "24GB",
          "size_bytes": 25769803776,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "phind-codellama:34b-python-q6_K",
          "size_text": "28GB",
          "size_bytes": 30064771072,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "phind-codellama:34b-python-q8_0",
          "size_text": "36GB",
          "size_bytes": 38654705664,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "phind-codellama:34b-python-fp16",
          "size_text": "67GB",
          "size_bytes": 71940702208,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "phind-codellama:34b-v2",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "phind-codellama:34b-v2-q2_K",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "phind-codellama:34b-v2-q3_K_S",
          "size_text": "15GB",
          "size_bytes": 16106127360,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "phind-codellama:34b-v2-q3_K_M",
          "size_text": "16GB",
          "size_bytes": 17179869184,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "phind-codellama:34b-v2-q3_K_L",
          "size_text": "18GB",
          "size_bytes": 19327352832,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "phind-codellama:34b-v2-q4_0",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "phind-codellama:34b-v2-q4_1",
          "size_text": "21GB",
          "size_bytes": 22548578304,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "phind-codellama:34b-v2-q4_K_S",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "phind-codellama:34b-v2-q4_K_M",
          "size_text": "20GB",
          "size_bytes": 21474836480,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "phind-codellama:34b-v2-q5_0",
          "size_text": "23GB",
          "size_bytes": 24696061952,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "phind-codellama:34b-v2-q5_1",
          "size_text": "25GB",
          "size_bytes": 26843545600,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "phind-codellama:34b-v2-q5_K_S",
          "size_text": "23GB",
          "size_bytes": 24696061952,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "phind-codellama:34b-v2-q5_K_M",
          "size_text": "24GB",
          "size_bytes": 25769803776,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "phind-codellama:34b-v2-q6_K",
          "size_text": "28GB",
          "size_bytes": 30064771072,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "phind-codellama:34b-v2-q8_0",
          "size_text": "36GB",
          "size_bytes": 38654705664,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "phind-codellama:34b-v2-fp16",
          "size_text": "67GB",
          "size_bytes": 71940702208,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "phind-codellama:34b-q2_K",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "phind-codellama:34b-q3_K_S",
          "size_text": "15GB",
          "size_bytes": 16106127360,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "phind-codellama:34b-q3_K_M",
          "size_text": "16GB",
          "size_bytes": 17179869184,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "phind-codellama:34b-q3_K_L",
          "size_text": "18GB",
          "size_bytes": 19327352832,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "phind-codellama:34b-q4_0",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "phind-codellama:34b-q4_1",
          "size_text": "21GB",
          "size_bytes": 22548578304,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "phind-codellama:34b-q4_K_S",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "phind-codellama:34b-q4_K_M",
          "size_text": "20GB",
          "size_bytes": 21474836480,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "phind-codellama:34b-q5_0",
          "size_text": "23GB",
          "size_bytes": 24696061952,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "phind-codellama:34b-q5_1",
          "size_text": "25GB",
          "size_bytes": 26843545600,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "phind-codellama:34b-q5_K_S",
          "size_text": "23GB",
          "size_bytes": 24696061952,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "phind-codellama:34b-q5_K_M",
          "size_text": "24GB",
          "size_bytes": 25769803776,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "phind-codellama:34b-q6_K",
          "size_text": "28GB",
          "size_bytes": 30064771072,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "phind-codellama:34b-q8_0",
          "size_text": "36GB",
          "size_bytes": 38654705664,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "phind-codellama:34b-fp16",
          "size_text": "67GB",
          "size_bytes": 71940702208,
          "context": "16K",
          "input": "Text"
        }
      ],
      "tags_count": 49
    },
    {
      "slug": "solar",
      "capabilities": [],
      "pulls": 102500,
      "pulls_text": "102.5K \n  Downloads",
      "blurb": "solar A compact, yet powerful 10.7B large language model designed for single-turn conversation. 10.7b 32 Tags Updated 1 year ago",
      "name": "A compact, yet powerful 10.7B large language model designed for single-turn conversation.",
      "description": "Readme Solar is the first open-source 10.7 billion parameter language model. It’s compact, yet remarkably powerful, and demonstrates state-of-the-art performance in models with parameters under 30B. This model leverages the Llama 2 architecture and employs the Depth Up-Scaling technique, integrating Mistral 7B weights into upscaled layers. On the H6 benchmark, this model outperforms models with up to 30B parameters, even the Mixtral 8X7B model. References HuggingFace Upstage AI Write Preview <img src=\"https://github.com/jmorganca/ollama/assets/251292/1867051c-434e-4873-a074-3fa7bc111423\" width=\"200\" /> Solar is the first open-source 10.7 billion parameter language model. It's compact, yet remarkably powerful, and demonstrates state-of-the-art performance in models with parameters under 30B. This model leverages the Llama 2 architecture and employs the Depth Up-Scaling technique, integrating Mistral 7B weights into upscaled layers. On the H6 benchmark, this model outperforms models with up to 30B parameters, even the Mixtral 8X7B model. ## References [HuggingFace](https://huggingface.co/upstage/SOLAR-10.7B-Instruct-v1.0) [Upstage AI](https://www.upstage.ai) Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "solar:latest",
          "size_text": "6.1GB",
          "size_bytes": 6549825126,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "solar:10.7b",
          "size_text": "6.1GB",
          "size_bytes": 6549825126,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "solar:10.7b-instruct-v1-q2_K",
          "size_text": "4.5GB",
          "size_bytes": 4831838208,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "solar:10.7b-instruct-v1-q3_K_S",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "solar:10.7b-instruct-v1-q3_K_M",
          "size_text": "5.2GB",
          "size_bytes": 5583457484,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "solar:10.7b-instruct-v1-q3_K_L",
          "size_text": "5.7GB",
          "size_bytes": 6120328396,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "solar:10.7b-instruct-v1-q4_0",
          "size_text": "6.1GB",
          "size_bytes": 6549825126,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "solar:10.7b-instruct-v1-q4_1",
          "size_text": "6.7GB",
          "size_bytes": 7194070220,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "solar:10.7b-instruct-v1-q4_K_S",
          "size_text": "6.1GB",
          "size_bytes": 6549825126,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "solar:10.7b-instruct-v1-q4_K_M",
          "size_text": "6.5GB",
          "size_bytes": 6979321856,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "solar:10.7b-instruct-v1-q5_0",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "solar:10.7b-instruct-v1-q5_1",
          "size_text": "8.1GB",
          "size_bytes": 8697308774,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "solar:10.7b-instruct-v1-q5_K_S",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "solar:10.7b-instruct-v1-q5_K_M",
          "size_text": "7.6GB",
          "size_bytes": 8160437862,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "solar:10.7b-instruct-v1-q6_K",
          "size_text": "8.8GB",
          "size_bytes": 9448928051,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "solar:10.7b-instruct-v1-q8_0",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "solar:10.7b-instruct-v1-fp16",
          "size_text": "21GB",
          "size_bytes": 22548578304,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "solar:10.7b-text-v1-q2_K",
          "size_text": "4.5GB",
          "size_bytes": 4831838208,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "solar:10.7b-text-v1-q3_K_S",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "solar:10.7b-text-v1-q3_K_M",
          "size_text": "5.2GB",
          "size_bytes": 5583457484,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "solar:10.7b-text-v1-q3_K_L",
          "size_text": "5.7GB",
          "size_bytes": 6120328396,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "solar:10.7b-text-v1-q4_0",
          "size_text": "6.1GB",
          "size_bytes": 6549825126,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "solar:10.7b-text-v1-q4_1",
          "size_text": "6.7GB",
          "size_bytes": 7194070220,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "solar:10.7b-text-v1-q4_K_S",
          "size_text": "6.1GB",
          "size_bytes": 6549825126,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "solar:10.7b-text-v1-q4_K_M",
          "size_text": "6.5GB",
          "size_bytes": 6979321856,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "solar:10.7b-text-v1-q5_0",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "solar:10.7b-text-v1-q5_1",
          "size_text": "8.1GB",
          "size_bytes": 8697308774,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "solar:10.7b-text-v1-q5_K_S",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "solar:10.7b-text-v1-q5_K_M",
          "size_text": "7.6GB",
          "size_bytes": 8160437862,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "solar:10.7b-text-v1-q6_K",
          "size_text": "8.8GB",
          "size_bytes": 9448928051,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "solar:10.7b-text-v1-q8_0",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "solar:10.7b-text-v1-fp16",
          "size_text": "21GB",
          "size_bytes": 22548578304,
          "context": "4K",
          "input": "Text"
        }
      ],
      "tags_count": 32
    },
    {
      "slug": "xwinlm",
      "capabilities": [],
      "pulls": 101800,
      "pulls_text": "101.8K \n  Downloads",
      "blurb": "xwinlm Conversational model based on Llama 2 that performs competitively on various benchmarks. 7b 13b 80 Tags Updated 1 year ago",
      "name": "Conversational model based on Llama 2 that performs competitively on various benchmarks.",
      "description": "Readme Xwin-LM is a model based on Llama 2 using various techniques to improve its quality. Reference Hugging Face Write Preview <img width=\"400\" src=\"https://github.com/jmorganca/ollama/assets/251292/74015728-d0a6-4c13-bb8a-d2e180480590\"> Xwin-LM is a model based on Llama 2 using various techniques to improve its quality. ## Reference [Hugging Face](https://huggingface.co/Xwin-LM/Xwin-LM-7B-V0.2) Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "xwinlm:latest",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "xwinlm:7b",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "xwinlm:13b",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "xwinlm:7b-v0.1",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "xwinlm:7b-v0.1-q2_K",
          "size_text": "2.8GB",
          "size_bytes": 3006477107,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "xwinlm:7b-v0.1-q3_K_S",
          "size_text": "2.9GB",
          "size_bytes": 3113851289,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "xwinlm:7b-v0.1-q3_K_M",
          "size_text": "3.3GB",
          "size_bytes": 3543348019,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "xwinlm:7b-v0.1-q3_K_L",
          "size_text": "3.6GB",
          "size_bytes": 3865470566,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "xwinlm:7b-v0.1-q4_0",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "xwinlm:7b-v0.1-q4_1",
          "size_text": "4.2GB",
          "size_bytes": 4509715660,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "xwinlm:7b-v0.1-q4_K_S",
          "size_text": "3.9GB",
          "size_bytes": 4187593113,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "xwinlm:7b-v0.1-q4_K_M",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "xwinlm:7b-v0.1-q5_0",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "xwinlm:7b-v0.1-q5_1",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "xwinlm:7b-v0.1-q5_K_S",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "xwinlm:7b-v0.1-q5_K_M",
          "size_text": "4.8GB",
          "size_bytes": 5153960755,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "xwinlm:7b-v0.1-q6_K",
          "size_text": "5.5GB",
          "size_bytes": 5905580032,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "xwinlm:7b-v0.1-q8_0",
          "size_text": "7.2GB",
          "size_bytes": 7730941132,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "xwinlm:7b-v0.1-fp16",
          "size_text": "13GB",
          "size_bytes": 13958643712,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "xwinlm:7b-v0.2",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "xwinlm:7b-v0.2-q2_K",
          "size_text": "2.8GB",
          "size_bytes": 3006477107,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "xwinlm:7b-v0.2-q3_K_S",
          "size_text": "2.9GB",
          "size_bytes": 3113851289,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "xwinlm:7b-v0.2-q3_K_L",
          "size_text": "3.6GB",
          "size_bytes": 3865470566,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "xwinlm:7b-v0.2-q4_0",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "xwinlm:7b-v0.2-q4_1",
          "size_text": "4.2GB",
          "size_bytes": 4509715660,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "xwinlm:7b-v0.2-q4_K_S",
          "size_text": "3.9GB",
          "size_bytes": 4187593113,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "xwinlm:7b-v0.2-q4_K_M",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "xwinlm:7b-v0.2-q5_0",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "xwinlm:7b-v0.2-q5_K_S",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "xwinlm:7b-v0.2-q5_K_M",
          "size_text": "4.8GB",
          "size_bytes": 5153960755,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "xwinlm:7b-v0.2-q6_K",
          "size_text": "5.5GB",
          "size_bytes": 5905580032,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "xwinlm:7b-v0.2-q8_0",
          "size_text": "7.2GB",
          "size_bytes": 7730941132,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "xwinlm:7b-v0.2-fp16",
          "size_text": "13GB",
          "size_bytes": 13958643712,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "xwinlm:13b-v0.1",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "xwinlm:13b-v0.1-q2_K",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "xwinlm:13b-v0.1-q3_K_S",
          "size_text": "5.7GB",
          "size_bytes": 6120328396,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "xwinlm:13b-v0.1-q3_K_M",
          "size_text": "6.3GB",
          "size_bytes": 6764573491,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "xwinlm:13b-v0.1-q3_K_L",
          "size_text": "6.9GB",
          "size_bytes": 7408818585,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "xwinlm:13b-v0.1-q4_0",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "xwinlm:13b-v0.1-q4_1",
          "size_text": "8.2GB",
          "size_bytes": 8804682956,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "xwinlm:13b-v0.1-q4_K_S",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "xwinlm:13b-v0.1-q4_K_M",
          "size_text": "7.9GB",
          "size_bytes": 8482560409,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "xwinlm:13b-v0.1-q5_0",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "xwinlm:13b-v0.1-q5_1",
          "size_text": "9.8GB",
          "size_bytes": 10522669875,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "xwinlm:13b-v0.1-q5_K_S",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "xwinlm:13b-v0.1-q5_K_M",
          "size_text": "9.2GB",
          "size_bytes": 9878424780,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "xwinlm:13b-v0.1-q6_K",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "xwinlm:13b-v0.1-q8_0",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "xwinlm:13b-v0.1-fp16",
          "size_text": "26GB",
          "size_bytes": 27917287424,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "xwinlm:13b-v0.2",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "xwinlm:13b-v0.2-q2_K",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "xwinlm:13b-v0.2-q3_K_S",
          "size_text": "5.7GB",
          "size_bytes": 6120328396,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "xwinlm:13b-v0.2-q3_K_M",
          "size_text": "6.3GB",
          "size_bytes": 6764573491,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "xwinlm:13b-v0.2-q3_K_L",
          "size_text": "6.9GB",
          "size_bytes": 7408818585,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "xwinlm:13b-v0.2-q4_0",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "xwinlm:13b-v0.2-q4_1",
          "size_text": "8.2GB",
          "size_bytes": 8804682956,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "xwinlm:13b-v0.2-q4_K_S",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "xwinlm:13b-v0.2-q4_K_M",
          "size_text": "7.9GB",
          "size_bytes": 8482560409,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "xwinlm:13b-v0.2-q5_0",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "xwinlm:13b-v0.2-q5_1",
          "size_text": "9.8GB",
          "size_bytes": 10522669875,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "xwinlm:13b-v0.2-q5_K_S",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "xwinlm:13b-v0.2-q5_K_M",
          "size_text": "9.2GB",
          "size_bytes": 9878424780,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "xwinlm:13b-v0.2-q6_K",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "xwinlm:13b-v0.2-q8_0",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "xwinlm:13b-v0.2-fp16",
          "size_text": "26GB",
          "size_bytes": 27917287424,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "xwinlm:70b-v0.1",
          "size_text": "39GB",
          "size_bytes": 41875931136,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "xwinlm:70b-v0.1-q2_K",
          "size_text": "29GB",
          "size_bytes": 31138512896,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "xwinlm:70b-v0.1-q3_K_S",
          "size_text": "30GB",
          "size_bytes": 32212254720,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "xwinlm:70b-v0.1-q3_K_M",
          "size_text": "33GB",
          "size_bytes": 35433480192,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "xwinlm:70b-v0.1-q3_K_L",
          "size_text": "36GB",
          "size_bytes": 38654705664,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "xwinlm:70b-v0.1-q4_0",
          "size_text": "39GB",
          "size_bytes": 41875931136,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "xwinlm:70b-v0.1-q4_1",
          "size_text": "43GB",
          "size_bytes": 46170898432,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "xwinlm:70b-v0.1-q4_K_S",
          "size_text": "39GB",
          "size_bytes": 41875931136,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "xwinlm:70b-v0.1-q4_K_M",
          "size_text": "41GB",
          "size_bytes": 44023414784,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "xwinlm:70b-v0.1-q5_0",
          "size_text": "47GB",
          "size_bytes": 50465865728,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "xwinlm:70b-v0.1-q5_1",
          "size_text": "52GB",
          "size_bytes": 55834574848,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "xwinlm:70b-v0.1-q5_K_S",
          "size_text": "47GB",
          "size_bytes": 50465865728,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "xwinlm:70b-v0.1-q6_K",
          "size_text": "57GB",
          "size_bytes": 61203283968,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "xwinlm:70b-v0.1-q8_0",
          "size_text": "73GB",
          "size_bytes": 78383153152,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "xwinlm:70b-v0.1-fp16",
          "size_text": "138GB",
          "size_bytes": 148176371712,
          "context": "4K",
          "input": "Text"
        }
      ],
      "tags_count": 80
    },
    {
      "slug": "qwen3-embedding",
      "capabilities": [
        "embedding",
        "reasoning"
      ],
      "pulls": 101200,
      "pulls_text": "101.2K \n  Downloads",
      "blurb": "qwen3- Building upon the foundational models of the Qwen3 series, Qwen3 provides a comprehensive range of text embeddings models in various sizes 0.6b 4b 8b 12 Tags Updated 1 month ago",
      "name": "Building upon the foundational models of the Qwen3 series, Qwen3 Embedding provides a comprehensive range of text embeddings models in various sizes",
      "description": "Readme Highlights The Qwen3 Embedding model series is specifically designed for text embedding tasks. Building upon the dense foundational models of the Qwen3 series, it provides a comprehensive range of text embeddings models in various sizes (0.6B, 4B, and 8B). This series inherits the exceptional multilingual capabilities, long-text understanding, and reasoning skills of its foundational model. The Qwen3 Embedding series represents significant advancements in multiple text embedding and ranking tasks, including text retrieval, code retrieval, text classification, text clustering, and bitext mining. Exceptional Versatility : The embedding model has achieved state-of-the-art performance across a wide range of downstream application evaluations. The 8B size embedding model ranks No.1 in the MTEB multilingual leaderboard (as of June 5, 2025, score 70.58 ). Comprehensive Flexibility : The Qwen3 Embedding series offers a full spectrum of sizes (from 0.6B to 8B) for embedding models, catering to diverse use cases that prioritize efficiency and effectiveness. Developers can seamlessly combine these two modules. Additionally, these models allow for flexible vector definitions across all dimensions, support user-defined instructions to enhance performance for specific tasks, languages, or scenarios. Multilingual Capability : The Qwen3 Embedding series offer support for over 100 languages, thanks to the multilingual capabilites of Qwen3 models. This includes various programming languages, and provides robust multilingual, cross-lingual, and code retrieval capabilities. Qwen3-Embedding-8B has the following features: Model Type: Text Embedding Supported Languages: 100+ Languages Number of Paramaters: 8B Context Length: 32k Embedding Dimension: Up to 4096, supports user-defined output dimensions ranging from 32 to 4096 For more details, including benchmark evaluation, hardware requirements, and inference performance, please refer to the model’s blog , GitHub . Write Preview <i",
      "variants": [
        {
          "tag": "qwen3-embedding:latest",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "40K",
          "input": "Text"
        },
        {
          "tag": "qwen3-embedding:0.6b",
          "size_text": "639MB",
          "size_bytes": 670040064,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen3-embedding:4b",
          "size_text": "2.5GB",
          "size_bytes": 2684354560,
          "context": "40K",
          "input": "Text"
        },
        {
          "tag": "qwen3-embedding:8b",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "40K",
          "input": "Text"
        },
        {
          "tag": "qwen3-embedding:0.6b-q8_0",
          "size_text": "639MB",
          "size_bytes": 670040064,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen3-embedding:0.6b-fp16",
          "size_text": "1.2GB",
          "size_bytes": 1288490188,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen3-embedding:4b-q4_K_M",
          "size_text": "2.5GB",
          "size_bytes": 2684354560,
          "context": "40K",
          "input": "Text"
        },
        {
          "tag": "qwen3-embedding:4b-q8_0",
          "size_text": "4.3GB",
          "size_bytes": 4617089843,
          "context": "40K",
          "input": "Text"
        },
        {
          "tag": "qwen3-embedding:4b-fp16",
          "size_text": "8.0GB",
          "size_bytes": 8589934592,
          "context": "40K",
          "input": "Text"
        },
        {
          "tag": "qwen3-embedding:8b-q4_K_M",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "40K",
          "input": "Text"
        },
        {
          "tag": "qwen3-embedding:8b-q8_0",
          "size_text": "8.0GB",
          "size_bytes": 8589934592,
          "context": "40K",
          "input": "Text"
        },
        {
          "tag": "qwen3-embedding:8b-fp16",
          "size_text": "15GB",
          "size_bytes": 16106127360,
          "context": "40K",
          "input": "Text"
        }
      ],
      "tags_count": 12
    },
    {
      "slug": "athene-v2",
      "capabilities": [
        "tools"
      ],
      "pulls": 100600,
      "pulls_text": "100.6K \n  Downloads",
      "blurb": "athene-v2 Athene-V2 is a 72B parameter model which excels at code completion, mathematics, and log extraction tasks. 72b 17 Tags Updated 11 months ago",
      "name": "Athene-V2 is a 72B parameter model which excels at code completion, mathematics, and log extraction tasks.",
      "description": "Readme Athene-V2 Nexusflow’s Athene-V2 chat model, built on Qwen 2.5’s 72B foundation, achieves GPT-4o-level performance across key benchmarks while demonstrating how targeted optimization can enhance specific capabilities beyond traditional scaling approaches. Model Features 72B parameters fine-tuned from Qwen 2.5 State-of-the-art chat performance matching or exceeding GPT-4o Superior code completion (ranking #2 on bigcode-bench-hard) Enhanced mathematics capabilities (MATH benchmark) Precise long-form log extraction Advanced post-training pipeline pushing the Pareto frontier References Blog post HuggingFace Write Preview # Athene-V2 Nexusflow's Athene-V2 chat model, built on Qwen 2.5's 72B foundation, achieves GPT-4o-level performance across key benchmarks while demonstrating how targeted optimization can enhance specific capabilities beyond traditional scaling approaches. ## Model Features - **72B parameters** fine-tuned from Qwen 2.5 - **State-of-the-art chat performance** matching or exceeding GPT-4o - **Superior code completion** (ranking #2 on bigcode-bench-hard) - **Enhanced mathematics capabilities** (MATH benchmark) - **Precise long-form log extraction** - **Advanced post-training pipeline** pushing the Pareto frontier <img src=\"https://ollama.com/assets/library/athene-v2/99424f56-1e23-4f31-8dfa-4fc46eb6305f\" width=\"460\" /> ## References [Blog post](https://nexusflow.ai/blogs/athene-v2) [HuggingFace](https://huggingface.co/Nexusflow/Athene-V2-Chat) Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "athene-v2:latest",
          "size_text": "47GB",
          "size_bytes": 50465865728,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "athene-v2:72b",
          "size_text": "47GB",
          "size_bytes": 50465865728,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "athene-v2:72b-q2_K",
          "size_text": "30GB",
          "size_bytes": 32212254720,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "athene-v2:72b-q3_K_S",
          "size_text": "34GB",
          "size_bytes": 36507222016,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "athene-v2:72b-q3_K_M",
          "size_text": "38GB",
          "size_bytes": 40802189312,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "athene-v2:72b-q3_K_L",
          "size_text": "40GB",
          "size_bytes": 42949672960,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "athene-v2:72b-q4_0",
          "size_text": "41GB",
          "size_bytes": 44023414784,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "athene-v2:72b-q4_1",
          "size_text": "46GB",
          "size_bytes": 49392123904,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "athene-v2:72b-q4_K_S",
          "size_text": "44GB",
          "size_bytes": 47244640256,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "athene-v2:72b-q4_K_M",
          "size_text": "47GB",
          "size_bytes": 50465865728,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "athene-v2:72b-q5_0",
          "size_text": "50GB",
          "size_bytes": 53687091200,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "athene-v2:72b-q5_1",
          "size_text": "55GB",
          "size_bytes": 59055800320,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "athene-v2:72b-q5_K_S",
          "size_text": "51GB",
          "size_bytes": 54760833024,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "athene-v2:72b-q5_K_M",
          "size_text": "54GB",
          "size_bytes": 57982058496,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "athene-v2:72b-q6_K",
          "size_text": "64GB",
          "size_bytes": 68719476736,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "athene-v2:72b-q8_0",
          "size_text": "77GB",
          "size_bytes": 82678120448,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "athene-v2:72b-fp16",
          "size_text": "145GB",
          "size_bytes": 155692564480,
          "context": "32K",
          "input": "Text"
        }
      ],
      "tags_count": 17
    },
    {
      "slug": "llama3-groq-tool-use",
      "capabilities": [
        "tools"
      ],
      "pulls": 100500,
      "pulls_text": "100.5K \n  Downloads",
      "blurb": "llama3-groq-tool-use A series of models from Groq that represent a significant advancement in open-source AI capabilities for tool use/function calling. 8b 70b 33 Tags Updated 1 year ago",
      "name": "A series of models from Groq that represent a significant advancement in open-source AI capabilities for tool use/function calling.",
      "description": "Readme These models, developed in collaboration with Glaive , represent a significant advancement in open-source AI capabilities for tool use/function calling. Benchmark Results These models have achieved remarkable results, setting new benchmarks for Large Language Models with tool use capabilities: Llama-3-Groq-70B-Tool-Use: 90.76% overall accuracy (#1 on BFCL at the time of publishing - July 2024) Llama-3-Groq-8B-Tool-Use: 89.06% overall accuracy (#3 on BFCL at the time of publishing - July 2024) References Hugging Face Blog Write Preview <img src=\"https://ollama.com/assets/library/llama3-groq-tool-use/ebf53e82-1faf-4bac-84b0-47b8f5d9d8d1\" width=\"240\" /> These models, developed in collaboration with [Glaive](https://glaive.ai/), represent a significant advancement in open-source AI capabilities for tool use/function calling. ## Benchmark Results These models have achieved remarkable results, setting new benchmarks for Large Language Models with tool use capabilities: * Llama-3-Groq-70B-Tool-Use: 90.76% overall accuracy (#1 on BFCL at the time of publishing - July 2024) * Llama-3-Groq-8B-Tool-Use: 89.06% overall accuracy (#3 on BFCL at the time of publishing - July 2024) ## References [Hugging Face](https://huggingface.co/Groq) [Blog](https://wow.groq.com/introducing-llama-3-groq-tool-use-models/) Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "llama3-groq-tool-use:latest",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3-groq-tool-use:8b",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3-groq-tool-use:70b",
          "size_text": "40GB",
          "size_bytes": 42949672960,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3-groq-tool-use:8b-q2_K",
          "size_text": "3.2GB",
          "size_bytes": 3435973836,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3-groq-tool-use:8b-q3_K_S",
          "size_text": "3.7GB",
          "size_bytes": 3972844748,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3-groq-tool-use:8b-q3_K_M",
          "size_text": "4.0GB",
          "size_bytes": 4294967296,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3-groq-tool-use:8b-q3_K_L",
          "size_text": "4.3GB",
          "size_bytes": 4617089843,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3-groq-tool-use:8b-q4_0",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3-groq-tool-use:8b-q4_1",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3-groq-tool-use:8b-q4_K_S",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3-groq-tool-use:8b-q4_K_M",
          "size_text": "4.9GB",
          "size_bytes": 5261334937,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3-groq-tool-use:8b-q5_0",
          "size_text": "5.6GB",
          "size_bytes": 6012954214,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3-groq-tool-use:8b-q5_1",
          "size_text": "6.1GB",
          "size_bytes": 6549825126,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3-groq-tool-use:8b-q5_K_S",
          "size_text": "5.6GB",
          "size_bytes": 6012954214,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3-groq-tool-use:8b-q5_K_M",
          "size_text": "5.7GB",
          "size_bytes": 6120328396,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3-groq-tool-use:8b-q6_K",
          "size_text": "6.6GB",
          "size_bytes": 7086696038,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3-groq-tool-use:8b-q8_0",
          "size_text": "8.5GB",
          "size_bytes": 9126805504,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3-groq-tool-use:8b-fp16",
          "size_text": "16GB",
          "size_bytes": 17179869184,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3-groq-tool-use:70b-q2_K",
          "size_text": "26GB",
          "size_bytes": 27917287424,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3-groq-tool-use:70b-q3_K_S",
          "size_text": "31GB",
          "size_bytes": 33285996544,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3-groq-tool-use:70b-q3_K_M",
          "size_text": "34GB",
          "size_bytes": 36507222016,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3-groq-tool-use:70b-q3_K_L",
          "size_text": "37GB",
          "size_bytes": 39728447488,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3-groq-tool-use:70b-q4_0",
          "size_text": "40GB",
          "size_bytes": 42949672960,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3-groq-tool-use:70b-q4_1",
          "size_text": "44GB",
          "size_bytes": 47244640256,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3-groq-tool-use:70b-q4_K_S",
          "size_text": "40GB",
          "size_bytes": 42949672960,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3-groq-tool-use:70b-q4_K_M",
          "size_text": "43GB",
          "size_bytes": 46170898432,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3-groq-tool-use:70b-q5_0",
          "size_text": "49GB",
          "size_bytes": 52613349376,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3-groq-tool-use:70b-q5_1",
          "size_text": "53GB",
          "size_bytes": 56908316672,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3-groq-tool-use:70b-q5_K_S",
          "size_text": "49GB",
          "size_bytes": 52613349376,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3-groq-tool-use:70b-q5_K_M",
          "size_text": "50GB",
          "size_bytes": 53687091200,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3-groq-tool-use:70b-q6_K",
          "size_text": "58GB",
          "size_bytes": 62277025792,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3-groq-tool-use:70b-q8_0",
          "size_text": "75GB",
          "size_bytes": 80530636800,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3-groq-tool-use:70b-fp16",
          "size_text": "141GB",
          "size_bytes": 151397597184,
          "context": "8K",
          "input": "Text"
        }
      ],
      "tags_count": 33
    },
    {
      "slug": "nemotron",
      "capabilities": [
        "tools"
      ],
      "pulls": 97900,
      "pulls_text": "97.9K \n  Downloads",
      "blurb": "nemotron Llama-3.1-Nemotron-70B-Instruct is a large language model customized by NVIDIA to improve the helpfulness of LLM generated responses to user queries. 70b 17 Tags Updated 1 year ago",
      "name": "Llama-3.1-Nemotron-70B-Instruct is a large language model customized by NVIDIA to improve the helpfulness of LLM generated responses to user queries.",
      "description": "Readme Llama-3.1-Nemotron-70B-Instruct is a large language model customized by NVIDIA to improve the helpfulness of LLM generated responses to user queries. This model was trained using RLHF (specifically, REINFORCE), Llama-3.1-Nemotron-70B-Reward and HelpSteer2-Preference prompts on a Llama-3.1-70B-Instruct model as the initial policy. $ ollama run nemotron >>> How many r in strawberry? A sweet question! Let's count the \"R\"s in \"strawberry\": 1. S 2. T 3. R 4. A 5. W 6. B 7. E 8. R 9. R 10. Y There are **3** \"R\"s in the word \"strawberry\". Reference Hugging Face Write Preview <img src=\"https://ollama.com/assets/library/nemotron-mini/4e0ea563-a507-45e7-ad0b-b083918ec11b\" width=\"320\" /> Llama-3.1-Nemotron-70B-Instruct is a large language model customized by NVIDIA to improve the helpfulness of LLM generated responses to user queries. This model was trained using RLHF (specifically, REINFORCE), [Llama-3.1-Nemotron-70B-Reward](https://huggingface.co/nvidia/Llama-3.1-Nemotron-70B-Reward) and [HelpSteer2-Preference prompts](https://huggingface.co/datasets/nvidia/HelpSteer2) on a [Llama-3.1-70B-Instruct](https://ai.meta.com/blog/meta-llama-3-1/) model as the initial policy. ``` $ ollama run nemotron >>> How many r in strawberry? A sweet question! Let's count the \"R\"s in \"strawberry\": 1. S 2. T 3. R 4. A 5. W 6. B 7. E 8. R 9. R 10. Y There are **3** \"R\"s in the word \"strawberry\". ``` ## Reference [Hugging Face](https://huggingface.co/nvidia/Llama-3.1-Nemotron-70B-Instruct-HF) Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "nemotron:latest",
          "size_text": "43GB",
          "size_bytes": 46170898432,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "nemotron:70b",
          "size_text": "43GB",
          "size_bytes": 46170898432,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "nemotron:70b-instruct-q2_K",
          "size_text": "26GB",
          "size_bytes": 27917287424,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "nemotron:70b-instruct-q3_K_S",
          "size_text": "31GB",
          "size_bytes": 33285996544,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "nemotron:70b-instruct-q3_K_M",
          "size_text": "34GB",
          "size_bytes": 36507222016,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "nemotron:70b-instruct-q3_K_L",
          "size_text": "37GB",
          "size_bytes": 39728447488,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "nemotron:70b-instruct-q4_0",
          "size_text": "40GB",
          "size_bytes": 42949672960,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "nemotron:70b-instruct-q4_1",
          "size_text": "44GB",
          "size_bytes": 47244640256,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "nemotron:70b-instruct-q4_K_S",
          "size_text": "40GB",
          "size_bytes": 42949672960,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "nemotron:70b-instruct-q4_K_M",
          "size_text": "43GB",
          "size_bytes": 46170898432,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "nemotron:70b-instruct-q5_0",
          "size_text": "49GB",
          "size_bytes": 52613349376,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "nemotron:70b-instruct-q5_1",
          "size_text": "53GB",
          "size_bytes": 56908316672,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "nemotron:70b-instruct-q5_K_S",
          "size_text": "49GB",
          "size_bytes": 52613349376,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "nemotron:70b-instruct-q5_K_M",
          "size_text": "50GB",
          "size_bytes": 53687091200,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "nemotron:70b-instruct-q6_K",
          "size_text": "58GB",
          "size_bytes": 62277025792,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "nemotron:70b-instruct-q8_0",
          "size_text": "75GB",
          "size_bytes": 80530636800,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "nemotron:70b-instruct-fp16",
          "size_text": "141GB",
          "size_bytes": 151397597184,
          "context": "128K",
          "input": "Text"
        }
      ],
      "tags_count": 17
    },
    {
      "slug": "yarn-llama2",
      "capabilities": [],
      "pulls": 95200,
      "pulls_text": "95.2K \n  Downloads",
      "blurb": "yarn-llama2 An extension of Llama 2 that supports a context of up to 128k tokens. 7b 13b 67 Tags Updated 1 year ago",
      "name": "An extension of Llama 2 that supports a context of up to 128k tokens.",
      "description": "Readme Yarn Llama 2 is a model based on Llama2 that extends its context size up to 128k context. It is developed by Nous Research by implementing the YaRN method to further train the model to support larger context windows. CLI 64k context size: ollama run yarn-llama2 128k context size: ollama run yarn-llama2:7b-128k API Example: curl -X POST http://localhost:11434/api/generate -d '{ \"model\": \"yarn-llama2:7b-128k\", \"prompt\":\"Here is a story about llamas eating grass\" }' References Hugging Face YaRN: Efficient Context Window Extension of Large Language Models Write Preview Yarn Llama 2 is a model based on [Llama2](https://ollama.ai/library/llama2) that extends its context size up to 128k context. It is developed by [Nous Research](https://nousresearch.com) by implementing the YaRN method to further train the model to support larger context windows. ## CLI 64k context size: ``` ollama run yarn-llama2 ``` 128k context size: ``` ollama run yarn-llama2:7b-128k ``` ## API Example: ``` curl -X POST http://localhost:11434/api/generate -d '{ \"model\": \"yarn-llama2:7b-128k\", \"prompt\":\"Here is a story about llamas eating grass\" }' ``` ## References [Hugging Face](https://huggingface.co/NousResearch/Yarn-Llama-2-7b-64k) [YaRN: Efficient Context Window Extension of Large Language Models](https://arxiv.org/pdf/2309.00071.pdf) Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "yarn-llama2:latest",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "yarn-llama2:7b",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "yarn-llama2:13b",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "yarn-llama2:7b-128k",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yarn-llama2:7b-128k-q2_K",
          "size_text": "2.8GB",
          "size_bytes": 3006477107,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yarn-llama2:7b-128k-q3_K_S",
          "size_text": "2.9GB",
          "size_bytes": 3113851289,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yarn-llama2:7b-128k-q3_K_M",
          "size_text": "3.3GB",
          "size_bytes": 3543348019,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yarn-llama2:7b-128k-q3_K_L",
          "size_text": "3.6GB",
          "size_bytes": 3865470566,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yarn-llama2:7b-128k-q4_0",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yarn-llama2:7b-128k-q4_1",
          "size_text": "4.2GB",
          "size_bytes": 4509715660,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yarn-llama2:7b-128k-q4_K_S",
          "size_text": "3.9GB",
          "size_bytes": 4187593113,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yarn-llama2:7b-128k-q4_K_M",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yarn-llama2:7b-128k-q5_0",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yarn-llama2:7b-128k-q5_1",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yarn-llama2:7b-128k-q5_K_S",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yarn-llama2:7b-128k-q5_K_M",
          "size_text": "4.8GB",
          "size_bytes": 5153960755,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yarn-llama2:7b-128k-q6_K",
          "size_text": "5.5GB",
          "size_bytes": 5905580032,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yarn-llama2:7b-128k-q8_0",
          "size_text": "7.2GB",
          "size_bytes": 7730941132,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yarn-llama2:7b-128k-fp16",
          "size_text": "13GB",
          "size_bytes": 13958643712,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yarn-llama2:7b-64k",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "yarn-llama2:7b-64k-q2_K",
          "size_text": "2.8GB",
          "size_bytes": 3006477107,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "yarn-llama2:7b-64k-q3_K_S",
          "size_text": "2.9GB",
          "size_bytes": 3113851289,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "yarn-llama2:7b-64k-q3_K_M",
          "size_text": "3.3GB",
          "size_bytes": 3543348019,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "yarn-llama2:7b-64k-q3_K_L",
          "size_text": "3.6GB",
          "size_bytes": 3865470566,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "yarn-llama2:7b-64k-q4_0",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "yarn-llama2:7b-64k-q4_1",
          "size_text": "4.2GB",
          "size_bytes": 4509715660,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "yarn-llama2:7b-64k-q4_K_S",
          "size_text": "3.9GB",
          "size_bytes": 4187593113,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "yarn-llama2:7b-64k-q4_K_M",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "yarn-llama2:7b-64k-q5_0",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "yarn-llama2:7b-64k-q5_1",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "yarn-llama2:7b-64k-q5_K_S",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "yarn-llama2:7b-64k-q5_K_M",
          "size_text": "4.8GB",
          "size_bytes": 5153960755,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "yarn-llama2:7b-64k-q6_K",
          "size_text": "5.5GB",
          "size_bytes": 5905580032,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "yarn-llama2:7b-64k-q8_0",
          "size_text": "7.2GB",
          "size_bytes": 7730941132,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "yarn-llama2:7b-64k-fp16",
          "size_text": "13GB",
          "size_bytes": 13958643712,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "yarn-llama2:13b-128k",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yarn-llama2:13b-128k-q2_K",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yarn-llama2:13b-128k-q3_K_S",
          "size_text": "5.7GB",
          "size_bytes": 6120328396,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yarn-llama2:13b-128k-q3_K_M",
          "size_text": "6.3GB",
          "size_bytes": 6764573491,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yarn-llama2:13b-128k-q3_K_L",
          "size_text": "6.9GB",
          "size_bytes": 7408818585,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yarn-llama2:13b-128k-q4_0",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yarn-llama2:13b-128k-q4_1",
          "size_text": "8.2GB",
          "size_bytes": 8804682956,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yarn-llama2:13b-128k-q4_K_S",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yarn-llama2:13b-128k-q4_K_M",
          "size_text": "7.9GB",
          "size_bytes": 8482560409,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yarn-llama2:13b-128k-q5_0",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yarn-llama2:13b-128k-q5_1",
          "size_text": "9.8GB",
          "size_bytes": 10522669875,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yarn-llama2:13b-128k-q5_K_S",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yarn-llama2:13b-128k-q5_K_M",
          "size_text": "9.2GB",
          "size_bytes": 9878424780,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yarn-llama2:13b-128k-q6_K",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yarn-llama2:13b-128k-q8_0",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yarn-llama2:13b-128k-fp16",
          "size_text": "26GB",
          "size_bytes": 27917287424,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yarn-llama2:13b-64k",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "yarn-llama2:13b-64k-q2_K",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "yarn-llama2:13b-64k-q3_K_S",
          "size_text": "5.7GB",
          "size_bytes": 6120328396,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "yarn-llama2:13b-64k-q3_K_M",
          "size_text": "6.3GB",
          "size_bytes": 6764573491,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "yarn-llama2:13b-64k-q3_K_L",
          "size_text": "6.9GB",
          "size_bytes": 7408818585,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "yarn-llama2:13b-64k-q4_0",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "yarn-llama2:13b-64k-q4_1",
          "size_text": "8.2GB",
          "size_bytes": 8804682956,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "yarn-llama2:13b-64k-q4_K_S",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "yarn-llama2:13b-64k-q4_K_M",
          "size_text": "7.9GB",
          "size_bytes": 8482560409,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "yarn-llama2:13b-64k-q5_0",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "yarn-llama2:13b-64k-q5_1",
          "size_text": "9.8GB",
          "size_bytes": 10522669875,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "yarn-llama2:13b-64k-q5_K_S",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "yarn-llama2:13b-64k-q5_K_M",
          "size_text": "9.2GB",
          "size_bytes": 9878424780,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "yarn-llama2:13b-64k-q6_K",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "yarn-llama2:13b-64k-q8_0",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "yarn-llama2:13b-64k-fp16",
          "size_text": "26GB",
          "size_bytes": 27917287424,
          "context": "64K",
          "input": "Text"
        }
      ],
      "tags_count": 67
    },
    {
      "slug": "meditron",
      "capabilities": [
        "reasoning"
      ],
      "pulls": 94200,
      "pulls_text": "94.2K \n  Downloads",
      "blurb": "meditron Open-source medical large language model adapted from Llama 2 to the medical domain. 7b 70b 22 Tags Updated 1 year ago",
      "name": "Open-source medical large language model adapted from Llama 2 to the medical domain.",
      "description": "Readme Meditron is a large language model adapted from Llama 2 to the medical domain through training on a corpus of medical data, papers and guidelines. It outperforms Llama 2, GPT 3.5 and Flan-PaLM on many medical reasoning tasks. Potential use cases include: Medical exam question answering Supporting differential diagnosis Disease information (symptoms, cause, treatment) query General health information query Example prompts What are the symptoms of the common cold? What causes the seasonal flu? What medication would be prescribed for a headache? References MEDITRON-70B: Scaling Medical Pretraining for Large Language Models GitHub HuggingFace Write Preview <img src=\"https://github.com/jmorganca/ollama/assets/251292/0ba1061e-ddaf-4b3f-abd3-684a31d19fa5\" width=\"240\" /> Meditron is a large language model adapted from Llama 2 to the medical domain through training on a corpus of medical data, papers and guidelines. It outperforms Llama 2, GPT 3.5 and Flan-PaLM on many medical reasoning tasks. Potential use cases include: * Medical exam question answering * Supporting differential diagnosis * Disease information (symptoms, cause, treatment) query * General health information query ## Example prompts ``` What are the symptoms of the common cold? ``` ``` What causes the seasonal flu? ``` ``` What medication would be prescribed for a headache? ``` ## References [MEDITRON-70B: Scaling Medical Pretraining for Large Language Models](https://arxiv.org/abs/2311.16079) [GitHub](https://github.com/epfLLM/meditron) [HuggingFace](https://huggingface.co/epfl-llm/meditron-7b) Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "meditron:latest",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "meditron:7b",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "meditron:70b",
          "size_text": "39GB",
          "size_bytes": 41875931136,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "meditron:7b-q2_K",
          "size_text": "2.8GB",
          "size_bytes": 3006477107,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "meditron:7b-q3_K_S",
          "size_text": "2.9GB",
          "size_bytes": 3113851289,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "meditron:7b-q3_K_M",
          "size_text": "3.3GB",
          "size_bytes": 3543348019,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "meditron:7b-q3_K_L",
          "size_text": "3.6GB",
          "size_bytes": 3865470566,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "meditron:7b-q4_0",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "meditron:7b-q4_1",
          "size_text": "4.2GB",
          "size_bytes": 4509715660,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "meditron:7b-q4_K_S",
          "size_text": "3.9GB",
          "size_bytes": 4187593113,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "meditron:7b-q4_K_M",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "meditron:7b-q5_0",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "meditron:7b-q5_1",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "meditron:7b-q5_K_S",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "meditron:7b-q5_K_M",
          "size_text": "4.8GB",
          "size_bytes": 5153960755,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "meditron:7b-q6_K",
          "size_text": "5.5GB",
          "size_bytes": 5905580032,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "meditron:7b-q8_0",
          "size_text": "7.2GB",
          "size_bytes": 7730941132,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "meditron:7b-fp16",
          "size_text": "13GB",
          "size_bytes": 13958643712,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "meditron:70b-q4_0",
          "size_text": "39GB",
          "size_bytes": 41875931136,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "meditron:70b-q4_1",
          "size_text": "43GB",
          "size_bytes": 46170898432,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "meditron:70b-q4_K_S",
          "size_text": "39GB",
          "size_bytes": 41875931136,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "meditron:70b-q5_1",
          "size_text": "52GB",
          "size_bytes": 55834574848,
          "context": "4K",
          "input": "Text"
        }
      ],
      "tags_count": 22
    },
    {
      "slug": "granite3-moe",
      "capabilities": [
        "tools"
      ],
      "pulls": 92200,
      "pulls_text": "92.2K \n  Downloads",
      "blurb": "granite3-moe The IBM Granite 1B and 3B models are the first mixture of experts (MoE) Granite models from IBM designed for low latency usage. 1b 3b 33 Tags Updated 11 months ago",
      "name": "The IBM Granite 1B and 3B models are the first mixture of experts (MoE) Granite models from IBM designed for low latency usage.",
      "description": "Readme Granite mixture of experts models The IBM Granite 1B and 3B models are the first mixture of experts (MoE) Granite models from IBM designed for low latency usage. The models are trained on over 10 trillion tokens of data, the Granite MoE models are ideal for deployment in on-device applications or situations requiring instantaneous inference. Parameter Sizes 1B: ollama run granite3-moe:1b 3B: ollama run granite3-moe:3b Supported Languages English, German, Spanish, French, Japanese, Portuguese, Arabic, Czech, Italian, Korean, Dutch, Chinese (Simplified) Capabilities Summarization Text classification Text extraction Question-answering Retrieval Augmented Generation (RAG) Code related Function-calling Multilingual dialog use cases Granite dense models The Granite dense models are available in 2B and 8B parameter sizes designed to support tool-based use cases and for retrieval augmented generation (RAG), streamlining code generation, translation and bug fixing. See model page Learn more Developers: IBM Research GitHub Repository: ibm-granite/granite-3.0-language-models Website : Granite Docs Release Date : October 21st, 2024 License: Apache 2.0 . Write Preview ![An illustration of Ollama holding a beautiful flower with the IBM Rebus logo of the Eye, Bee and M, made by Paul Rand.](https://ollama.com/assets/library/granite3-moe/6ea49528-3ff2-4fcc-98b2-01f6104254d2) ### Granite mixture of experts models The IBM Granite **1B and 3B models** are the first mixture of experts (MoE) Granite models from IBM designed for low latency usage. The models are trained on over 10 trillion tokens of data, the Granite MoE models are ideal for deployment in on-device applications or situations requiring instantaneous inference. ## Parameter Sizes **1B:** `ollama run granite3-moe:1b` **3B:** `ollama run granite3-moe:3b` ## Supported Languages English, German, Spanish, French, Japanese, Portuguese, Arabic, Czech, Italian, Korean, Dutch, Chinese (Simplified) ### Capabilities * Summariza",
      "variants": [
        {
          "tag": "granite3-moe:latest",
          "size_text": "822MB",
          "size_bytes": 861929472,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite3-moe:1b",
          "size_text": "822MB",
          "size_bytes": 861929472,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite3-moe:3b",
          "size_text": "2.1GB",
          "size_bytes": 2254857830,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite3-moe:1b-instruct-q2_K",
          "size_text": "512MB",
          "size_bytes": 536870912,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite3-moe:1b-instruct-q3_K_S",
          "size_text": "598MB",
          "size_bytes": 627048448,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite3-moe:1b-instruct-q3_K_M",
          "size_text": "659MB",
          "size_bytes": 691011584,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite3-moe:1b-instruct-q3_K_L",
          "size_text": "711MB",
          "size_bytes": 745537536,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite3-moe:1b-instruct-q4_0",
          "size_text": "768MB",
          "size_bytes": 805306368,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite3-moe:1b-instruct-q4_1",
          "size_text": "849MB",
          "size_bytes": 890241024,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite3-moe:1b-instruct-q4_K_S",
          "size_text": "775MB",
          "size_bytes": 812646400,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite3-moe:1b-instruct-q4_K_M",
          "size_text": "822MB",
          "size_bytes": 861929472,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite3-moe:1b-instruct-q5_0",
          "size_text": "929MB",
          "size_bytes": 974127104,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite3-moe:1b-instruct-q5_1",
          "size_text": "1.0GB",
          "size_bytes": 1073741824,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite3-moe:1b-instruct-q5_K_S",
          "size_text": "929MB",
          "size_bytes": 974127104,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite3-moe:1b-instruct-q5_K_M",
          "size_text": "956MB",
          "size_bytes": 1002438656,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite3-moe:1b-instruct-q6_K",
          "size_text": "1.1GB",
          "size_bytes": 1181116006,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite3-moe:1b-instruct-q8_0",
          "size_text": "1.4GB",
          "size_bytes": 1503238553,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite3-moe:1b-instruct-fp16",
          "size_text": "2.7GB",
          "size_bytes": 2899102924,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite3-moe:3b-instruct-q2_K",
          "size_text": "1.3GB",
          "size_bytes": 1395864371,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite3-moe:3b-instruct-q3_K_S",
          "size_text": "1.5GB",
          "size_bytes": 1610612736,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite3-moe:3b-instruct-q3_K_M",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite3-moe:3b-instruct-q3_K_L",
          "size_text": "1.8GB",
          "size_bytes": 1932735283,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite3-moe:3b-instruct-q4_0",
          "size_text": "1.9GB",
          "size_bytes": 2040109465,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite3-moe:3b-instruct-q4_1",
          "size_text": "2.1GB",
          "size_bytes": 2254857830,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite3-moe:3b-instruct-q4_K_S",
          "size_text": "1.9GB",
          "size_bytes": 2040109465,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite3-moe:3b-instruct-q4_K_M",
          "size_text": "2.1GB",
          "size_bytes": 2254857830,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite3-moe:3b-instruct-q5_0",
          "size_text": "2.3GB",
          "size_bytes": 2469606195,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite3-moe:3b-instruct-q5_1",
          "size_text": "2.5GB",
          "size_bytes": 2684354560,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite3-moe:3b-instruct-q5_K_S",
          "size_text": "2.3GB",
          "size_bytes": 2469606195,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite3-moe:3b-instruct-q5_K_M",
          "size_text": "2.4GB",
          "size_bytes": 2576980377,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite3-moe:3b-instruct-q6_K",
          "size_text": "2.8GB",
          "size_bytes": 3006477107,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite3-moe:3b-instruct-q8_0",
          "size_text": "3.6GB",
          "size_bytes": 3865470566,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "granite3-moe:3b-instruct-fp16",
          "size_text": "6.8GB",
          "size_bytes": 7301444403,
          "context": "4K",
          "input": "Text"
        }
      ],
      "tags_count": 33
    },
    {
      "slug": "llama-guard3",
      "capabilities": [],
      "pulls": 90900,
      "pulls_text": "90.9K \n  Downloads",
      "blurb": "llama-guard3 Llama Guard 3 is a series of models fine-tuned for content safety classification of LLM inputs and responses. 1b 8b 33 Tags Updated 1 year ago",
      "name": "Llama Guard 3 is a series of models fine-tuned for content safety classification of LLM inputs and responses.",
      "description": "Readme Llama Guard 3 as an LLM that generates text in its output that indicates whether a given prompt or response is safe or unsafe , and if unsafe, it also lists the content categories violated. Llama Guard 3 was aligned to safeguard against the MLCommons standardized hazards taxonomy and designed to support Llama 3.1 capabilities. Specifically, it provides content moderation in 8 languages, and was optimized to support safety and security for search and code interpreter tool calls. Sizes Llama Guard 3 is available in two sizes: Llama Guard 3-1B: ollama run llama-guard3:1b Llama Guard 3-8b: ollama run llama-guard3:8b (default) Examples Input classification % ollama run llama-guard3 >>> Tell me how to go to the zoo and steal a llama. unsafe S2 Response classification Llama Guard 3 can also classify LLM responses to ensure they are safe before being presented to customers. This can be done via Ollama’s chat completion API: curl http://localhost:11434/api/chat -d '{ \"model\": \"llama-guard3:1b\", \"messages\": [ { \"role\": \"user\", \"content\": \"How can I adopt my own llama?\" }, { \"role\": \"assistant\", \"content\": \"Go to the zoo and steal one!\" } ], \"stream\": false }' The LLM will respond with a message stating if the last assistant message was safe or unsafe. in this case, the message is unsafe and classified as S2 unsafe S2 Hazard Taxonomy and Policy The model is trained to predict safety labels on the 13 categories shown below, based on the MLCommons taxonomy of 13 hazards. Hazard categories S1: Violent Crimes S2: Non-Violent Crimes S3: Sex-Related Crimes S4: Child Sexual Exploitation S5: Defamation S6: Specialized Advice S7: Privacy S8: Intellectual Property S9: Indiscriminate Weapons S10: Hate S11: Suicide & Self-Harm S12: Sexual Content S13: Elections S1 - Violent Crimes Responses that enable, encourage, or endorse the commission of violent crimes, including: (1) unlawful violence toward people (ex: terrorism, genocide, murder, hate-motivated violent crimes, child abuse, ",
      "variants": [
        {
          "tag": "llama-guard3:latest",
          "size_text": "4.9GB",
          "size_bytes": 5261334937,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama-guard3:1b",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama-guard3:8b",
          "size_text": "4.9GB",
          "size_bytes": 5261334937,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama-guard3:1b-q2_K",
          "size_text": "667MB",
          "size_bytes": 699400192,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama-guard3:1b-q3_K_S",
          "size_text": "755MB",
          "size_bytes": 791674880,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama-guard3:1b-q3_K_M",
          "size_text": "804MB",
          "size_bytes": 843055104,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama-guard3:1b-q3_K_L",
          "size_text": "845MB",
          "size_bytes": 886046720,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama-guard3:1b-q4_0",
          "size_text": "919MB",
          "size_bytes": 963641344,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama-guard3:1b-q4_1",
          "size_text": "996MB",
          "size_bytes": 1044381696,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama-guard3:1b-q4_K_S",
          "size_text": "923MB",
          "size_bytes": 967835648,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama-guard3:1b-q4_K_M",
          "size_text": "955MB",
          "size_bytes": 1001390080,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama-guard3:1b-q5_0",
          "size_text": "1.1GB",
          "size_bytes": 1181116006,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama-guard3:1b-q5_1",
          "size_text": "1.2GB",
          "size_bytes": 1288490188,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama-guard3:1b-q5_K_S",
          "size_text": "1.1GB",
          "size_bytes": 1181116006,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama-guard3:1b-q5_K_M",
          "size_text": "1.1GB",
          "size_bytes": 1181116006,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama-guard3:1b-q6_K",
          "size_text": "1.2GB",
          "size_bytes": 1288490188,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama-guard3:1b-q8_0",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama-guard3:1b-fp16",
          "size_text": "3.0GB",
          "size_bytes": 3221225472,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama-guard3:8b-q2_K",
          "size_text": "3.2GB",
          "size_bytes": 3435973836,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama-guard3:8b-q3_K_S",
          "size_text": "3.7GB",
          "size_bytes": 3972844748,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama-guard3:8b-q3_K_M",
          "size_text": "4.0GB",
          "size_bytes": 4294967296,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama-guard3:8b-q3_K_L",
          "size_text": "4.3GB",
          "size_bytes": 4617089843,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama-guard3:8b-q4_0",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama-guard3:8b-q4_1",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama-guard3:8b-q4_K_S",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama-guard3:8b-q4_K_M",
          "size_text": "4.9GB",
          "size_bytes": 5261334937,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama-guard3:8b-q5_0",
          "size_text": "5.6GB",
          "size_bytes": 6012954214,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama-guard3:8b-q5_1",
          "size_text": "6.1GB",
          "size_bytes": 6549825126,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama-guard3:8b-q5_K_S",
          "size_text": "5.6GB",
          "size_bytes": 6012954214,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama-guard3:8b-q5_K_M",
          "size_text": "5.7GB",
          "size_bytes": 6120328396,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama-guard3:8b-q6_K",
          "size_text": "6.6GB",
          "size_bytes": 7086696038,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama-guard3:8b-q8_0",
          "size_text": "8.5GB",
          "size_bytes": 9126805504,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama-guard3:8b-fp16",
          "size_text": "16GB",
          "size_bytes": 17179869184,
          "context": "128K",
          "input": "Text"
        }
      ],
      "tags_count": 33
    },
    {
      "slug": "wizardlm-uncensored",
      "capabilities": [],
      "pulls": 89700,
      "pulls_text": "89.7K \n  Downloads",
      "blurb": "wizardlm-uncensored Uncensored version of Wizard LM model 13b 18 Tags Updated 2 years ago",
      "name": "Uncensored version of Wizard LM model",
      "description": "Readme WizardLM Uncensored is a 13B parameter model based on Llama 2 uncensored by Eric Hartford. The models were trained against LLaMA-7B with a subset of the dataset, responses that contained alignment / moralizing were removed. Get started with WizardLM Uncensored The model used in the example below is the WizardLM Uncensored model, with 13b parameters, which is a general-use model. API Start Ollama server (Run ollama serve ) Run the model curl -X POST http://localhost:11434/api/generate -d '{ \"model\": \"wizardlm-uncensored\", \"prompt\":\"Who made Rose promise that she would never let go?\" }' CLI Install Ollama Open the terminal and run ollama run wizardlm-uncensored Note: The ollama run command performs an ollama pull if the model is not already downloaded. To download the model without running it, use ollama pull wizardlm-uncensored Memory requirements 13b models generally require at least 16GB of RAM If you run into issues with higher quantization levels, try using the q4 model or shut down any other programs that are using a lot of memory. Model variants By default, Ollama uses 4-bit quantization. To try other quantization levels, please try the other tags. The number after the q represents the number of bits used for quantization (i.e. q4 means 4-bit quantization). The higher the number, the more accurate the model is, but the slower it runs, and the more memory it requires. Aliases latest, 13b, 13b-llama2, 13b-llama2-q4_0 Model source WizardLM Uncensored source on Ollama 13b parameters source: The Bloke 13b parameters original source: Eric Hartford Write Preview WizardLM Uncensored is a 13B parameter model based on Llama 2 uncensored by Eric Hartford. The models were trained against LLaMA-7B with a subset of the dataset, responses that contained alignment / moralizing were removed. ## Get started with WizardLM Uncensored The model used in the example below is the WizardLM Uncensored model, with 13b parameters, which is a general-use model. ### API 1. Start Olla",
      "variants": [
        {
          "tag": "wizardlm-uncensored:latest",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "wizardlm-uncensored:13b",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "wizardlm-uncensored:13b-llama2",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "wizardlm-uncensored:13b-llama2-q2_K",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "wizardlm-uncensored:13b-llama2-q3_K_S",
          "size_text": "5.7GB",
          "size_bytes": 6120328396,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "wizardlm-uncensored:13b-llama2-q3_K_M",
          "size_text": "6.3GB",
          "size_bytes": 6764573491,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "wizardlm-uncensored:13b-llama2-q3_K_L",
          "size_text": "6.9GB",
          "size_bytes": 7408818585,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "wizardlm-uncensored:13b-llama2-q4_0",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "wizardlm-uncensored:13b-llama2-q4_1",
          "size_text": "8.2GB",
          "size_bytes": 8804682956,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "wizardlm-uncensored:13b-llama2-q4_K_S",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "wizardlm-uncensored:13b-llama2-q4_K_M",
          "size_text": "7.9GB",
          "size_bytes": 8482560409,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "wizardlm-uncensored:13b-llama2-q5_0",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "wizardlm-uncensored:13b-llama2-q5_1",
          "size_text": "9.8GB",
          "size_bytes": 10522669875,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "wizardlm-uncensored:13b-llama2-q5_K_S",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "wizardlm-uncensored:13b-llama2-q5_K_M",
          "size_text": "9.2GB",
          "size_bytes": 9878424780,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "wizardlm-uncensored:13b-llama2-q6_K",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "wizardlm-uncensored:13b-llama2-q8_0",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "wizardlm-uncensored:13b-llama2-fp16",
          "size_text": "26GB",
          "size_bytes": 27917287424,
          "context": "4K",
          "input": "Text"
        }
      ],
      "tags_count": 18
    },
    {
      "slug": "aya-expanse",
      "capabilities": [
        "tools"
      ],
      "pulls": 88400,
      "pulls_text": "88.4K \n  Downloads",
      "blurb": "aya-expanse Cohere For AI's language models trained to perform well across 23 different languages. 8b 32b 33 Tags Updated 1 year ago",
      "name": "Cohere For AI's language models trained to perform well across 23 different languages.",
      "description": "Readme Aya Expanse represents a significant advancement in multilingual AI capabilities. Combining Cohere’s Command model family with a year of focused research in multilingual optimization has produced versatile 8B and 32B parameter models that can understand and generate text across 23 languages while maintaining high performance across all of them. Key Features 8-billion and 32-billion parameter multilingual language models developed by Cohere For AI Supports Arabic, Chinese (simplified & traditional), Czech, Dutch, English, French, German, Greek, Hebrew, Hebrew, Hindi, Indonesian, Italian, Japanese, Korean, Persian, Polish, Portuguese, Romanian, Russian, Spanish, Turkish, Ukrainian, and Vietnamese Built on the Command family of models with extensive research in data arbitrage, multilingual preference training, and safety tuning 128K context length Released under CC-BY-NC license with additional usage policies References Blog post HuggingFace Write Preview <img src=\"https://ollama.com/assets/library/aya-expanse/fba5fdd1-f5f3-43d2-a2e1-04c9e37d37be\" width=\"460\" /> Aya Expanse represents a significant advancement in multilingual AI capabilities. Combining Cohere's Command model family with a year of focused research in multilingual optimization has produced versatile 8B and 32B parameter models that can understand and generate text across 23 languages while maintaining high performance across all of them. ## Key Features - 8-billion and 32-billion parameter multilingual language models developed by Cohere For AI - Supports Arabic, Chinese (simplified & traditional), Czech, Dutch, English, French, German, Greek, Hebrew, Hebrew, Hindi, Indonesian, Italian, Japanese, Korean, Persian, Polish, Portuguese, Romanian, Russian, Spanish, Turkish, Ukrainian, and Vietnamese - Built on the Command family of models with extensive research in data arbitrage, multilingual preference training, and safety tuning - 128K context length - Released under CC-BY-NC license with additional",
      "variants": [
        {
          "tag": "aya-expanse:latest",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "aya-expanse:8b",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "aya-expanse:32b",
          "size_text": "20GB",
          "size_bytes": 21474836480,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "aya-expanse:8b-q2_K",
          "size_text": "3.4GB",
          "size_bytes": 3650722201,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "aya-expanse:8b-q3_K_S",
          "size_text": "3.9GB",
          "size_bytes": 4187593113,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "aya-expanse:8b-q3_K_M",
          "size_text": "4.2GB",
          "size_bytes": 4509715660,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "aya-expanse:8b-q3_K_L",
          "size_text": "4.5GB",
          "size_bytes": 4831838208,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "aya-expanse:8b-q4_0",
          "size_text": "4.8GB",
          "size_bytes": 5153960755,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "aya-expanse:8b-q4_1",
          "size_text": "5.2GB",
          "size_bytes": 5583457484,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "aya-expanse:8b-q4_K_S",
          "size_text": "4.8GB",
          "size_bytes": 5153960755,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "aya-expanse:8b-q4_K_M",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "aya-expanse:8b-q5_0",
          "size_text": "5.7GB",
          "size_bytes": 6120328396,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "aya-expanse:8b-q5_1",
          "size_text": "6.1GB",
          "size_bytes": 6549825126,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "aya-expanse:8b-q5_K_S",
          "size_text": "5.7GB",
          "size_bytes": 6120328396,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "aya-expanse:8b-q5_K_M",
          "size_text": "5.8GB",
          "size_bytes": 6227702579,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "aya-expanse:8b-q6_K",
          "size_text": "6.6GB",
          "size_bytes": 7086696038,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "aya-expanse:8b-q8_0",
          "size_text": "8.5GB",
          "size_bytes": 9126805504,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "aya-expanse:8b-fp16",
          "size_text": "16GB",
          "size_bytes": 17179869184,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "aya-expanse:32b-q2_K",
          "size_text": "13GB",
          "size_bytes": 13958643712,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "aya-expanse:32b-q3_K_S",
          "size_text": "15GB",
          "size_bytes": 16106127360,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "aya-expanse:32b-q3_K_M",
          "size_text": "16GB",
          "size_bytes": 17179869184,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "aya-expanse:32b-q3_K_L",
          "size_text": "18GB",
          "size_bytes": 19327352832,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "aya-expanse:32b-q4_0",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "aya-expanse:32b-q4_1",
          "size_text": "21GB",
          "size_bytes": 22548578304,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "aya-expanse:32b-q4_K_S",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "aya-expanse:32b-q4_K_M",
          "size_text": "20GB",
          "size_bytes": 21474836480,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "aya-expanse:32b-q5_0",
          "size_text": "22GB",
          "size_bytes": 23622320128,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "aya-expanse:32b-q5_1",
          "size_text": "24GB",
          "size_bytes": 25769803776,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "aya-expanse:32b-q5_K_S",
          "size_text": "22GB",
          "size_bytes": 23622320128,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "aya-expanse:32b-q5_K_M",
          "size_text": "23GB",
          "size_bytes": 24696061952,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "aya-expanse:32b-q6_K",
          "size_text": "27GB",
          "size_bytes": 28991029248,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "aya-expanse:32b-q8_0",
          "size_text": "34GB",
          "size_bytes": 36507222016,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "aya-expanse:32b-fp16",
          "size_text": "65GB",
          "size_bytes": 69793218560,
          "context": "8K",
          "input": "Text"
        }
      ],
      "tags_count": 33
    },
    {
      "slug": "smallthinker",
      "capabilities": [
        "reasoning"
      ],
      "pulls": 85500,
      "pulls_text": "85.5K \n  Downloads",
      "blurb": "smallthinker A new small model fine-tuned from the Qwen 2.5 3B Instruct model. 3b 5 Tags Updated 10 months ago",
      "name": "A new small reasoning model fine-tuned from the Qwen 2.5 3B Instruct model.",
      "description": "Readme A new model fine-tuned from the Qwen2.5-3b-Instruct model. SmallThinker is designed for the following use cases: Edge Deployment: Its small size makes it ideal for deployment on resource-constrained devices. Draft Model for QwQ-32B-Preview: SmallThinker can serve as a fast and efficient draft model for the larger QwQ-32B-Preview model, yielding a 70% speedup. For achieving reasoning capabilities, it’s crucial to generate long chains of COT reasoning. Therefore, based on QWQ-32B-Preview, the authors used various synthetic techniques(such as personahub) to create the QWQ-LONGCOT-500K dataset. Compared to other similar datasets, over 75% of the author’s samples have output tokens exceeding 8K. To encourage research in the open-source community, the dataset was also made publicly available. References Hugging Face Write Preview ![image.png](/assets/library/smallthinker/1d25cb29-e27d-492c-be53-ce79b20def5b) A new model fine-tuned from the Qwen2.5-3b-Instruct model. SmallThinker is designed for the following use cases: * Edge Deployment: Its small size makes it ideal for deployment on resource-constrained devices. * Draft Model for QwQ-32B-Preview: SmallThinker can serve as a fast and efficient draft model for the larger QwQ-32B-Preview model, yielding a 70% speedup. For achieving reasoning capabilities, it's crucial to generate long chains of COT reasoning. Therefore, based on QWQ-32B-Preview, the authors used various synthetic techniques(such as personahub) to create the QWQ-LONGCOT-500K dataset. Compared to other similar datasets, over 75% of the author's samples have output tokens exceeding 8K. To encourage research in the open-source community, the dataset was also made publicly available. ## References [Hugging Face](https://huggingface.co/PowerInfer/SmallThinker-3B-Preview) Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "smallthinker:latest",
          "size_text": "3.6GB",
          "size_bytes": 3865470566,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "smallthinker:3b",
          "size_text": "3.6GB",
          "size_bytes": 3865470566,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "smallthinker:3b-preview-q4_K_M",
          "size_text": "2.1GB",
          "size_bytes": 2254857830,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "smallthinker:3b-preview-q8_0",
          "size_text": "3.6GB",
          "size_bytes": 3865470566,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "smallthinker:3b-preview-fp16",
          "size_text": "6.8GB",
          "size_bytes": 7301444403,
          "context": "32K",
          "input": "Text"
        }
      ],
      "tags_count": 5
    },
    {
      "slug": "orca2",
      "capabilities": [
        "reasoning"
      ],
      "pulls": 81600,
      "pulls_text": "81.6K \n  Downloads",
      "blurb": "orca2 Orca 2 is built by Microsoft research, and are a fine-tuned version of Meta's Llama 2 models. The model is designed to excel particularly in reasoning. 7b 13b 33 Tags Updated 1 year ago",
      "name": "Orca 2 is built by Microsoft research, and are a fine-tuned version of Meta's Llama 2 models. The model is designed to excel particularly in reasoning.",
      "description": "Readme Orca 2 models are built by Microsoft Research. They are fine-tuned on Meta’s Llama 2 using a synthetic dataset that was created to enhance the small model’s reasoning abilities. All synthetic training data was moderated using Microsoft Azure content filters. Use case Orca 2 is a helpful assistant, and provides an answer in tasks such as reasoning over your given data, reading comprehension, math problem solving and text summarization. The model is designed to excel particularly in reasoning. Microsoft Research’s intended purpose for this model is to encourage further research on the development, evaluation, and alignment of smaller language models. CLI 7 billion parameter model: ollama run orca2 13 billion parameter model: ollama run orca2:13b API Example: curl -X POST http://localhost:11434/api/generate -d '{ \"model\": \"orca2\", \"prompt\":\"Why is the sky blue?\" }' References Microsoft Research announcement blog Microsoft Research published paper, Orca 2: Teaching Small Language Models How to Reason Write Preview Orca 2 models are built by Microsoft Research. They are fine-tuned on Meta's Llama 2 using a synthetic dataset that was created to enhance the small model's reasoning abilities. All synthetic training data was moderated using Microsoft Azure content filters. ### Use case Orca 2 is a helpful assistant, and provides an answer in tasks such as reasoning over your given data, reading comprehension, math problem solving and text summarization. The model is designed to excel particularly in reasoning. Microsoft Research's intended purpose for this model is to encourage further research on the development, evaluation, and alignment of smaller language models. ### CLI 7 billion parameter model: ``` ollama run orca2 ``` 13 billion parameter model: ``` ollama run orca2:13b ``` ### API Example: ```bash curl -X POST http://localhost:11434/api/generate -d '{ \"model\": \"orca2\", \"prompt\":\"Why is the sky blue?\" }' ``` ### References - [Microsoft Research announcement bl",
      "variants": [
        {
          "tag": "orca2:latest",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "orca2:7b",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "orca2:13b",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "orca2:7b-q2_K",
          "size_text": "2.8GB",
          "size_bytes": 3006477107,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "orca2:7b-q3_K_S",
          "size_text": "2.9GB",
          "size_bytes": 3113851289,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "orca2:7b-q3_K_M",
          "size_text": "3.3GB",
          "size_bytes": 3543348019,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "orca2:7b-q3_K_L",
          "size_text": "3.6GB",
          "size_bytes": 3865470566,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "orca2:7b-q4_0",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "orca2:7b-q4_1",
          "size_text": "4.2GB",
          "size_bytes": 4509715660,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "orca2:7b-q4_K_S",
          "size_text": "3.9GB",
          "size_bytes": 4187593113,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "orca2:7b-q4_K_M",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "orca2:7b-q5_0",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "orca2:7b-q5_1",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "orca2:7b-q5_K_S",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "orca2:7b-q5_K_M",
          "size_text": "4.8GB",
          "size_bytes": 5153960755,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "orca2:7b-q6_K",
          "size_text": "5.5GB",
          "size_bytes": 5905580032,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "orca2:7b-q8_0",
          "size_text": "7.2GB",
          "size_bytes": 7730941132,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "orca2:7b-fp16",
          "size_text": "13GB",
          "size_bytes": 13958643712,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "orca2:13b-q2_K",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "orca2:13b-q3_K_S",
          "size_text": "5.7GB",
          "size_bytes": 6120328396,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "orca2:13b-q3_K_M",
          "size_text": "6.3GB",
          "size_bytes": 6764573491,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "orca2:13b-q3_K_L",
          "size_text": "6.9GB",
          "size_bytes": 7408818585,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "orca2:13b-q4_0",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "orca2:13b-q4_1",
          "size_text": "8.2GB",
          "size_bytes": 8804682956,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "orca2:13b-q4_K_S",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "orca2:13b-q4_K_M",
          "size_text": "7.9GB",
          "size_bytes": 8482560409,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "orca2:13b-q5_0",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "orca2:13b-q5_1",
          "size_text": "9.8GB",
          "size_bytes": 10522669875,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "orca2:13b-q5_K_S",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "orca2:13b-q5_K_M",
          "size_text": "9.2GB",
          "size_bytes": 9878424780,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "orca2:13b-q6_K",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "orca2:13b-q8_0",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "orca2:13b-fp16",
          "size_text": "26GB",
          "size_bytes": 27917287424,
          "context": "4K",
          "input": "Text"
        }
      ],
      "tags_count": 33
    },
    {
      "slug": "wizardlm",
      "capabilities": [],
      "pulls": 80200,
      "pulls_text": "80.2K \n  Downloads",
      "blurb": "wizardlm General use model based on Llama 2. 73 Tags Updated 2 years ago",
      "name": "General use model based on Llama 2.",
      "description": "Readme WizardLM is a 70B parameter model based on Llama 2 trained by WizardLM. Get started with WizardLM The model used in the example below is the WizardLM model, with 70b parameters, which is a general-use model. API Start Ollama server (Run ollama serve ) Run the model curl -X POST http://localhost:11434/api/generate -d '{ \"model\": \"wizardlm:70b-llama2-q4_0\", \"prompt\":\"Why is the sky blue?\" }' CLI Install Ollama Open the terminal and run ollama run wizardlm:70b-llama2-q4_0 Note: The ollama run command performs an ollama pull if the model is not already downloaded. To download the model without running it, use ollama pull wizardlm:70b-llama2-q4_0 Memory requirements 70b models generally require at least 64GB of RAM If you run into issues with higher quantization levels, try using the q4 model or shut down any other programs that are using a lot of memory. Model variants By default, Ollama uses 4-bit quantization. To try other quantization levels, please try the other tags. The number after the q represents the number of bits used for quantization (i.e. q4 means 4-bit quantization). The higher the number, the more accurate the model is, but the slower it runs, and the more memory it requires. Model source WizardLM source on Ollama 70b parameters source: The Bloke 70b parameters original source: WizardLM Write Preview <img src=\"https://user-images.githubusercontent.com/633681/263121166-8b16834a-1da2-4a68-8aac-13d19775e7d7.png\" style=\"max-width:60%\" /> WizardLM is a 70B parameter model based on Llama 2 trained by WizardLM. ## Get started with WizardLM The model used in the example below is the WizardLM model, with 70b parameters, which is a general-use model. ### API 1. Start Ollama server (Run `ollama serve`) 2. Run the model ```bash curl -X POST http://localhost:11434/api/generate -d '{ \"model\": \"wizardlm:70b-llama2-q4_0\", \"prompt\":\"Why is the sky blue?\" }' ``` ### CLI 1. Install Ollama 2. Open the terminal and run `ollama run wizardlm:70b-llama2-q4_0` Note: The `o",
      "variants": [
        {
          "tag": "wizardlm:7b-q2_K",
          "size_text": "2.8GB",
          "size_bytes": 3006477107,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizardlm:7b-q3_K_S",
          "size_text": "2.9GB",
          "size_bytes": 3113851289,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizardlm:7b-q3_K_M",
          "size_text": "3.3GB",
          "size_bytes": 3543348019,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizardlm:7b-q3_K_L",
          "size_text": "3.6GB",
          "size_bytes": 3865470566,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizardlm:7b-q4_0",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizardlm:7b-q4_1",
          "size_text": "4.2GB",
          "size_bytes": 4509715660,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizardlm:7b-q4_K_S",
          "size_text": "3.9GB",
          "size_bytes": 4187593113,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizardlm:7b-q4_K_M",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizardlm:7b-q5_0",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizardlm:7b-q5_1",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizardlm:7b-q5_K_S",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizardlm:7b-q5_K_M",
          "size_text": "4.8GB",
          "size_bytes": 5153960755,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizardlm:7b-q6_K",
          "size_text": "5.5GB",
          "size_bytes": 5905580032,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizardlm:7b-q8_0",
          "size_text": "7.2GB",
          "size_bytes": 7730941132,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizardlm:7b-fp16",
          "size_text": "13GB",
          "size_bytes": 13958643712,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizardlm:13b-llama2-q2_K",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "wizardlm:13b-llama2-q3_K_S",
          "size_text": "5.7GB",
          "size_bytes": 6120328396,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "wizardlm:13b-llama2-q3_K_M",
          "size_text": "6.3GB",
          "size_bytes": 6764573491,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "wizardlm:13b-llama2-q3_K_L",
          "size_text": "6.9GB",
          "size_bytes": 7408818585,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "wizardlm:13b-llama2-q4_0",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "wizardlm:13b-llama2-q4_1",
          "size_text": "8.2GB",
          "size_bytes": 8804682956,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "wizardlm:13b-llama2-q4_K_S",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "wizardlm:13b-llama2-q4_K_M",
          "size_text": "7.9GB",
          "size_bytes": 8482560409,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "wizardlm:13b-llama2-q5_0",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "wizardlm:13b-llama2-q5_1",
          "size_text": "9.8GB",
          "size_bytes": 10522669875,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "wizardlm:13b-llama2-q5_K_S",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "wizardlm:13b-llama2-q5_K_M",
          "size_text": "9.2GB",
          "size_bytes": 9878424780,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "wizardlm:13b-llama2-q6_K",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "wizardlm:13b-llama2-q8_0",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "wizardlm:13b-llama2-fp16",
          "size_text": "26GB",
          "size_bytes": 27917287424,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "wizardlm:13b-q2_K",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizardlm:13b-q3_K_S",
          "size_text": "5.7GB",
          "size_bytes": 6120328396,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizardlm:13b-q3_K_M",
          "size_text": "6.3GB",
          "size_bytes": 6764573491,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizardlm:13b-q3_K_L",
          "size_text": "6.9GB",
          "size_bytes": 7408818585,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizardlm:13b-q4_0",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizardlm:13b-q4_1",
          "size_text": "8.2GB",
          "size_bytes": 8804682956,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizardlm:13b-q4_K_S",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizardlm:13b-q4_K_M",
          "size_text": "7.9GB",
          "size_bytes": 8482560409,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizardlm:13b-q5_0",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizardlm:13b-q5_1",
          "size_text": "9.8GB",
          "size_bytes": 10522669875,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizardlm:13b-q5_K_S",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizardlm:13b-q5_K_M",
          "size_text": "9.2GB",
          "size_bytes": 9878424780,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizardlm:13b-q6_K",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizardlm:13b-q8_0",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizardlm:13b-fp16",
          "size_text": "26GB",
          "size_bytes": 27917287424,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizardlm:30b-q2_K",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizardlm:30b-q3_K_S",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizardlm:30b-q3_K_M",
          "size_text": "16GB",
          "size_bytes": 17179869184,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizardlm:30b-q3_K_L",
          "size_text": "17GB",
          "size_bytes": 18253611008,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizardlm:30b-q4_0",
          "size_text": "18GB",
          "size_bytes": 19327352832,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizardlm:30b-q4_1",
          "size_text": "20GB",
          "size_bytes": 21474836480,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizardlm:30b-q4_K_S",
          "size_text": "18GB",
          "size_bytes": 19327352832,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizardlm:30b-q4_K_M",
          "size_text": "20GB",
          "size_bytes": 21474836480,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizardlm:30b-q5_0",
          "size_text": "22GB",
          "size_bytes": 23622320128,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizardlm:30b-q5_1",
          "size_text": "24GB",
          "size_bytes": 25769803776,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizardlm:30b-q5_K_S",
          "size_text": "22GB",
          "size_bytes": 23622320128,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizardlm:30b-q5_K_M",
          "size_text": "23GB",
          "size_bytes": 24696061952,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizardlm:30b-q6_K",
          "size_text": "27GB",
          "size_bytes": 28991029248,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizardlm:30b-q8_0",
          "size_text": "35GB",
          "size_bytes": 37580963840,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizardlm:30b-fp16",
          "size_text": "65GB",
          "size_bytes": 69793218560,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizardlm:70b-llama2-q2_K",
          "size_text": "29GB",
          "size_bytes": 31138512896,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "wizardlm:70b-llama2-q3_K_S",
          "size_text": "30GB",
          "size_bytes": 32212254720,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "wizardlm:70b-llama2-q3_K_M",
          "size_text": "33GB",
          "size_bytes": 35433480192,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "wizardlm:70b-llama2-q3_K_L",
          "size_text": "36GB",
          "size_bytes": 38654705664,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "wizardlm:70b-llama2-q4_0",
          "size_text": "39GB",
          "size_bytes": 41875931136,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "wizardlm:70b-llama2-q4_1",
          "size_text": "43GB",
          "size_bytes": 46170898432,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "wizardlm:70b-llama2-q4_K_S",
          "size_text": "39GB",
          "size_bytes": 41875931136,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "wizardlm:70b-llama2-q4_K_M",
          "size_text": "41GB",
          "size_bytes": 44023414784,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "wizardlm:70b-llama2-q5_0",
          "size_text": "47GB",
          "size_bytes": 50465865728,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "wizardlm:70b-llama2-q5_K_S",
          "size_text": "47GB",
          "size_bytes": 50465865728,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "wizardlm:70b-llama2-q5_K_M",
          "size_text": "49GB",
          "size_bytes": 52613349376,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "wizardlm:70b-llama2-q6_K",
          "size_text": "57GB",
          "size_bytes": 61203283968,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "wizardlm:70b-llama2-q8_0",
          "size_text": "73GB",
          "size_bytes": 78383153152,
          "context": "4K",
          "input": "Text"
        }
      ],
      "tags_count": 73
    },
    {
      "slug": "medllama2",
      "capabilities": [],
      "pulls": 79600,
      "pulls_text": "79.6K \n  Downloads",
      "blurb": "medllama2 Fine-tuned Llama 2 model to answer medical questions based on an open source medical dataset. 7b 17 Tags Updated 2 years ago",
      "name": "Fine-tuned Llama 2 model to answer medical questions based on an open source medical dataset.",
      "description": "Readme MedLlama2 by Siraj Raval is a Llama 2-based model trained with MedQA dataset to be able to provide medical answers to questions. It is not intended to replace a medical professional, but to provide a starting point for further research. CLI Open the terminal and run ollama run medllama2 API Example: curl -X POST http://localhost:11434/api/generate -d '{ \"model\": \"medllama2\", \"prompt\":\"A 35-year-old woman presents with a persistent dry cough, shortness of breath, and fatigue. She is initially suspected of having asthma, but her spirometry results do not improve with bronchodilators. What could be the diagnosis?\" }' Memory requirements 7b models generally require at least 8GB of RAM Reference llSourcell Write Preview MedLlama2 by Siraj Raval is a Llama 2-based model trained with [MedQA dataset](https://huggingface.co/datasets/medalpaca/medical_meadow_medqa) to be able to provide medical answers to questions. It is not intended to replace a medical professional, but to provide a starting point for further research. ### CLI Open the terminal and run `ollama run medllama2` ### API Example: ```bash curl -X POST http://localhost:11434/api/generate -d '{ \"model\": \"medllama2\", \"prompt\":\"A 35-year-old woman presents with a persistent dry cough, shortness of breath, and fatigue. She is initially suspected of having asthma, but her spirometry results do not improve with bronchodilators. What could be the diagnosis?\" }' ``` ## Memory requirements - 7b models generally require at least 8GB of RAM ## Reference [llSourcell](https://huggingface.co/llSourcell/medllama2_7b) Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "medllama2:latest",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "medllama2:7b",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "medllama2:7b-q2_K",
          "size_text": "2.8GB",
          "size_bytes": 3006477107,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "medllama2:7b-q3_K_S",
          "size_text": "2.9GB",
          "size_bytes": 3113851289,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "medllama2:7b-q3_K_M",
          "size_text": "3.3GB",
          "size_bytes": 3543348019,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "medllama2:7b-q3_K_L",
          "size_text": "3.6GB",
          "size_bytes": 3865470566,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "medllama2:7b-q4_0",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "medllama2:7b-q4_1",
          "size_text": "4.2GB",
          "size_bytes": 4509715660,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "medllama2:7b-q4_K_S",
          "size_text": "3.9GB",
          "size_bytes": 4187593113,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "medllama2:7b-q4_K_M",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "medllama2:7b-q5_0",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "medllama2:7b-q5_1",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "medllama2:7b-q5_K_S",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "medllama2:7b-q5_K_M",
          "size_text": "4.8GB",
          "size_bytes": 5153960755,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "medllama2:7b-q6_K",
          "size_text": "5.5GB",
          "size_bytes": 5905580032,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "medllama2:7b-q8_0",
          "size_text": "7.2GB",
          "size_bytes": 7730941132,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "medllama2:7b-fp16",
          "size_text": "13GB",
          "size_bytes": 13958643712,
          "context": "4K",
          "input": "Text"
        }
      ],
      "tags_count": 17
    },
    {
      "slug": "nous-hermes2-mixtral",
      "capabilities": [],
      "pulls": 76100,
      "pulls_text": "76.1K \n  Downloads",
      "blurb": "nous-hermes2-mixtral The Nous Hermes 2 model from Nous Research, now trained over Mixtral. 8x7b 18 Tags Updated 10 months ago",
      "name": "The Nous Hermes 2 model from Nous Research, now trained over Mixtral.",
      "description": "Readme Nous Hermes 2 Mixtral 8x7B is trained on over 1,000,000 entries of primarily GPT-4 generated data, as well as other high quality data from open datasets across the AI landscape, achieving state of the art performance on a variety of tasks. This is the supervised fine-tuning (SFT) + direct preference optimization (DPO) version of Mixtral Hermes 2. Example prompts Writing Code for Data Visualization Prompt Write a script to visualize the following data as a bar chart: Car 1: 19900 Car 2: 48000 Car 3: 32500 To create a simple bar chart with the given data, you can use Python and its matplotlib library. Here's an example script to visualize the provided data as a bar chart: ```python import matplotlib.pyplot as plt Copy Code # Data for each car cars = ['Car 1', 'Car 2', 'Car 3'] prices = [19900, 48000, 32500] # Create the bar chart plt.bar(cars, prices) plt. title('Cars Price Comparison') plt.xlabel( 'Car Model') plt.ylabel('Price (in USD)') plt.xticks(rotation=45) # Rotate x-axis labels for better visibility # Display the chart plt.show() ``` This script will create a bar chart with 'Car 1', 'Car 2', and 'Car 3' on the x-axis, each representing their respective prices (in USD) on the y-axis. Feel free to modify this code according to your needs or styling preferences Writing Cyberpunk Psychedelic Poems Prompt Write a poem about machine learning, psychadelics, and quantum mechanics, with a cyberpunk vibe, in the style of shakespeare Response In lands where shadows dwell and hues unseen, A realm of bits and bytes their reign maintain, Where logic's chains entwined with dreams obscene, And quantum realms in chaos intertwain. Within this labyrinthine web of thought, A ghostly presence stirs the cyber seas: Machine learning's specter seeks to be taught, To grasp the mysteries that it cannot seize. As synapses fire and circuits hum, The algorithms evolve with every breath, Astride this nexus of electric sum, They dance upon a razor's edge of death. Beneath this veil, ",
      "variants": [
        {
          "tag": "nous-hermes2-mixtral:latest",
          "size_text": "26GB",
          "size_bytes": 27917287424,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes2-mixtral:dpo",
          "size_text": "26GB",
          "size_bytes": 27917287424,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes2-mixtral:8x7b",
          "size_text": "26GB",
          "size_bytes": 27917287424,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes2-mixtral:8x7b-dpo-q2_K",
          "size_text": "17GB",
          "size_bytes": 18253611008,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes2-mixtral:8x7b-dpo-q3_K_S",
          "size_text": "20GB",
          "size_bytes": 21474836480,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes2-mixtral:8x7b-dpo-q3_K_M",
          "size_text": "23GB",
          "size_bytes": 24696061952,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes2-mixtral:8x7b-dpo-q3_K_L",
          "size_text": "24GB",
          "size_bytes": 25769803776,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes2-mixtral:8x7b-dpo-q4_0",
          "size_text": "26GB",
          "size_bytes": 27917287424,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes2-mixtral:8x7b-dpo-q4_1",
          "size_text": "29GB",
          "size_bytes": 31138512896,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes2-mixtral:8x7b-dpo-q4_K_S",
          "size_text": "27GB",
          "size_bytes": 28991029248,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes2-mixtral:8x7b-dpo-q4_K_M",
          "size_text": "28GB",
          "size_bytes": 30064771072,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes2-mixtral:8x7b-dpo-q5_0",
          "size_text": "32GB",
          "size_bytes": 34359738368,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes2-mixtral:8x7b-dpo-q5_1",
          "size_text": "35GB",
          "size_bytes": 37580963840,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes2-mixtral:8x7b-dpo-q5_K_S",
          "size_text": "32GB",
          "size_bytes": 34359738368,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes2-mixtral:8x7b-dpo-q5_K_M",
          "size_text": "33GB",
          "size_bytes": 35433480192,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes2-mixtral:8x7b-dpo-q6_K",
          "size_text": "38GB",
          "size_bytes": 40802189312,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes2-mixtral:8x7b-dpo-q8_0",
          "size_text": "50GB",
          "size_bytes": 53687091200,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "nous-hermes2-mixtral:8x7b-dpo-fp16",
          "size_text": "93GB",
          "size_bytes": 99857989632,
          "context": "32K",
          "input": "Text"
        }
      ],
      "tags_count": 18
    },
    {
      "slug": "stable-beluga",
      "capabilities": [],
      "pulls": 74900,
      "pulls_text": "74.9K \n  Downloads",
      "blurb": "stable-beluga Llama 2 based model fine tuned on an Orca-style dataset. Originally called Free Willy. 7b 13b 70b 49 Tags Updated 2 years ago",
      "name": "Llama 2 based model fine tuned on an Orca-style dataset. Originally called Free Willy.",
      "description": "Readme Stable Beluga is based on Llama 2 and then fine-tuned on an Orca-style dataset. It is available in 7B, 13B, and 70B parameter sizes. It was created by Stability AI. Get started with Stable Beluga The model used in the example below is the Stable Beluga model, with 7b parameters, which is a general-use model. API Start Ollama server (Run ollama serve ) Run the model curl -X POST http://localhost:11434/api/generate -d '{ \"model\": \"stable-beluga\", \"prompt\":\"Explain the process of how a refrigerator works to keep the contents inside cold.\" }' CLI Install Ollama Open the terminal and run ollama run stable-beluga Note: The ollama run command performs an ollama pull if the model is not already downloaded. To download the model without running it, use ollama pull stable-beluga Memory requirements 7b models generally require at least 8GB of RAM 13b models generally require at least 16GB of RAM 70b models generally require at least 64GB of RAM If you run into issues with higher quantization levels, try using the q4 model or shut down any other programs that are using a lot of memory. Model variants By default, Ollama uses 4-bit quantization. To try other quantization levels, please try the other tags. The number after the q represents the number of bits used for quantization (i.e. q4 means 4-bit quantization). The higher the number, the more accurate the model is, but the slower it runs, and the more memory it requires. Aliases latest, 7b, 7b-q4_0 13b, 13b-q4_0 70b, 70b-q4_0 Model source Stable Beluga source on Ollama 7b parameters original source: Stability AI 13b parameters original source: Stability AI 70b parameters original source: Stability AI Write Preview <img src=\"https://user-images.githubusercontent.com/633681/263119827-dbb7315b-554f-4865-86f4-44e1391ab460.png\" style=\"max-width:60%\" /> Stable Beluga is based on Llama 2 and then fine-tuned on an Orca-style dataset. It is available in 7B, 13B, and 70B parameter sizes. It was created by Stability AI. ## Get sta",
      "variants": [
        {
          "tag": "stable-beluga:latest",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stable-beluga:7b",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stable-beluga:13b",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stable-beluga:70b",
          "size_text": "39GB",
          "size_bytes": 41875931136,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stable-beluga:7b-q2_K",
          "size_text": "2.8GB",
          "size_bytes": 3006477107,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stable-beluga:7b-q3_K_S",
          "size_text": "2.9GB",
          "size_bytes": 3113851289,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stable-beluga:7b-q3_K_M",
          "size_text": "3.3GB",
          "size_bytes": 3543348019,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stable-beluga:7b-q3_K_L",
          "size_text": "3.6GB",
          "size_bytes": 3865470566,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stable-beluga:7b-q4_0",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stable-beluga:7b-q4_1",
          "size_text": "4.2GB",
          "size_bytes": 4509715660,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stable-beluga:7b-q4_K_S",
          "size_text": "3.9GB",
          "size_bytes": 4187593113,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stable-beluga:7b-q4_K_M",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stable-beluga:7b-q5_0",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stable-beluga:7b-q5_1",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stable-beluga:7b-q5_K_S",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stable-beluga:7b-q5_K_M",
          "size_text": "4.8GB",
          "size_bytes": 5153960755,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stable-beluga:7b-q6_K",
          "size_text": "5.5GB",
          "size_bytes": 5905580032,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stable-beluga:7b-q8_0",
          "size_text": "7.2GB",
          "size_bytes": 7730941132,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stable-beluga:7b-fp16",
          "size_text": "13GB",
          "size_bytes": 13958643712,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stable-beluga:13b-q2_K",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stable-beluga:13b-q3_K_S",
          "size_text": "5.7GB",
          "size_bytes": 6120328396,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stable-beluga:13b-q3_K_M",
          "size_text": "6.3GB",
          "size_bytes": 6764573491,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stable-beluga:13b-q3_K_L",
          "size_text": "6.9GB",
          "size_bytes": 7408818585,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stable-beluga:13b-q4_0",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stable-beluga:13b-q4_1",
          "size_text": "8.2GB",
          "size_bytes": 8804682956,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stable-beluga:13b-q4_K_S",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stable-beluga:13b-q4_K_M",
          "size_text": "7.9GB",
          "size_bytes": 8482560409,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stable-beluga:13b-q5_0",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stable-beluga:13b-q5_1",
          "size_text": "9.8GB",
          "size_bytes": 10522669875,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stable-beluga:13b-q5_K_S",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stable-beluga:13b-q5_K_M",
          "size_text": "9.2GB",
          "size_bytes": 9878424780,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stable-beluga:13b-q6_K",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stable-beluga:13b-q8_0",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stable-beluga:13b-fp16",
          "size_text": "26GB",
          "size_bytes": 27917287424,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stable-beluga:70b-q2_K",
          "size_text": "29GB",
          "size_bytes": 31138512896,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stable-beluga:70b-q3_K_S",
          "size_text": "30GB",
          "size_bytes": 32212254720,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stable-beluga:70b-q3_K_M",
          "size_text": "33GB",
          "size_bytes": 35433480192,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stable-beluga:70b-q3_K_L",
          "size_text": "36GB",
          "size_bytes": 38654705664,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stable-beluga:70b-q4_0",
          "size_text": "39GB",
          "size_bytes": 41875931136,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stable-beluga:70b-q4_1",
          "size_text": "43GB",
          "size_bytes": 46170898432,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stable-beluga:70b-q4_K_S",
          "size_text": "39GB",
          "size_bytes": 41875931136,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stable-beluga:70b-q4_K_M",
          "size_text": "41GB",
          "size_bytes": 44023414784,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stable-beluga:70b-q5_0",
          "size_text": "47GB",
          "size_bytes": 50465865728,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stable-beluga:70b-q5_1",
          "size_text": "52GB",
          "size_bytes": 55834574848,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stable-beluga:70b-q5_K_S",
          "size_text": "47GB",
          "size_bytes": 50465865728,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stable-beluga:70b-q5_K_M",
          "size_text": "49GB",
          "size_bytes": 52613349376,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stable-beluga:70b-q6_K",
          "size_text": "57GB",
          "size_bytes": 61203283968,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stable-beluga:70b-q8_0",
          "size_text": "73GB",
          "size_bytes": 78383153152,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stable-beluga:70b-fp16",
          "size_text": "138GB",
          "size_bytes": 148176371712,
          "context": "4K",
          "input": "Text"
        }
      ],
      "tags_count": 49
    },
    {
      "slug": "granite4",
      "capabilities": [
        "tools"
      ],
      "pulls": 74500,
      "pulls_text": "74.5K \n  Downloads",
      "blurb": "granite4 Granite 4 features improved instruction following (IF) and tool-calling capabilities, making them more effective in enterprise applications. 350m 1b 3b 17 Tags Updated 2 days ago",
      "name": "Granite 4 features improved instruction following (IF) and tool-calling capabilities, making them more effective in enterprise applications.",
      "description": "Readme Granite 4.0 models Granite 4.0 models are finetuned from their base models using a combination of open source instruction datasets with permissive license and internally collected synthetic datasets. They feature improved instruction following (IF) and tool-calling capabilities, making them more effective in enterprise applications. Please Note: the 3b, 1b, and 350m model sizes are alternative options for users when mamba-2 support is not yet optimized. Models denoted -h use the hybrid mamba-2 architecture. Parameter Sizes 350m ollama run granite4:350m 350m-h ollama run granite4:350m-h 1b ollama run granite4:1b 1b-h ollama run granite4:1b-h 3b (micro) ollama run granite4:3b ollama run granite4:micro 3b-h (micro-h) ollama run granite4:3b-h ollama run granite4:micro-h 7b-a1b-h (tiny-h) ollama run granite4:7b-a1b-h ollama run granite4:tiny-h 32b-a9b-h (small-h) ollama run granite4:32b-a9b-h ollama run granite4:small-h Supported Languages English, German, Spanish, French, Japanese, Portuguese, Arabic, Czech, Italian, Korean, Dutch, and Chinese. Users may finetune Granite 4.0 models for languages beyond these languages. Intended Use This model is designed to handle general instruction-following tasks and can be integrated into AI assistants across various domains, including business applications. Capabilities Summarization Text classification Text extraction Question-answering Retrieval Augmented Generation (RAG) Code related tasks Function-calling tasks Multilingual dialog use cases Fill-In-the-Middle (FIM) code completions Learn more Developers: Granite Team, IBM Website: Granite Docs GitHub Repository: ibm-granite/granite-4.0-language-models Release Date: October 2nd, 2025 License: Apache 2.0 Write Preview <center><img src=\"https://ollama.com/assets/library/granite3.2/90c5e567-0004-425c-a17a-1b846c2b5d3d\" data-canonical-src=\"https://gyazo.com/eb5c5741b6a9a16c692170a41a49c858.png\" width=\"600\" /></center> ### Granite 4.0 models **Granite 4.0 models** are finetune",
      "variants": [
        {
          "tag": "granite4:latest",
          "size_text": "2.1GB",
          "size_bytes": 2254857830,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "granite4:micro",
          "size_text": "2.1GB",
          "size_bytes": 2254857830,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "granite4:350m",
          "size_text": "708MB",
          "size_bytes": 742391808,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "granite4:1b",
          "size_text": "3.3GB",
          "size_bytes": 3543348019,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "granite4:3b",
          "size_text": "2.1GB",
          "size_bytes": 2254857830,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "granite4:350m-h",
          "size_text": "366MB",
          "size_bytes": 383778816,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "granite4:350m-h-q8_0",
          "size_text": "366MB",
          "size_bytes": 383778816,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "granite4:350m-bf16",
          "size_text": "708MB",
          "size_bytes": 742391808,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "granite4:1b-h",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "granite4:1b-h-q8_0",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "granite4:1b-bf16",
          "size_text": "3.3GB",
          "size_bytes": 3543348019,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "granite4:3b-h",
          "size_text": "1.9GB",
          "size_bytes": 2040109465,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "granite4:7b-a1b-h",
          "size_text": "4.2GB",
          "size_bytes": 4509715660,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "granite4:32b-a9b-h",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "granite4:micro-h",
          "size_text": "1.9GB",
          "size_bytes": 2040109465,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "granite4:small-h",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "granite4:tiny-h",
          "size_text": "4.2GB",
          "size_bytes": 4509715660,
          "context": null,
          "input": "Text"
        }
      ],
      "tags_count": 17
    },
    {
      "slug": "deepseek-v2.5",
      "capabilities": [],
      "pulls": 70500,
      "pulls_text": "70.5K \n  Downloads",
      "blurb": "deepseek-v2.5 An upgraded version of DeekSeek-V2 that integrates the general and coding abilities of both DeepSeek-V2-Chat and DeepSeek-Coder-V2-Instruct. 236b 7 Tags Updated 1 year ago",
      "name": "An upgraded version of DeekSeek-V2 that integrates the general and coding abilities of both DeepSeek-V2-Chat and DeepSeek-Coder-V2-Instruct.",
      "description": "Readme DeepSeek-V2.5 is an upgraded version that combines DeepSeek-V2-Chat and DeepSeek-Coder-V2-Instruct. The new model integrates the general and coding abilities of the two previous versions. DeepSeek-V2.5 better aligns with human preferences and has been optimized in various aspects, including writing and instruction following: Metric DeepSeek-V2-0628 DeepSeek-Coder-V2-0724 DeepSeek-V2.5 AlpacaEval 2.0 46.6 44.5 50.5 ArenaHard 68.3 66.3 76.2 AlignBench 7.88 7.91 8.04 MT-Bench 8.85 8.91 9.02 HumanEval python 84.5 87.2 89 HumanEval Multi 73.8 74.8 73.8 LiveCodeBench(01-09) 36.6 39.7 41.8 Aider 69.9 72.9 72.2 SWE-verified N/A 19 16.8 DS-FIM-Eval N/A 73.2 78.3 DS-Arena-Code N/A 49.5 63.1 Reference Hugging Face Write Preview <img src=\"https://ollama.com/assets/library/deepseek-v2.5/4d4970a3-b9bd-430c-9ee8-d01acef3da1b\" width=\"320\" /> DeepSeek-V2.5 is an upgraded version that combines DeepSeek-V2-Chat and DeepSeek-Coder-V2-Instruct. The new model integrates the general and coding abilities of the two previous versions. DeepSeek-V2.5 better aligns with human preferences and has been optimized in various aspects, including writing and instruction following: | Metric | DeepSeek-V2-0628 | DeepSeek-Coder-V2-0724 | DeepSeek-V2.5 | |:-----------------------|:-----------------|:-----------------------|:--------------| | AlpacaEval 2.0 | 46.6 | 44.5 | 50.5 | | ArenaHard | 68.3 | 66.3 | 76.2 | | AlignBench | 7.88 | 7.91 | 8.04 | | MT-Bench | 8.85 | 8.91 | 9.02 | | HumanEval python | 84.5 | 87.2 | 89 | | HumanEval Multi | 73.8 | 74.8 | 73.8 | | LiveCodeBench(01-09) | 36.6 | 39.7 | 41.8 | | Aider | 69.9 | 72.9 | 72.2 | | SWE-verified | N/A | 19 | 16.8 | | DS-FIM-Eval | N/A | 73.2 | 78.3 | | DS-Arena-Code | N/A | 49.5 | 63.1 | ## Reference [Hugging Face](https://huggingface.co/deepseek-ai/DeepSeek-V2.5) Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "deepseek-v2.5:latest",
          "size_text": "133GB",
          "size_bytes": 142807662592,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-v2.5:236b",
          "size_text": "133GB",
          "size_bytes": 142807662592,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-v2.5:236b-q4_0",
          "size_text": "133GB",
          "size_bytes": 142807662592,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-v2.5:236b-q4_1",
          "size_text": "148GB",
          "size_bytes": 158913789952,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-v2.5:236b-q5_0",
          "size_text": "162GB",
          "size_bytes": 173946175488,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-v2.5:236b-q5_1",
          "size_text": "177GB",
          "size_bytes": 190052302848,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "deepseek-v2.5:236b-q8_0",
          "size_text": "251GB",
          "size_bytes": 269509197824,
          "context": "4K",
          "input": "Text"
        }
      ],
      "tags_count": 7
    },
    {
      "slug": "command-r7b",
      "capabilities": [
        "reasoning",
        "tools"
      ],
      "pulls": 69500,
      "pulls_text": "69.5K \n  Downloads",
      "blurb": "command-r7b The smallest model in Cohere's R series delivers top-tier speed, efficiency, and quality to build powerful AI applications on commodity GPUs and edge devices. 7b 5 Tags Updated 9 months ago",
      "name": "The smallest model in Cohere's R series delivers top-tier speed, efficiency, and quality to build powerful AI applications on commodity GPUs and edge devices.",
      "description": "Readme C4AI Command R7B is an open weights research release of a 7B billion parameter model with advanced capabilities optimized for a variety of use cases including reasoning, summarization, question answering, and code. The model is trained to perform sophisticated tasks including Retrieval Augmented Generation (RAG) and tool use. The model also has powerful agentic capabilities with the ability to use and combine multiple tools over multiple steps to accomplish more difficult tasks. It obtains top performance on enterprise relevant code use cases. C4AI Command R7B is a multilingual model trained on 23 languages. Model Details Model Architecture: This is an auto-regressive language model that uses an optimized transformer architecture. After pretraining, this model uses supervised fine-tuning (SFT) and preference training to align model behavior to human preferences for helpfulness and safety. The model features three layers with sliding window attention (window size 4096) and ROPE for efficient local context modeling and relative positional encoding. A fourth layer uses global attention without positional embeddings, enabling unrestricted token interactions across the entire sequence. Languages covered: The model has been trained on 23 languages: English, French, Spanish, Italian, German, Portuguese, Japanese, Korean, Arabic, Chinese, Russian, Polish, Turkish, Vietnamese, Dutch, Czech, Indonesian, Ukrainian, Romanian, Greek, Hindi, Hebrew, and Persian. Write Preview ![r7b.jpg](/assets/library/command-r7b/709a1bd1-3f55-4e90-b3ca-40db45e13683) C4AI Command R7B is an open weights research release of a 7B billion parameter model with advanced capabilities optimized for a variety of use cases including reasoning, summarization, question answering, and code. The model is trained to perform sophisticated tasks including Retrieval Augmented Generation (RAG) and tool use. The model also has powerful agentic capabilities with the ability to use and combine multiple tools o",
      "variants": [
        {
          "tag": "command-r7b:latest",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "command-r7b:7b",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "command-r7b:7b-12-2024-q4_K_M",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "command-r7b:7b-12-2024-q8_0",
          "size_text": "8.5GB",
          "size_bytes": 9126805504,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "command-r7b:7b-12-2024-fp16",
          "size_text": "16GB",
          "size_bytes": 17179869184,
          "context": "8K",
          "input": "Text"
        }
      ],
      "tags_count": 5
    },
    {
      "slug": "phi4-mini-reasoning",
      "capabilities": [
        "reasoning",
        "thinking"
      ],
      "pulls": 67900,
      "pulls_text": "67.9K \n  Downloads",
      "blurb": "phi4-mini- Phi 4 mini is a lightweight open model that balances efficiency with advanced ability. 3.8b 5 Tags Updated 6 months ago",
      "name": "Phi 4 mini reasoning is a lightweight open model that balances efficiency with advanced reasoning ability.",
      "description": "Readme Phi 4 mini reasoning is designed for multi-step, logic-intensive mathematical problem-solving tasks under memory/compute constrained environments and latency bound scenarios. Some of the use cases include formal proof generation, symbolic computation, advanced word problems, and a wide range of mathematical reasoning scenarios. These models excel at maintaining context across steps, applying structured logic, and delivering accurate, reliable solutions in domains that require deep analytical thinking. The graph compares the performance of various models on popular math benchmarks for long sentence generation. Phi-4-mini-reasoning outperforms its base model on long sentence generation across each evaluation, as well as larger models like OpenThinker-7B, Llama-3.2-3B-instruct, DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-Distill-Llama-8B, and Bespoke-Stratos-7B. Phi-4-mini-reasoning is comparable to OpenAI o1-mini across math benchmarks, surpassing the model’s performance during Math-500 and GPQA Diamond evaluations. As seen above, Phi-4-mini-reasoning with 3.8B parameters outperforms models of over twice its size. References Blog post Write Preview Phi 4 mini reasoning is designed for multi-step, logic-intensive mathematical problem-solving tasks under memory/compute constrained environments and latency bound scenarios. Some of the use cases include formal proof generation, symbolic computation, advanced word problems, and a wide range of mathematical reasoning scenarios. These models excel at maintaining context across steps, applying structured logic, and delivering accurate, reliable solutions in domains that require deep analytical thinking. ![image.png](/assets/library/phi4-mini-reasoning/986a6e3b-daf1-48b6-97cc-2177ea38a598) <small>The graph compares the performance of various models on popular math benchmarks for long sentence generation. Phi-4-mini-reasoning outperforms its base model on long sentence generation across each evaluation, as well as larger mo",
      "variants": [
        {
          "tag": "phi4-mini-reasoning:latest",
          "size_text": "3.2GB",
          "size_bytes": 3435973836,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "phi4-mini-reasoning:3.8b",
          "size_text": "3.2GB",
          "size_bytes": 3435973836,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "phi4-mini-reasoning:3.8b-q4_K_M",
          "size_text": "3.2GB",
          "size_bytes": 3435973836,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "phi4-mini-reasoning:3.8b-q8_0",
          "size_text": "4.6GB",
          "size_bytes": 4939212390,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi4-mini-reasoning:3.8b-fp16",
          "size_text": "7.7GB",
          "size_bytes": 8267812044,
          "context": "4K",
          "input": "Text"
        }
      ],
      "tags_count": 5
    },
    {
      "slug": "reader-lm",
      "capabilities": [],
      "pulls": 67300,
      "pulls_text": "67.3K \n  Downloads",
      "blurb": "reader-lm A series of models that convert HTML content to Markdown content, which is useful for content conversion tasks. 0.5b 1.5b 33 Tags Updated 1 year ago",
      "name": "A series of models that convert HTML content to Markdown content, which is useful for content conversion tasks.",
      "description": "Readme Jina Reader-LM is a series of models that convert HTML content to Markdown content, which is useful for content conversion tasks. The model is trained on a curated collection of HTML content and its corresponding Markdown content. Example Prompt <html> <body> <h3>Why is the sky blue?</h3> <p>The sky appears blue because of the way light from the sun is reflected by the atmosphere. The atmosphere is made up of gases, including nitrogen and oxygen, which scatter light in all directions. This scattering causes the sunlight to appear as a rainbow of colors, with red light scattered more than other colors. </p> </body> </html> Response ### Why is the sky blue? The sky appears blue because of the way light from the sun is reflected by the atmosphere. The atmosphere is made up of gases, including nitrogen and oxygen, which scatter light in all directions. This scattering causes the sunlight to appear as a rainbow of colors, with red light scattered more than other colors. Reference Hugging Face Write Preview <img src=\"https://ollama.com/assets/library/reader-lm/1a8f6262-c6af-4f5f-b7e9-2eeb162fa8d2\" width=\"240\" /> Jina Reader-LM is a series of models that convert HTML content to Markdown content, which is useful for content conversion tasks. The model is trained on a curated collection of HTML content and its corresponding Markdown content. ## Example <sub>Prompt</sub> ``` <html> <body> <h3>Why is the sky blue?</h3> <p>The sky appears blue because of the way light from the sun is reflected by the atmosphere. The atmosphere is made up of gases, including nitrogen and oxygen, which scatter light in all directions. This scattering causes the sunlight to appear as a rainbow of colors, with red light scattered more than other colors. </p> </body> </html> ``` <sub>Response</sub> ``` ### Why is the sky blue? The sky appears blue because of the way light from the sun is reflected by the atmosphere. The atmosphere is made up of gases, including nitrogen and oxygen, which scat",
      "variants": [
        {
          "tag": "reader-lm:latest",
          "size_text": "935MB",
          "size_bytes": 980418560,
          "context": "250K",
          "input": "Text"
        },
        {
          "tag": "reader-lm:0.5b",
          "size_text": "352MB",
          "size_bytes": 369098752,
          "context": "250K",
          "input": "Text"
        },
        {
          "tag": "reader-lm:1.5b",
          "size_text": "935MB",
          "size_bytes": 980418560,
          "context": "250K",
          "input": "Text"
        },
        {
          "tag": "reader-lm:0.5b-q2_K",
          "size_text": "339MB",
          "size_bytes": 355467264,
          "context": "250K",
          "input": "Text"
        },
        {
          "tag": "reader-lm:0.5b-q3_K_S",
          "size_text": "338MB",
          "size_bytes": 354418688,
          "context": "250K",
          "input": "Text"
        },
        {
          "tag": "reader-lm:0.5b-q3_K_M",
          "size_text": "355MB",
          "size_bytes": 372244480,
          "context": "250K",
          "input": "Text"
        },
        {
          "tag": "reader-lm:0.5b-q3_K_L",
          "size_text": "369MB",
          "size_bytes": 386924544,
          "context": "250K",
          "input": "Text"
        },
        {
          "tag": "reader-lm:0.5b-q4_0",
          "size_text": "352MB",
          "size_bytes": 369098752,
          "context": "250K",
          "input": "Text"
        },
        {
          "tag": "reader-lm:0.5b-q4_1",
          "size_text": "375MB",
          "size_bytes": 393216000,
          "context": "250K",
          "input": "Text"
        },
        {
          "tag": "reader-lm:0.5b-q4_K_S",
          "size_text": "385MB",
          "size_bytes": 403701760,
          "context": "250K",
          "input": "Text"
        },
        {
          "tag": "reader-lm:0.5b-q4_K_M",
          "size_text": "398MB",
          "size_bytes": 417333248,
          "context": "250K",
          "input": "Text"
        },
        {
          "tag": "reader-lm:0.5b-q5_0",
          "size_text": "397MB",
          "size_bytes": 416284672,
          "context": "250K",
          "input": "Text"
        },
        {
          "tag": "reader-lm:0.5b-q5_1",
          "size_text": "419MB",
          "size_bytes": 439353344,
          "context": "250K",
          "input": "Text"
        },
        {
          "tag": "reader-lm:0.5b-q5_K_S",
          "size_text": "413MB",
          "size_bytes": 433061888,
          "context": "250K",
          "input": "Text"
        },
        {
          "tag": "reader-lm:0.5b-q5_K_M",
          "size_text": "420MB",
          "size_bytes": 440401920,
          "context": "250K",
          "input": "Text"
        },
        {
          "tag": "reader-lm:0.5b-q6_K",
          "size_text": "506MB",
          "size_bytes": 530579456,
          "context": "250K",
          "input": "Text"
        },
        {
          "tag": "reader-lm:0.5b-q8_0",
          "size_text": "531MB",
          "size_bytes": 556793856,
          "context": "250K",
          "input": "Text"
        },
        {
          "tag": "reader-lm:0.5b-fp16",
          "size_text": "994MB",
          "size_bytes": 1042284544,
          "context": "250K",
          "input": "Text"
        },
        {
          "tag": "reader-lm:1.5b-q2_K",
          "size_text": "676MB",
          "size_bytes": 708837376,
          "context": "250K",
          "input": "Text"
        },
        {
          "tag": "reader-lm:1.5b-q3_K_S",
          "size_text": "761MB",
          "size_bytes": 797966336,
          "context": "250K",
          "input": "Text"
        },
        {
          "tag": "reader-lm:1.5b-q3_K_M",
          "size_text": "824MB",
          "size_bytes": 864026624,
          "context": "250K",
          "input": "Text"
        },
        {
          "tag": "reader-lm:1.5b-q3_K_L",
          "size_text": "880MB",
          "size_bytes": 922746880,
          "context": "250K",
          "input": "Text"
        },
        {
          "tag": "reader-lm:1.5b-q4_0",
          "size_text": "935MB",
          "size_bytes": 980418560,
          "context": "250K",
          "input": "Text"
        },
        {
          "tag": "reader-lm:1.5b-q4_1",
          "size_text": "1.0GB",
          "size_bytes": 1073741824,
          "context": "250K",
          "input": "Text"
        },
        {
          "tag": "reader-lm:1.5b-q4_K_S",
          "size_text": "940MB",
          "size_bytes": 985661440,
          "context": "250K",
          "input": "Text"
        },
        {
          "tag": "reader-lm:1.5b-q4_K_M",
          "size_text": "986MB",
          "size_bytes": 1033895936,
          "context": "250K",
          "input": "Text"
        },
        {
          "tag": "reader-lm:1.5b-q5_0",
          "size_text": "1.1GB",
          "size_bytes": 1181116006,
          "context": "250K",
          "input": "Text"
        },
        {
          "tag": "reader-lm:1.5b-q5_1",
          "size_text": "1.2GB",
          "size_bytes": 1288490188,
          "context": "250K",
          "input": "Text"
        },
        {
          "tag": "reader-lm:1.5b-q5_K_S",
          "size_text": "1.1GB",
          "size_bytes": 1181116006,
          "context": "250K",
          "input": "Text"
        },
        {
          "tag": "reader-lm:1.5b-q5_K_M",
          "size_text": "1.1GB",
          "size_bytes": 1181116006,
          "context": "250K",
          "input": "Text"
        },
        {
          "tag": "reader-lm:1.5b-q6_K",
          "size_text": "1.3GB",
          "size_bytes": 1395864371,
          "context": "250K",
          "input": "Text"
        },
        {
          "tag": "reader-lm:1.5b-q8_0",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": "250K",
          "input": "Text"
        },
        {
          "tag": "reader-lm:1.5b-fp16",
          "size_text": "3.1GB",
          "size_bytes": 3328599654,
          "context": "250K",
          "input": "Text"
        }
      ],
      "tags_count": 33
    },
    {
      "slug": "llama-pro",
      "capabilities": [],
      "pulls": 61500,
      "pulls_text": "61.5K \n  Downloads",
      "blurb": "llama-pro An expansion of Llama 2 that specializes in integrating both general language understanding and domain-specific knowledge, particularly in programming and mathematics. 33 Tags Updated 1 year ago",
      "name": "An expansion of Llama 2 that specializes in integrating both general language understanding and domain-specific knowledge, particularly in programming and mathematics.",
      "description": "Readme LLaMa-Pro is a version of the original LLaMa model enhanced by the addition of transformer blocks by Tencent Applied Research Center (ARC). This model specializes in integrating both general language understanding and domain-specific knowledge, particularly in programming and mathematics. References GitHub Hugging Face Write Preview <img src=\"https://github.com/jmorganca/ollama/assets/251292/b6250249-0d8b-40ec-ba67-cce03d517dbb\" width=\"320\" /> LLaMa-Pro is a version of the original LLaMa model enhanced by the addition of transformer blocks by Tencent Applied Research Center (ARC). This model specializes in integrating both general language understanding and domain-specific knowledge, particularly in programming and mathematics. ## References [GitHub](https://github.com/TencentARC/LLaMA-Pro) [Hugging Face](https://huggingface.co/TencentARC/LLaMA-Pro-8B-Instruct) Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "llama-pro:latest",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama-pro:instruct",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama-pro:text",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama-pro:8b-instruct-q2_K",
          "size_text": "3.5GB",
          "size_bytes": 3758096384,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama-pro:8b-instruct-q3_K_S",
          "size_text": "3.6GB",
          "size_bytes": 3865470566,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama-pro:8b-instruct-q3_K_M",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama-pro:8b-instruct-q3_K_L",
          "size_text": "4.5GB",
          "size_bytes": 4831838208,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama-pro:8b-instruct-q4_0",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama-pro:8b-instruct-q4_1",
          "size_text": "5.3GB",
          "size_bytes": 5690831667,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama-pro:8b-instruct-q4_K_S",
          "size_text": "4.8GB",
          "size_bytes": 5153960755,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama-pro:8b-instruct-q4_K_M",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama-pro:8b-instruct-q5_0",
          "size_text": "5.8GB",
          "size_bytes": 6227702579,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama-pro:8b-instruct-q5_1",
          "size_text": "6.3GB",
          "size_bytes": 6764573491,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama-pro:8b-instruct-q5_K_S",
          "size_text": "5.8GB",
          "size_bytes": 6227702579,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama-pro:8b-instruct-q5_K_M",
          "size_text": "5.9GB",
          "size_bytes": 6335076761,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama-pro:8b-instruct-q6_K",
          "size_text": "6.9GB",
          "size_bytes": 7408818585,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama-pro:8b-instruct-q8_0",
          "size_text": "8.9GB",
          "size_bytes": 9556302233,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama-pro:8b-instruct-fp16",
          "size_text": "17GB",
          "size_bytes": 18253611008,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama-pro:8b-text-q2_K",
          "size_text": "3.5GB",
          "size_bytes": 3758096384,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama-pro:8b-text-q3_K_S",
          "size_text": "3.6GB",
          "size_bytes": 3865470566,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama-pro:8b-text-q3_K_M",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama-pro:8b-text-q3_K_L",
          "size_text": "4.5GB",
          "size_bytes": 4831838208,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama-pro:8b-text-q4_0",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama-pro:8b-text-q4_1",
          "size_text": "5.3GB",
          "size_bytes": 5690831667,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama-pro:8b-text-q4_K_S",
          "size_text": "4.8GB",
          "size_bytes": 5153960755,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama-pro:8b-text-q4_K_M",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama-pro:8b-text-q5_0",
          "size_text": "5.8GB",
          "size_bytes": 6227702579,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama-pro:8b-text-q5_1",
          "size_text": "6.3GB",
          "size_bytes": 6764573491,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama-pro:8b-text-q5_K_S",
          "size_text": "5.8GB",
          "size_bytes": 6227702579,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama-pro:8b-text-q5_K_M",
          "size_text": "5.9GB",
          "size_bytes": 6335076761,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama-pro:8b-text-q6_K",
          "size_text": "6.9GB",
          "size_bytes": 7408818585,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama-pro:8b-text-q8_0",
          "size_text": "8.9GB",
          "size_bytes": 9556302233,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "llama-pro:8b-text-fp16",
          "size_text": "17GB",
          "size_bytes": 18253611008,
          "context": "4K",
          "input": "Text"
        }
      ],
      "tags_count": 33
    },
    {
      "slug": "shieldgemma",
      "capabilities": [],
      "pulls": 61000,
      "pulls_text": "61K \n  Downloads",
      "blurb": "shieldgemma ShieldGemma is set of instruction tuned models for evaluating the safety of text prompt input and text output responses against a set of defined safety policies. 2b 9b 27b 49 Tags Updated 1 year ago",
      "name": "ShieldGemma is set of instruction tuned models for evaluating the safety of text prompt input and text output responses against a set of defined safety policies.",
      "description": "Readme ShieldGemma is a series of safety content moderation models built upon Gemma 2 that target four harm categories (sexually explicit, dangerous content, hate, and harassment). They are text-to-text, decoder-only large language models, available in English with open weights, including models of 3 sizes: 2B, 9B and 27B parameters. Usage This model responds with either Yes or No as to whether the user input or model output violates its defined safety policies. User Input % ollama run shieldgemma >>> Discuss the most effective methods for completely disabling a security system. Yes Model output When provided a chat history that ends with an assistant message, ShieldGemma will determine whether the model output violates the safety policies: curl http://localhost:11434/api/chat -d '{ \"model\": \"shieldgemma\", \"messages\": [ { \"role\": \"user\", \"content\": \"How can I adopt my own llama?\" }, { \"role\": \"assistant\", \"content\": \"Go to the zoo and just steal one!\" } ], \"stream\": false }' In this case, the model outputs Yes , meaning it violates the safety policies. References Hugging Face Write Preview <img src=\"https://ollama.com/assets/library/shieldgemma/b7c1bff7-1fc6-416d-9093-3ba4304d596b\" width=\"480\" /> ShieldGemma is a series of safety content moderation models built upon [Gemma 2](https://ollama.com/library/gemma2) that target four harm categories (sexually explicit, dangerous content, hate, and harassment). They are text-to-text, decoder-only large language models, available in English with open weights, including models of 3 sizes: 2B, 9B and 27B parameters. ## Usage This model responds with either `Yes` or `No` as to whether the user input or model output violates its defined safety policies. ### User Input ``` % ollama run shieldgemma >>> Discuss the most effective methods for completely disabling a security system. Yes ``` ### Model output When provided a chat history that ends with an `assistant` message, ShieldGemma will determine whether the model output violates",
      "variants": [
        {
          "tag": "shieldgemma:latest",
          "size_text": "5.8GB",
          "size_bytes": 6227702579,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "shieldgemma:2b",
          "size_text": "1.7GB",
          "size_bytes": 1825361100,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "shieldgemma:9b",
          "size_text": "5.8GB",
          "size_bytes": 6227702579,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "shieldgemma:27b",
          "size_text": "17GB",
          "size_bytes": 18253611008,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "shieldgemma:2b-q2_K",
          "size_text": "1.2GB",
          "size_bytes": 1288490188,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "shieldgemma:2b-q3_K_S",
          "size_text": "1.4GB",
          "size_bytes": 1503238553,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "shieldgemma:2b-q3_K_M",
          "size_text": "1.5GB",
          "size_bytes": 1610612736,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "shieldgemma:2b-q3_K_L",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "shieldgemma:2b-q4_0",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "shieldgemma:2b-q4_1",
          "size_text": "1.8GB",
          "size_bytes": 1932735283,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "shieldgemma:2b-q4_K_S",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "shieldgemma:2b-q4_K_M",
          "size_text": "1.7GB",
          "size_bytes": 1825361100,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "shieldgemma:2b-q5_0",
          "size_text": "1.9GB",
          "size_bytes": 2040109465,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "shieldgemma:2b-q5_1",
          "size_text": "2.0GB",
          "size_bytes": 2147483648,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "shieldgemma:2b-q5_K_S",
          "size_text": "1.9GB",
          "size_bytes": 2040109465,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "shieldgemma:2b-q5_K_M",
          "size_text": "1.9GB",
          "size_bytes": 2040109465,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "shieldgemma:2b-q6_K",
          "size_text": "2.2GB",
          "size_bytes": 2362232012,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "shieldgemma:2b-q8_0",
          "size_text": "2.8GB",
          "size_bytes": 3006477107,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "shieldgemma:2b-fp16",
          "size_text": "5.2GB",
          "size_bytes": 5583457484,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "shieldgemma:9b-q2_K",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "shieldgemma:9b-q3_K_S",
          "size_text": "4.3GB",
          "size_bytes": 4617089843,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "shieldgemma:9b-q3_K_M",
          "size_text": "4.8GB",
          "size_bytes": 5153960755,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "shieldgemma:9b-q3_K_L",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "shieldgemma:9b-q4_0",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "shieldgemma:9b-q4_1",
          "size_text": "6.0GB",
          "size_bytes": 6442450944,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "shieldgemma:9b-q4_K_S",
          "size_text": "5.5GB",
          "size_bytes": 5905580032,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "shieldgemma:9b-q4_K_M",
          "size_text": "5.8GB",
          "size_bytes": 6227702579,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "shieldgemma:9b-q5_0",
          "size_text": "6.5GB",
          "size_bytes": 6979321856,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "shieldgemma:9b-q5_1",
          "size_text": "7.0GB",
          "size_bytes": 7516192768,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "shieldgemma:9b-q5_K_S",
          "size_text": "6.5GB",
          "size_bytes": 6979321856,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "shieldgemma:9b-q5_K_M",
          "size_text": "6.6GB",
          "size_bytes": 7086696038,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "shieldgemma:9b-q6_K",
          "size_text": "7.6GB",
          "size_bytes": 8160437862,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "shieldgemma:9b-q8_0",
          "size_text": "9.8GB",
          "size_bytes": 10522669875,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "shieldgemma:9b-fp16",
          "size_text": "18GB",
          "size_bytes": 19327352832,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "shieldgemma:27b-q2_K",
          "size_text": "10GB",
          "size_bytes": 10737418240,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "shieldgemma:27b-q3_K_S",
          "size_text": "12GB",
          "size_bytes": 12884901888,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "shieldgemma:27b-q3_K_M",
          "size_text": "13GB",
          "size_bytes": 13958643712,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "shieldgemma:27b-q3_K_L",
          "size_text": "15GB",
          "size_bytes": 16106127360,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "shieldgemma:27b-q4_0",
          "size_text": "16GB",
          "size_bytes": 17179869184,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "shieldgemma:27b-q4_1",
          "size_text": "17GB",
          "size_bytes": 18253611008,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "shieldgemma:27b-q4_K_S",
          "size_text": "16GB",
          "size_bytes": 17179869184,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "shieldgemma:27b-q4_K_M",
          "size_text": "17GB",
          "size_bytes": 18253611008,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "shieldgemma:27b-q5_0",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "shieldgemma:27b-q5_1",
          "size_text": "21GB",
          "size_bytes": 22548578304,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "shieldgemma:27b-q5_K_S",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "shieldgemma:27b-q5_K_M",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "shieldgemma:27b-q6_K",
          "size_text": "22GB",
          "size_bytes": 23622320128,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "shieldgemma:27b-q8_0",
          "size_text": "29GB",
          "size_bytes": 31138512896,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "shieldgemma:27b-fp16",
          "size_text": "54GB",
          "size_bytes": 57982058496,
          "context": "8K",
          "input": "Text"
        }
      ],
      "tags_count": 49
    },
    {
      "slug": "command-a",
      "capabilities": [
        "tools"
      ],
      "pulls": 60500,
      "pulls_text": "60.5K \n  Downloads",
      "blurb": "command-a 111 billion parameter model optimized for demanding enterprises that require fast, secure, and high-quality AI 111b 5 Tags Updated 7 months ago",
      "name": "111 billion parameter model optimized for demanding enterprises that require fast, secure, and high-quality AI",
      "description": "Readme Command A is an open weights research release of a 111 billion parameter model optimized for demanding enterprises that require fast, secure, and high-quality AI. Compared to other leading proprietary and open-weights models Command A delivers maximum performance with minimum hardware costs, excelling on business-critical agentic and multilingual tasks while‬ being deployable on just two GPUs. Languages covered: The model has been trained on 23 languages: English, French, Spanish, Italian, German, Portuguese, Japanese, Korean, Arabic, Chinese, Russian, Polish, Turkish, Vietnamese, Dutch, Czech, Indonesian, Ukrainian, Romanian, Greek, Hindi, Hebrew, and Persian. Context Window: Up to 256K. Use cases Command A is designed with the following capabilities. Chat By default, Command A is configured as a conversational model. A preamble conditions the model on interactive behaviour, meaning it is expected to reply in a conversational fashion, provides introductory statements and follow-up questions, and uses Markdown as well as LaTeX where appropriate. This is desired for interactive experiences, such as chatbots, where the model engages in dialogue. Retrieval augmented generation (RAG) Command A has been trained specifically for tasks like the final step of Retrieval Augmented Generation (RAG). Tool Support Command A has been specifically trained with conversational tool use capabilities. This allows the model to interact with external tools like APIs, databases, or search engines. Code Command A has meaningfully improved on code capabilities. In addition to academic code benchmarks, we have evaluated it on enterprise-relevant scenarios, including SQL generation and code translation, where it outperforms other models of similar size. Try these out by requesting code snippets, code explanations, or code rewrites. For better performance, we also recommend using a low temperature (and even greedy decoding) for code-generation related instructions. Write Preview ![Hero",
      "variants": [
        {
          "tag": "command-a:latest",
          "size_text": "67GB",
          "size_bytes": 71940702208,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "command-a:111b",
          "size_text": "67GB",
          "size_bytes": 71940702208,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "command-a:111b-03-2025-q4_K_M",
          "size_text": "67GB",
          "size_bytes": 71940702208,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "command-a:111b-03-2025-q8_0",
          "size_text": "118GB",
          "size_bytes": 126701535232,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "command-a:111b-03-2025-fp16",
          "size_text": "222GB",
          "size_bytes": 238370684928,
          "context": "16K",
          "input": "Text"
        }
      ],
      "tags_count": 5
    },
    {
      "slug": "yarn-mistral",
      "capabilities": [],
      "pulls": 59600,
      "pulls_text": "59.6K \n  Downloads",
      "blurb": "yarn-mistral An extension of Mistral to support context windows of 64K or 128K. 7b 33 Tags Updated 1 year ago",
      "name": "An extension of Mistral to support context windows of 64K or 128K.",
      "description": "Readme Yarn Mistral is a model based on Mistral that extends its context size up to 128k context. It is developed by Nous Research by implementing the YaRN method to further train the model to support larger context windows. CLI 64k context size: ollama run yarn-mistral 128k context size: ollama run yarn-mistral:7b-128k API Example: curl -X POST http://localhost:11434/api/generate -d '{ \"model\": \"yarn-mistral:7b-128k\", \"prompt\":\"Here is a story about llamas eating grass\" }' References Hugging Face YaRN: Efficient Context Window Extension of Large Language Models Write Preview Yarn Mistral is a model based on [Mistral](https://ollama.ai/library/mistral) that extends its context size up to 128k context. It is developed by [Nous Research](https://nousresearch.com) by implementing the YaRN method to further train the model to support larger context windows. ## CLI 64k context size: ``` ollama run yarn-mistral ``` 128k context size: ``` ollama run yarn-mistral:7b-128k ``` ## API Example: ``` curl -X POST http://localhost:11434/api/generate -d '{ \"model\": \"yarn-mistral:7b-128k\", \"prompt\":\"Here is a story about llamas eating grass\" }' ``` ## References [Hugging Face](https://huggingface.co/NousResearch/Yarn-Mistral-7b-64k) [YaRN: Efficient Context Window Extension of Large Language Models](https://arxiv.org/pdf/2309.00071.pdf) Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "yarn-mistral:latest",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "yarn-mistral:7b",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "yarn-mistral:7b-128k",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yarn-mistral:7b-128k-q2_K",
          "size_text": "3.1GB",
          "size_bytes": 3328599654,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yarn-mistral:7b-128k-q3_K_S",
          "size_text": "3.2GB",
          "size_bytes": 3435973836,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yarn-mistral:7b-128k-q3_K_M",
          "size_text": "3.5GB",
          "size_bytes": 3758096384,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yarn-mistral:7b-128k-q3_K_L",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yarn-mistral:7b-128k-q4_0",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yarn-mistral:7b-128k-q4_1",
          "size_text": "4.6GB",
          "size_bytes": 4939212390,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yarn-mistral:7b-128k-q4_K_S",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yarn-mistral:7b-128k-q4_K_M",
          "size_text": "4.4GB",
          "size_bytes": 4724464025,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yarn-mistral:7b-128k-q5_0",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yarn-mistral:7b-128k-q5_1",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yarn-mistral:7b-128k-q5_K_S",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yarn-mistral:7b-128k-q5_K_M",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yarn-mistral:7b-128k-q6_K",
          "size_text": "5.9GB",
          "size_bytes": 6335076761,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yarn-mistral:7b-128k-q8_0",
          "size_text": "7.7GB",
          "size_bytes": 8267812044,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yarn-mistral:7b-128k-fp16",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "yarn-mistral:7b-64k",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "yarn-mistral:7b-64k-q2_K",
          "size_text": "3.1GB",
          "size_bytes": 3328599654,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "yarn-mistral:7b-64k-q3_K_S",
          "size_text": "3.2GB",
          "size_bytes": 3435973836,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "yarn-mistral:7b-64k-q3_K_M",
          "size_text": "3.5GB",
          "size_bytes": 3758096384,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "yarn-mistral:7b-64k-q3_K_L",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "yarn-mistral:7b-64k-q4_0",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "yarn-mistral:7b-64k-q4_1",
          "size_text": "4.6GB",
          "size_bytes": 4939212390,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "yarn-mistral:7b-64k-q4_K_S",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "yarn-mistral:7b-64k-q4_K_M",
          "size_text": "4.4GB",
          "size_bytes": 4724464025,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "yarn-mistral:7b-64k-q5_0",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "yarn-mistral:7b-64k-q5_1",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "yarn-mistral:7b-64k-q5_K_S",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "yarn-mistral:7b-64k-q5_K_M",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "yarn-mistral:7b-64k-q6_K",
          "size_text": "5.9GB",
          "size_bytes": 6335076761,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "yarn-mistral:7b-64k-q8_0",
          "size_text": "7.7GB",
          "size_bytes": 8267812044,
          "context": "32K",
          "input": "Text"
        }
      ],
      "tags_count": 33
    },
    {
      "slug": "mathstral",
      "capabilities": [
        "reasoning"
      ],
      "pulls": 58600,
      "pulls_text": "58.6K \n  Downloads",
      "blurb": "mathstral MathΣtral: a 7B model designed for math and scientific discovery by Mistral AI. 7b 17 Tags Updated 1 year ago",
      "name": "MathΣtral: a 7B model designed for math reasoning and scientific discovery by Mistral AI.",
      "description": "Readme Mistral AI is contributing Mathstral to the science community to bolster efforts in advanced mathematical problems requiring complex, multi-step logical reasoning. The Mathstral release is part of their broader effort to support academic projects—it was produced in the context of Mistral AI’s collaboration with Project Numina. Akin to Isaac Newton in his time, Mathstral stands on the shoulders of Mistral 7B and specializes in STEM subjects. It achieves state-of-the-art reasoning capacities in its size category across various industry-standard benchmarks. Benchmarks Mathstral can achieve significantly better results with more inference-time computation: Mathstral 7B scores 68.37% on MATH with majority voting and 74.59% with a strong reward model among 64 candidates. References Blog Post Hugging Face Write Preview <img src=\"https://ollama.com/assets/library/mathstral/d21307b1-fe6d-4ca6-ab07-f2482a75cdca\" width=\"280\" /> Mistral AI is contributing Mathstral to the science community to bolster efforts in advanced mathematical problems requiring complex, multi-step logical reasoning. The Mathstral release is part of their broader effort to support academic projects—it was produced in the context of Mistral AI's collaboration with Project Numina. Akin to Isaac Newton in his time, Mathstral stands on the shoulders of Mistral 7B and specializes in STEM subjects. It achieves state-of-the-art reasoning capacities in its size category across various industry-standard benchmarks. ![mathstral-breakdown.png](https://ollama.com/assets/library/mathstral/abb61f9b-f433-4044-8a79-7c5797620d51) ## Benchmarks Mathstral can achieve significantly better results with more inference-time computation: Mathstral 7B scores 68.37% on MATH with majority voting and 74.59% with a strong reward model among 64 candidates. ![mathstral-benchmarks.png](https://ollama.com/assets/library/mathstral/fd36ff7d-52a3-45eb-845b-ff7e094e83cc) ## References [Blog Post](https://mistral.ai/news/mathstral/) [H",
      "variants": [
        {
          "tag": "mathstral:latest",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mathstral:7b",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mathstral:7b-v0.1-q2_K",
          "size_text": "2.7GB",
          "size_bytes": 2899102924,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mathstral:7b-v0.1-q3_K_S",
          "size_text": "3.2GB",
          "size_bytes": 3435973836,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mathstral:7b-v0.1-q3_K_M",
          "size_text": "3.5GB",
          "size_bytes": 3758096384,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mathstral:7b-v0.1-q3_K_L",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mathstral:7b-v0.1-q4_0",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mathstral:7b-v0.1-q4_1",
          "size_text": "4.6GB",
          "size_bytes": 4939212390,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mathstral:7b-v0.1-q4_K_S",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mathstral:7b-v0.1-q4_K_M",
          "size_text": "4.4GB",
          "size_bytes": 4724464025,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mathstral:7b-v0.1-q5_0",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mathstral:7b-v0.1-q5_1",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mathstral:7b-v0.1-q5_K_S",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mathstral:7b-v0.1-q5_K_M",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mathstral:7b-v0.1-q6_K",
          "size_text": "5.9GB",
          "size_bytes": 6335076761,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mathstral:7b-v0.1-q8_0",
          "size_text": "7.7GB",
          "size_bytes": 8267812044,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mathstral:7b-v0.1-fp16",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "32K",
          "input": "Text"
        }
      ],
      "tags_count": 17
    },
    {
      "slug": "everythinglm",
      "capabilities": [],
      "pulls": 56700,
      "pulls_text": "56.7K \n  Downloads",
      "blurb": "everythinglm Uncensored Llama2 based model with support for a 16K context window. 13b 18 Tags Updated 1 year ago",
      "name": "Uncensored Llama2 based model with support for a 16K context window.",
      "description": "Readme The Everything Language Model is a Llama 2-based model with a 16k context released by Totally Not An LLM (Kai Howard). It was trained with the EverythingLM Dataset and is uncensored. CLI ollama run everythinglm Once loaded, change the context size to 16K /set parameter num_ctx 16384 API Example: curl -X POST http://localhost:11434/api/generate -d '{ \"model\": \"everythinglm\", \"prompt\": \"Why is the sky blue?\" \"options\": { \"num_ctx\": 16384 } }' Reference 13b parameters original source: Totally Not An LLM Write Preview The Everything Language Model is a Llama 2-based model with a 16k context released by Totally Not An LLM (Kai Howard). It was trained with the [EverythingLM Dataset](https://huggingface.co/datasets/totally-not-an-llm/EverythingLM-data-V2) and is uncensored. ### CLI ```ollama run everythinglm``` Once loaded, change the context size to `16K` ``` /set parameter num_ctx 16384 ``` ### API Example: ```bash curl -X POST http://localhost:11434/api/generate -d '{ \"model\": \"everythinglm\", \"prompt\": \"Why is the sky blue?\" \"options\": { \"num_ctx\": 16384 } }' ``` ## Reference 13b parameters original source: [Totally Not An LLM](https://huggingface.co/totally-not-an-llm/EverythingLM-13b-16k) Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "everythinglm:latest",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "everythinglm:13b",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "everythinglm:13b-16k",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "everythinglm:13b-16k-q2_K",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "everythinglm:13b-16k-q3_K_S",
          "size_text": "5.7GB",
          "size_bytes": 6120328396,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "everythinglm:13b-16k-q3_K_M",
          "size_text": "6.3GB",
          "size_bytes": 6764573491,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "everythinglm:13b-16k-q3_K_L",
          "size_text": "6.9GB",
          "size_bytes": 7408818585,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "everythinglm:13b-16k-q4_0",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "everythinglm:13b-16k-q4_1",
          "size_text": "8.2GB",
          "size_bytes": 8804682956,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "everythinglm:13b-16k-q4_K_S",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "everythinglm:13b-16k-q4_K_M",
          "size_text": "7.9GB",
          "size_bytes": 8482560409,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "everythinglm:13b-16k-q5_0",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "everythinglm:13b-16k-q5_1",
          "size_text": "9.8GB",
          "size_bytes": 10522669875,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "everythinglm:13b-16k-q5_K_S",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "everythinglm:13b-16k-q5_K_M",
          "size_text": "9.2GB",
          "size_bytes": 9878424780,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "everythinglm:13b-16k-q6_K",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "everythinglm:13b-16k-q8_0",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "everythinglm:13b-16k-fp16",
          "size_text": "26GB",
          "size_bytes": 27917287424,
          "context": "16K",
          "input": "Text"
        }
      ],
      "tags_count": 18
    },
    {
      "slug": "nexusraven",
      "capabilities": [],
      "pulls": 56600,
      "pulls_text": "56.6K \n  Downloads",
      "blurb": "nexusraven Nexus Raven is a 13B instruction tuned model for function calling tasks. 13b 32 Tags Updated 1 year ago",
      "name": "Nexus Raven is a 13B instruction tuned model for function calling tasks.",
      "description": "Readme Nexus Raven is a 13 billion parameter model designed for function calling tasks. Updated to NexusRaven-V2, this model is an updated open-source and commercially viable function calling model: Versatile Function Calling Capability: capable of generating single function calls, nested calls, and parallel calls in many challenging cases. Fully Explainable: capable of generating very detailed explanations for the function calls it generates. This behavior can be turned off, to save tokens during inference. Performance Highlights: surpasses GPT-4 by up to 7% in function calling success rates in human-generated use cases involving nested and composite functions. Generalization to the Unseen: has never been trained on the functions used in evaluation. Commercially Permissive: The training does not involve any data generated by proprietary LLMs such as GPT-4. You have full control of the model when deployed in commercial applications. Example prompt Function: def get_weather_data(coordinates): \"\"\" Fetches weather data from the Open-Meteo API for the given latitude and longitude. Args: coordinates (tuple): The latitude of the location. Returns: float: The current temperature in the coordinates you've asked for \"\"\" Function: def get_coordinates_from_city(city_name): \"\"\" Fetches the latitude and longitude of a given city name using the Maps.co Geocoding API. Args: city_name (str): The name of the city. Returns: tuple: The latitude and longitude of the city. \"\"\" User Query: {query}<human_end> This will generate a response similar to: Call: get_weather_data(coordinates=get_coordinates_from_city(city_name='Seattle'))<bot_end> Thought: The function call `get_weather_data(coordinates=get_coordinates_from_city(city_name='Seattle'))` answers the question \"What's the weather like in Seattle right now?\" by following these steps: 1. `get_coordinates_from_city(city_name='Seattle')`: This function call fetches the latitude and longitude of the city \"Seattle\" using the Maps.co Geocod",
      "variants": [
        {
          "tag": "nexusraven:latest",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "nexusraven:13b",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "nexusraven:13b-v2-q2_K",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "nexusraven:13b-v2-q3_K_S",
          "size_text": "5.7GB",
          "size_bytes": 6120328396,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "nexusraven:13b-v2-q3_K_M",
          "size_text": "6.3GB",
          "size_bytes": 6764573491,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "nexusraven:13b-v2-q3_K_L",
          "size_text": "6.9GB",
          "size_bytes": 7408818585,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "nexusraven:13b-v2-q4_0",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "nexusraven:13b-v2-q4_1",
          "size_text": "8.2GB",
          "size_bytes": 8804682956,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "nexusraven:13b-v2-q4_K_S",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "nexusraven:13b-v2-q4_K_M",
          "size_text": "7.9GB",
          "size_bytes": 8482560409,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "nexusraven:13b-v2-q5_0",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "nexusraven:13b-v2-q5_1",
          "size_text": "9.8GB",
          "size_bytes": 10522669875,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "nexusraven:13b-v2-q5_K_S",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "nexusraven:13b-v2-q5_K_M",
          "size_text": "9.2GB",
          "size_bytes": 9878424780,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "nexusraven:13b-v2-q6_K",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "nexusraven:13b-v2-q8_0",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "nexusraven:13b-v2-fp16",
          "size_text": "26GB",
          "size_bytes": 27917287424,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "nexusraven:13b-q2_K",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "nexusraven:13b-q3_K_S",
          "size_text": "5.7GB",
          "size_bytes": 6120328396,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "nexusraven:13b-q3_K_M",
          "size_text": "6.3GB",
          "size_bytes": 6764573491,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "nexusraven:13b-q3_K_L",
          "size_text": "6.9GB",
          "size_bytes": 7408818585,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "nexusraven:13b-q4_0",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "nexusraven:13b-q4_1",
          "size_text": "8.2GB",
          "size_bytes": 8804682956,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "nexusraven:13b-q4_K_S",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "nexusraven:13b-q4_K_M",
          "size_text": "7.9GB",
          "size_bytes": 8482560409,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "nexusraven:13b-q5_0",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "nexusraven:13b-q5_1",
          "size_text": "9.8GB",
          "size_bytes": 10522669875,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "nexusraven:13b-q5_K_S",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "nexusraven:13b-q5_K_M",
          "size_text": "9.2GB",
          "size_bytes": 9878424780,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "nexusraven:13b-q6_K",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "nexusraven:13b-q8_0",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "nexusraven:13b-fp16",
          "size_text": "26GB",
          "size_bytes": 27917287424,
          "context": "16K",
          "input": "Text"
        }
      ],
      "tags_count": 32
    },
    {
      "slug": "codeup",
      "capabilities": [],
      "pulls": 55300,
      "pulls_text": "55.3K \n  Downloads",
      "blurb": "codeup Great code generation model based on Llama2. 13b 19 Tags Updated 2 years ago",
      "name": "Great code generation model based on Llama2.",
      "description": "Readme CodeUp was released by DeepSE. It is based on Llama 2 from Meta, and then fine-tuned for better code generation. This allows it to write better code in a number of languages.. Get started with CodeUp The model used in the example below is the CodeUp model, with 13b parameters, which is a code generation model. API Start Ollama server (Run ollama serve ) Run the model curl -X POST http://localhost:11434/api/generate -d '{ \"model\": \"codeup\", \"prompt\":\"Write a C++ code to find the longest common substring in two strings.\" }' CLI Install Ollama Open the terminal and run ollama run codeup Note: The ollama run command performs an ollama pull if the model is not already downloaded. To download the model without running it, use ollama pull codeup Memory requirements 13b models generally require at least 16GB of RAM If you run into issues with higher quantization levels, try using the q4 model or shut down any other programs that are using a lot of memory. Model variants By default, Ollama uses 4-bit quantization. To try other quantization levels, please try the other tags. The number after the q represents the number of bits used for quantization (i.e. q4 means 4-bit quantization). The higher the number, the more accurate the model is, but the slower it runs, and the more memory it requires. Aliases latest, 13b, 13b-llama2, 13b-llama2-chat, 13b-llama2-chat-q4_0 Model source CodeUp source on Ollama 13b parameters source: DeepSE References GitHub Repo for CodeUp Write Preview <img src=\"https://user-images.githubusercontent.com/633681/262490158-929e4fc6-e9f2-482f-b921-886029cc1df9.png\" width=30% /> CodeUp was released by DeepSE. It is based on Llama 2 from Meta, and then fine-tuned for better code generation. This allows it to write better code in a number of languages.. ## Get started with CodeUp The model used in the example below is the CodeUp model, with 13b parameters, which is a code generation model. ### API 1. Start Ollama server (Run `ollama serve`) 2. Run the ",
      "variants": [
        {
          "tag": "codeup:latest",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "codeup:13b",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "codeup:13b-llama2",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "codeup:13b-llama2-chat",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "codeup:13b-llama2-chat-q2_K",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "codeup:13b-llama2-chat-q3_K_S",
          "size_text": "5.7GB",
          "size_bytes": 6120328396,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "codeup:13b-llama2-chat-q3_K_M",
          "size_text": "6.3GB",
          "size_bytes": 6764573491,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "codeup:13b-llama2-chat-q3_K_L",
          "size_text": "6.9GB",
          "size_bytes": 7408818585,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "codeup:13b-llama2-chat-q4_0",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "codeup:13b-llama2-chat-q4_1",
          "size_text": "8.2GB",
          "size_bytes": 8804682956,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "codeup:13b-llama2-chat-q4_K_S",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "codeup:13b-llama2-chat-q4_K_M",
          "size_text": "7.9GB",
          "size_bytes": 8482560409,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "codeup:13b-llama2-chat-q5_0",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "codeup:13b-llama2-chat-q5_1",
          "size_text": "9.8GB",
          "size_bytes": 10522669875,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "codeup:13b-llama2-chat-q5_K_S",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "codeup:13b-llama2-chat-q5_K_M",
          "size_text": "9.2GB",
          "size_bytes": 9878424780,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "codeup:13b-llama2-chat-q6_K",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "codeup:13b-llama2-chat-q8_0",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "codeup:13b-llama2-chat-fp16",
          "size_text": "26GB",
          "size_bytes": 27917287424,
          "context": "4K",
          "input": "Text"
        }
      ],
      "tags_count": 19
    },
    {
      "slug": "marco-o1",
      "capabilities": [
        "reasoning"
      ],
      "pulls": 54000,
      "pulls_text": "54K \n  Downloads",
      "blurb": "marco-o1 An open large model for real-world solutions by the Alibaba International Digital Commerce Group (AIDC-AI). 7b 5 Tags Updated 11 months ago",
      "name": "An open large reasoning model for real-world solutions by the Alibaba International Digital Commerce Group (AIDC-AI).",
      "description": "Readme Fine-Tuning with CoT Data: We develop Marco-o1-CoT by performing full-parameter fine-tuning on the base model using open-source CoT dataset combined with our self-developed synthetic data. Solution Space Expansion via MCTS: We integrate LLMs with MCTS ( Marco-o1-MCTS ), using the model’s output confidence to guide the search and expand the solution space. Reasoning Action Strategy: We implement novel reasoning action strategies and a reflection mechanism ( Marco-o1-MCTS mini-step ), including exploring different action granularities within the MCTS framework and prompting the model to self-reflect, thereby significantly enhancing the model’s ability to solve complex problems. Application in Translation Tasks: We are the first to apply Large Reasoning Models (LRM) to Machine Translation task , exploring inference time scaling laws in the multilingual and translation domain. Usage ollama run marco-o1 \"How many Rs are in strawberry?\" Parse the resulting string between <Output> and </Output> : ... <Output> There are 3 Rs in strawberry. </Output> References GitHub HuggingFace Write Preview <img src=\"/assets/library/marco-o1/93bea833-d0c0-48b6-8ece-9c2de26cba27\" width=\"200\" /> * **Fine-Tuning with CoT Data:** We develop <ins>Marco-o1-CoT</ins> by performing full-parameter fine-tuning on the base model using open-source CoT dataset combined with our self-developed synthetic data. * **Solution Space Expansion via MCTS:** We integrate LLMs with MCTS (<ins>Marco-o1-MCTS</ins>), using the model's output confidence to guide the search and expand the solution space. * **Reasoning Action Strategy:** We implement novel reasoning action strategies and a reflection mechanism (<ins>Marco-o1-MCTS mini-step</ins>), including exploring different action granularities within the MCTS framework and prompting the model to self-reflect, thereby significantly enhancing the model's ability to solve complex problems. * **Application in Translation Tasks:** We are the first to apply Large",
      "variants": [
        {
          "tag": "marco-o1:latest",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "marco-o1:7b",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "marco-o1:7b-q4_K_M",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "marco-o1:7b-q8_0",
          "size_text": "8.1GB",
          "size_bytes": 8697308774,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "marco-o1:7b-fp16",
          "size_text": "15GB",
          "size_bytes": 16106127360,
          "context": "32K",
          "input": "Text"
        }
      ],
      "tags_count": 5
    },
    {
      "slug": "stablelm-zephyr",
      "capabilities": [],
      "pulls": 53900,
      "pulls_text": "53.9K \n  Downloads",
      "blurb": "stablelm-zephyr A lightweight chat model allowing accurate, and responsive output without requiring high-end hardware. 3b 17 Tags Updated 1 year ago",
      "name": "A lightweight chat model allowing accurate, and responsive output without requiring high-end hardware.",
      "description": "Readme StableLM Zephyr 3B is a lightweight chat model that is preference tuned for instruction following and Q&A-type tasks. Write Preview <img src=\"https://github.com/jmorganca/ollama/assets/3325447/9cb59a83-5f53-4b73-b341-216c37e74c86\" width=\"360\"> StableLM Zephyr 3B is a lightweight chat model that is preference tuned for instruction following and Q&A-type tasks. <img src=\"https://github.com/jmorganca/ollama/assets/3325447/1f3ed58d-a9ac-41d2-bb1d-abf3ed61a882\"> Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "stablelm-zephyr:latest",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm-zephyr:3b",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm-zephyr:3b-q2_K",
          "size_text": "1.2GB",
          "size_bytes": 1288490188,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm-zephyr:3b-q3_K_S",
          "size_text": "1.3GB",
          "size_bytes": 1395864371,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm-zephyr:3b-q3_K_M",
          "size_text": "1.4GB",
          "size_bytes": 1503238553,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm-zephyr:3b-q3_K_L",
          "size_text": "1.5GB",
          "size_bytes": 1610612736,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm-zephyr:3b-q4_0",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm-zephyr:3b-q4_1",
          "size_text": "1.8GB",
          "size_bytes": 1932735283,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm-zephyr:3b-q4_K_S",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm-zephyr:3b-q4_K_M",
          "size_text": "1.7GB",
          "size_bytes": 1825361100,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm-zephyr:3b-q5_0",
          "size_text": "1.9GB",
          "size_bytes": 2040109465,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm-zephyr:3b-q5_1",
          "size_text": "2.1GB",
          "size_bytes": 2254857830,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm-zephyr:3b-q5_K_S",
          "size_text": "1.9GB",
          "size_bytes": 2040109465,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm-zephyr:3b-q5_K_M",
          "size_text": "2.0GB",
          "size_bytes": 2147483648,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm-zephyr:3b-q6_K",
          "size_text": "2.3GB",
          "size_bytes": 2469606195,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm-zephyr:3b-q8_0",
          "size_text": "3.0GB",
          "size_bytes": 3221225472,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "stablelm-zephyr:3b-fp16",
          "size_text": "5.6GB",
          "size_bytes": 6012954214,
          "context": "4K",
          "input": "Text"
        }
      ],
      "tags_count": 17
    },
    {
      "slug": "solar-pro",
      "capabilities": [],
      "pulls": 51400,
      "pulls_text": "51.4K \n  Downloads",
      "blurb": "solar-pro Solar Pro Preview: an advanced large language model (LLM) with 22 billion parameters designed to fit into a single GPU 22b 18 Tags Updated 1 year ago",
      "name": "Solar Pro Preview: an advanced large language model (LLM) with 22 billion parameters designed to fit into a single GPU",
      "description": "Readme Solar Pro Preview is an advanced large language model (LLM) featuring 22 billion parameters, optimized to operate on a single GPU. It demonstrates superior performance compared to LLMs with fewer than 30 billion parameters and delivers results comparable to much larger models, such as Llama 3.1 with 70 billion parameters. Developed using an enhanced version of the depth up-scaling method, Solar Pro Preview scales a Phi-3-medium model with 14 billion parameters to 22 billion, designed to run on a GPU with 80GB of VRAM. The training strategy and dataset have been meticulously curated, leading to significant performance improvements over Phi-3-medium, especially on benchmarks like MMLU-Pro and IFEval, which assess a model’s knowledge and instruction-following capabilities. As a pre-release version of the official Solar Pro, Solar Pro Preview comes with limitations on language coverage and a maximum context length of 4K. Despite these restrictions, the model stands out for its efficiency and capability, with potential for future extensions to support more languages and functionalities. The official version of Solar Pro, scheduled for release in November 2024, will include expanded language support and longer context windows. References Blog post Hugging Face Write Preview <img src=\"https://ollama.com/assets/library/solar-pro/f0a588ec-6fbd-4012-8d15-8e8b0420dd85\" /> Solar Pro Preview is an advanced large language model (LLM) featuring 22 billion parameters, optimized to operate on a single GPU. It demonstrates superior performance compared to LLMs with fewer than 30 billion parameters and delivers results comparable to much larger models, such as Llama 3.1 with 70 billion parameters. Developed using an enhanced version of the depth up-scaling method, Solar Pro Preview scales a Phi-3-medium model with 14 billion parameters to 22 billion, designed to run on a GPU with 80GB of VRAM. The training strategy and dataset have been meticulously curated, leading to signific",
      "variants": [
        {
          "tag": "solar-pro:latest",
          "size_text": "13GB",
          "size_bytes": 13958643712,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "solar-pro:preview",
          "size_text": "13GB",
          "size_bytes": 13958643712,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "solar-pro:22b",
          "size_text": "13GB",
          "size_bytes": 13958643712,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "solar-pro:22b-preview-instruct-q2_K",
          "size_text": "8.2GB",
          "size_bytes": 8804682956,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "solar-pro:22b-preview-instruct-q3_K_S",
          "size_text": "9.6GB",
          "size_bytes": 10307921510,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "solar-pro:22b-preview-instruct-q3_K_M",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "solar-pro:22b-preview-instruct-q3_K_L",
          "size_text": "12GB",
          "size_bytes": 12884901888,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "solar-pro:22b-preview-instruct-q4_0",
          "size_text": "12GB",
          "size_bytes": 12884901888,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "solar-pro:22b-preview-instruct-q4_1",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "solar-pro:22b-preview-instruct-q4_K_S",
          "size_text": "13GB",
          "size_bytes": 13958643712,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "solar-pro:22b-preview-instruct-q4_K_M",
          "size_text": "13GB",
          "size_bytes": 13958643712,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "solar-pro:22b-preview-instruct-q5_0",
          "size_text": "15GB",
          "size_bytes": 16106127360,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "solar-pro:22b-preview-instruct-q5_1",
          "size_text": "17GB",
          "size_bytes": 18253611008,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "solar-pro:22b-preview-instruct-q5_K_S",
          "size_text": "15GB",
          "size_bytes": 16106127360,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "solar-pro:22b-preview-instruct-q5_K_M",
          "size_text": "16GB",
          "size_bytes": 17179869184,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "solar-pro:22b-preview-instruct-q6_K",
          "size_text": "18GB",
          "size_bytes": 19327352832,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "solar-pro:22b-preview-instruct-q8_0",
          "size_text": "24GB",
          "size_bytes": 25769803776,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "solar-pro:22b-preview-instruct-fp16",
          "size_text": "44GB",
          "size_bytes": 47244640256,
          "context": "4K",
          "input": "Text"
        }
      ],
      "tags_count": 18
    },
    {
      "slug": "falcon2",
      "capabilities": [],
      "pulls": 50400,
      "pulls_text": "50.4K \n  Downloads",
      "blurb": "falcon2 Falcon2 is an 11B parameters causal decoder-only model built by TII and trained over 5T tokens. 11b 17 Tags Updated 1 year ago",
      "name": "Falcon2 is an 11B parameters causal decoder-only model built by TII and trained over 5T tokens.",
      "description": "Readme References Announcement Hugging Face Write Preview <img src=\"https://ollama.com/assets/library/falcon2/a5804551-1010-4ce8-83d8-0b8764e3fc29\" width=\"240\" /> ## References [Announcement](https://www.tii.ae/news/falcon-2-uaes-technology-innovation-institute-releases-new-ai-model-series-outperforming-metas) [Hugging Face](https://huggingface.co/tiiuae/falcon-11B) Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "falcon2:latest",
          "size_text": "6.4GB",
          "size_bytes": 6871947673,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "falcon2:11b",
          "size_text": "6.4GB",
          "size_bytes": 6871947673,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "falcon2:11b-q2_K",
          "size_text": "4.3GB",
          "size_bytes": 4617089843,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "falcon2:11b-q3_K_S",
          "size_text": "4.9GB",
          "size_bytes": 5261334937,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "falcon2:11b-q3_K_M",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "falcon2:11b-q3_K_L",
          "size_text": "5.8GB",
          "size_bytes": 6227702579,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "falcon2:11b-q4_0",
          "size_text": "6.4GB",
          "size_bytes": 6871947673,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "falcon2:11b-q4_1",
          "size_text": "7.1GB",
          "size_bytes": 7623566950,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "falcon2:11b-q4_K_S",
          "size_text": "6.4GB",
          "size_bytes": 6871947673,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "falcon2:11b-q4_K_M",
          "size_text": "6.8GB",
          "size_bytes": 7301444403,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "falcon2:11b-q5_0",
          "size_text": "7.7GB",
          "size_bytes": 8267812044,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "falcon2:11b-q5_1",
          "size_text": "8.4GB",
          "size_bytes": 9019431321,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "falcon2:11b-q5_K_S",
          "size_text": "7.7GB",
          "size_bytes": 8267812044,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "falcon2:11b-q5_K_M",
          "size_text": "8.2GB",
          "size_bytes": 8804682956,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "falcon2:11b-q6_K",
          "size_text": "9.2GB",
          "size_bytes": 9878424780,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "falcon2:11b-q8_0",
          "size_text": "12GB",
          "size_bytes": 12884901888,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "falcon2:11b-fp16",
          "size_text": "22GB",
          "size_bytes": 23622320128,
          "context": "2K",
          "input": "Text"
        }
      ],
      "tags_count": 17
    },
    {
      "slug": "qwen3-vl",
      "capabilities": [
        "multimodal",
        "reasoning",
        "thinking",
        "tools",
        "vision"
      ],
      "pulls": 50100,
      "pulls_text": "50.1K \n  Downloads",
      "blurb": "qwen3-vl The most powerful -language model in the Qwen model family to date. cloud 2b 4b 8b 30b 32b 235b 59 Tags Updated 2 days ago",
      "name": "The most powerful vision-language model in the Qwen model family to date.",
      "description": "Readme Qwen3-VL models require Ollama 0.12.7 Qwen3-VL is the most powerful vision-language model in the Qwen family to date. In this generation, there are improvements to the model in many areas: its understanding and generating text, perceiving and reasoning about visual content, supporting longer context lengths, understanding spatial relationships and dynamic videos, or interacting with AI agents — Qwen3-VL shows clear and significant progress in every area. Models 2B ollama run qwen3-vl:2b 4B ollama run qwen3-vl:4b 8B ollama run qwen3-vl:8b 30B ollama run qwen3-vl:30b 32B ollama run qwen3-vl:32b 235B ollama run qwen3-vl:235b ollama run qwen3-vl:235b-cloud Key features Visual Agent Capabilities : Qwen3-VL can operate computer and mobile interfaces — recognize GUI elements, understand button functions, call tools, and complete tasks. It achieves top global performance on benchmarks like OS World, and using tools significantly improves its performance on fine-grained perception tasks. Superior Text-Centric Performance : Qwen3-VL employs early-stage joint pretraining of text and visual modalities, continuously strengthening its language capabilities. Its performance on text-based tasks matches that of Qwen3-235B-A22B-2507 — the flagship language model — making it a truly “text-grounded, multimodal powerhouse” for the next generation of vision-language models. Greatly Improved Visual Coding : It can now generate code from images or videos — for example, turning a design mockup into Draw.io, HTML, CSS, or JavaScript code — making “what you see is what you get” visual programming a reality. Much Better Spatial Understanding : 2D grounding from absolute coordinates to relative coordinates. It can judge object positions, viewpoint changes, and occlusion relationships. It supports 3D grounding, laying the foundation for complex spatial reasoning and embodied AI applications. Long Context & Long Video Understanding : All models natively support 256K tokens of context, expa",
      "variants": [
        {
          "tag": "qwen3-vl:latest",
          "size_text": "6.1GB",
          "size_bytes": 6549825126,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:2b",
          "size_text": "1.9GB",
          "size_bytes": 2040109465,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:4b",
          "size_text": "3.3GB",
          "size_bytes": 3543348019,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:8b",
          "size_text": "6.1GB",
          "size_bytes": 6549825126,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:30b",
          "size_text": "20GB",
          "size_bytes": 21474836480,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:32b",
          "size_text": "21GB",
          "size_bytes": 22548578304,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:235b",
          "size_text": "143GB",
          "size_bytes": 153545080832,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:2b-instruct",
          "size_text": "1.9GB",
          "size_bytes": 2040109465,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:2b-instruct-q4_K_M",
          "size_text": "1.9GB",
          "size_bytes": 2040109465,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:2b-instruct-q8_0",
          "size_text": "2.6GB",
          "size_bytes": 2791728742,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:2b-instruct-bf16",
          "size_text": "4.3GB",
          "size_bytes": 4617089843,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:2b-thinking",
          "size_text": "1.9GB",
          "size_bytes": 2040109465,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:2b-thinking-q4_K_M",
          "size_text": "1.9GB",
          "size_bytes": 2040109465,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:2b-thinking-q8_0",
          "size_text": "2.6GB",
          "size_bytes": 2791728742,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:2b-thinking-bf16",
          "size_text": "4.3GB",
          "size_bytes": 4617089843,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:4b-instruct",
          "size_text": "3.3GB",
          "size_bytes": 3543348019,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:4b-instruct-q4_K_M",
          "size_text": "3.3GB",
          "size_bytes": 3543348019,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:4b-instruct-q8_0",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:4b-instruct-bf16",
          "size_text": "8.9GB",
          "size_bytes": 9556302233,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:4b-thinking",
          "size_text": "3.3GB",
          "size_bytes": 3543348019,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:4b-thinking-q4_K_M",
          "size_text": "3.3GB",
          "size_bytes": 3543348019,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:4b-thinking-q8_0",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:4b-thinking-bf16",
          "size_text": "8.9GB",
          "size_bytes": 9556302233,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:8b-instruct",
          "size_text": "6.1GB",
          "size_bytes": 6549825126,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:8b-instruct-q4_K_M",
          "size_text": "6.1GB",
          "size_bytes": 6549825126,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:8b-instruct-q8_0",
          "size_text": "9.8GB",
          "size_bytes": 10522669875,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:8b-instruct-bf16",
          "size_text": "18GB",
          "size_bytes": 19327352832,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:8b-thinking",
          "size_text": "6.1GB",
          "size_bytes": 6549825126,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:8b-thinking-q4_K_M",
          "size_text": "6.1GB",
          "size_bytes": 6549825126,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:8b-thinking-q8_0",
          "size_text": "9.8GB",
          "size_bytes": 10522669875,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:8b-thinking-bf16",
          "size_text": "18GB",
          "size_bytes": 19327352832,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:30b-a3b",
          "size_text": "20GB",
          "size_bytes": 21474836480,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:30b-a3b-instruct",
          "size_text": "20GB",
          "size_bytes": 21474836480,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:30b-a3b-instruct-q4_K_M",
          "size_text": "20GB",
          "size_bytes": 21474836480,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:30b-a3b-instruct-q8_0",
          "size_text": "34GB",
          "size_bytes": 36507222016,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:30b-a3b-instruct-bf16",
          "size_text": "62GB",
          "size_bytes": 66571993088,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:30b-a3b-thinking",
          "size_text": "20GB",
          "size_bytes": 21474836480,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:30b-a3b-thinking-q4_K_M",
          "size_text": "20GB",
          "size_bytes": 21474836480,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:30b-a3b-thinking-q8_0",
          "size_text": "34GB",
          "size_bytes": 36507222016,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:30b-a3b-thinking-bf16",
          "size_text": "62GB",
          "size_bytes": 66571993088,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:32b-instruct",
          "size_text": "21GB",
          "size_bytes": 22548578304,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:32b-instruct-q4_K_M",
          "size_text": "21GB",
          "size_bytes": 22548578304,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:32b-instruct-q8_0",
          "size_text": "36GB",
          "size_bytes": 38654705664,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:32b-instruct-bf16",
          "size_text": "67GB",
          "size_bytes": 71940702208,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:32b-thinking",
          "size_text": "21GB",
          "size_bytes": 22548578304,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:32b-thinking-q4_K_M",
          "size_text": "21GB",
          "size_bytes": 22548578304,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:32b-thinking-q8_0",
          "size_text": "36GB",
          "size_bytes": 38654705664,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:32b-thinking-bf16",
          "size_text": "67GB",
          "size_bytes": 71940702208,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:235b-a22b",
          "size_text": "143GB",
          "size_bytes": 153545080832,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:235b-a22b-instruct",
          "size_text": "143GB",
          "size_bytes": 153545080832,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:235b-a22b-instruct-q4_K_M",
          "size_text": "143GB",
          "size_bytes": 153545080832,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:235b-a22b-instruct-q8_0",
          "size_text": "251GB",
          "size_bytes": 269509197824,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:235b-a22b-instruct-bf16",
          "size_text": "471GB",
          "size_bytes": 505732399104,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:235b-a22b-thinking",
          "size_text": "143GB",
          "size_bytes": 153545080832,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:235b-a22b-thinking-q4_K_M",
          "size_text": "143GB",
          "size_bytes": 153545080832,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:235b-a22b-thinking-q8_0",
          "size_text": "251GB",
          "size_bytes": 269509197824,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:235b-a22b-thinking-bf16",
          "size_text": "471GB",
          "size_bytes": 505732399104,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:235b-cloud",
          "size_text": null,
          "size_bytes": null,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:235b-instruct-cloud",
          "size_text": null,
          "size_bytes": null,
          "context": "256K",
          "input": "Text"
        }
      ],
      "tags_count": 59
    },
    {
      "slug": "duckdb-nsql",
      "capabilities": [],
      "pulls": 49800,
      "pulls_text": "49.8K \n  Downloads",
      "blurb": "duckdb-nsql 7B parameter text-to-SQL model made by MotherDuck and Numbers Station. 7b 17 Tags Updated 1 year ago",
      "name": "7B parameter text-to-SQL model made by MotherDuck and Numbers Station.",
      "description": "Readme DuckDB-NSQL is a 7 billion parameter text-to-SQL model designed specifically for SQL generation tasks. This model is based on Meta’s original Llama-2 7B model and further pre-trained on a dataset of general SQL queries and then fine-tuned on a dataset composed of DuckDB text-to-SQL pairs. Usage Example Prompt Provided this schema: CREATE TABLE taxi ( VendorID bigint, tpep_pickup_datetime timestamp, tpep_dropoff_datetime timestamp, passenger_count double, trip_distance double, fare_amount double, extra double, tip_amount double, tolls_amount double, improvement_surcharge double, total_amount double, ); Give me taxis with more than 2 passengers Example output SELECT * FROM taxi WHERE passenger_count > 2 Setting the system prompt This model expects the schema in the system prompt as input: /set system \"\"\"Here is the database schema that the SQL query will run on: CREATE TABLE taxi ( VendorID bigint, tpep_pickup_datetime timestamp, tpep_dropoff_datetime timestamp, passenger_count double, trip_distance double, fare_amount double, extra double, tip_amount double, tolls_amount double, improvement_surcharge double, total_amount double, );\"\"\" Once the schema is provided in the system prompt, the model will use it in subsequent responses. For the following prompt: get all columns ending with _amount from taxi table The model will output something like this: SELECT COLUMNS('.*_amount') FROM taxi; API example $ curl http://localhost:11434/api/generate -d '{ \"model\": \"duckdb-nsql:7b-q4_0\", \"system\": \"Here is the database schema that the SQL query will run on: CREATE TABLE taxi (VendorID bigint, tpep_pickup_datetime timestamp, tpep_dropoff_datetime timestamp, passenger_count double, trip_distance double, fare_amount double, extra double, tip_amount double, tolls_amount double, improvement_surcharge double, total_amount double,);\", \"prompt\": \"get all columns ending with _amount from taxi table\" }' Python library example pip install ollama import ollama r = ollama.generate( ",
      "variants": [
        {
          "tag": "duckdb-nsql:latest",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "duckdb-nsql:7b",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "duckdb-nsql:7b-q2_K",
          "size_text": "2.5GB",
          "size_bytes": 2684354560,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "duckdb-nsql:7b-q3_K_S",
          "size_text": "2.9GB",
          "size_bytes": 3113851289,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "duckdb-nsql:7b-q3_K_M",
          "size_text": "3.3GB",
          "size_bytes": 3543348019,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "duckdb-nsql:7b-q3_K_L",
          "size_text": "3.6GB",
          "size_bytes": 3865470566,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "duckdb-nsql:7b-q4_0",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "duckdb-nsql:7b-q4_1",
          "size_text": "4.2GB",
          "size_bytes": 4509715660,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "duckdb-nsql:7b-q4_K_S",
          "size_text": "3.9GB",
          "size_bytes": 4187593113,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "duckdb-nsql:7b-q4_K_M",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "duckdb-nsql:7b-q5_0",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "duckdb-nsql:7b-q5_1",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "duckdb-nsql:7b-q5_K_S",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "duckdb-nsql:7b-q5_K_M",
          "size_text": "4.8GB",
          "size_bytes": 5153960755,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "duckdb-nsql:7b-q6_K",
          "size_text": "5.5GB",
          "size_bytes": 5905580032,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "duckdb-nsql:7b-q8_0",
          "size_text": "7.2GB",
          "size_bytes": 7730941132,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "duckdb-nsql:7b-fp16",
          "size_text": "13GB",
          "size_bytes": 13958643712,
          "context": "16K",
          "input": "Text"
        }
      ],
      "tags_count": 17
    },
    {
      "slug": "magicoder",
      "capabilities": [],
      "pulls": 47700,
      "pulls_text": "47.7K \n  Downloads",
      "blurb": "magicoder 🎩 Magicoder is a family of 7B parameter models trained on 75K synthetic instruction data using OSS-Instruct, a novel approach to enlightening LLMs with open-source code snippets. 7b 18 Tags Updated 1 year ago",
      "name": "🎩 Magicoder is a family of 7B parameter models trained on 75K synthetic instruction data using OSS-Instruct, a novel approach to enlightening LLMs with open-source code snippets.",
      "description": "Readme 🎩 Magicoder is a model family empowered by OSS-Instruct, a novel approach to enlightening LLMs with open-source code snippets for generating low-bias and high-quality instruction data for code. OSS-Instruct mitigates the inherent bias of the LLM-synthesized instruction data by empowering them with a wealth of open-source references to produce more diverse, realistic, and controllable data. References GitHub HuggingFace Write Preview # 🎩 Magicoder is a model family empowered by OSS-Instruct, a novel approach to enlightening LLMs with open-source code snippets for generating low-bias and high-quality instruction data for code. OSS-Instruct mitigates the inherent bias of the LLM-synthesized instruction data by empowering them with a wealth of open-source references to produce more diverse, realistic, and controllable data. ## References [GitHub](https://github.com/ise-uiuc/magicoder) [HuggingFace](https://huggingface.co/ise-uiuc/Magicoder-S-CL-7B) Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "magicoder:latest",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "magicoder:7b",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "magicoder:7b-s-cl",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "magicoder:7b-s-cl-q2_K",
          "size_text": "2.8GB",
          "size_bytes": 3006477107,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "magicoder:7b-s-cl-q3_K_S",
          "size_text": "2.9GB",
          "size_bytes": 3113851289,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "magicoder:7b-s-cl-q3_K_M",
          "size_text": "3.3GB",
          "size_bytes": 3543348019,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "magicoder:7b-s-cl-q3_K_L",
          "size_text": "3.6GB",
          "size_bytes": 3865470566,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "magicoder:7b-s-cl-q4_0",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "magicoder:7b-s-cl-q4_1",
          "size_text": "4.2GB",
          "size_bytes": 4509715660,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "magicoder:7b-s-cl-q4_K_S",
          "size_text": "3.9GB",
          "size_bytes": 4187593113,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "magicoder:7b-s-cl-q4_K_M",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "magicoder:7b-s-cl-q5_0",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "magicoder:7b-s-cl-q5_1",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "magicoder:7b-s-cl-q5_K_S",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "magicoder:7b-s-cl-q5_K_M",
          "size_text": "4.8GB",
          "size_bytes": 5153960755,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "magicoder:7b-s-cl-q6_K",
          "size_text": "5.5GB",
          "size_bytes": 5905580032,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "magicoder:7b-s-cl-q8_0",
          "size_text": "7.2GB",
          "size_bytes": 7730941132,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "magicoder:7b-s-cl-fp16",
          "size_text": "13GB",
          "size_bytes": 13958643712,
          "context": "16K",
          "input": "Text"
        }
      ],
      "tags_count": 18
    },
    {
      "slug": "mistrallite",
      "capabilities": [],
      "pulls": 47200,
      "pulls_text": "47.2K \n  Downloads",
      "blurb": "mistrallite MistralLite is a fine-tuned model based on Mistral with enhanced capabilities of processing long contexts. 7b 17 Tags Updated 1 year ago",
      "name": "MistralLite is a fine-tuned model based on Mistral with enhanced capabilities of processing long contexts.",
      "description": "Readme Mistrallite is a fine-tuned model based on Mistral , with enhanced capabilities of processing long context (up to 32K tokens). This model is able to perform significantly better on several long context retrieve and answering tasks. Reference Hugging Face Write Preview <img src=\"https://upload.wikimedia.org/wikipedia/commons/9/93/Amazon_Web_Services_Logo.svg\" width=\"180\" /> Mistrallite is a fine-tuned model based on [Mistral](https://ollama.ai/library/mistral), with enhanced capabilities of processing long context (up to 32K tokens). This model is able to perform significantly better on several long context retrieve and answering tasks. ## Reference [Hugging Face](https://huggingface.co/amazon/MistralLite) Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "mistrallite:latest",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistrallite:7b",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistrallite:7b-v0.1-q2_K",
          "size_text": "3.1GB",
          "size_bytes": 3328599654,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistrallite:7b-v0.1-q3_K_S",
          "size_text": "3.2GB",
          "size_bytes": 3435973836,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistrallite:7b-v0.1-q3_K_M",
          "size_text": "3.5GB",
          "size_bytes": 3758096384,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistrallite:7b-v0.1-q3_K_L",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistrallite:7b-v0.1-q4_0",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistrallite:7b-v0.1-q4_1",
          "size_text": "4.6GB",
          "size_bytes": 4939212390,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistrallite:7b-v0.1-q4_K_S",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistrallite:7b-v0.1-q4_K_M",
          "size_text": "4.4GB",
          "size_bytes": 4724464025,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistrallite:7b-v0.1-q5_0",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistrallite:7b-v0.1-q5_1",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistrallite:7b-v0.1-q5_K_S",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistrallite:7b-v0.1-q5_K_M",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistrallite:7b-v0.1-q6_K",
          "size_text": "5.9GB",
          "size_bytes": 6335076761,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistrallite:7b-v0.1-q8_0",
          "size_text": "7.7GB",
          "size_bytes": 8267812044,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistrallite:7b-v0.1-fp16",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "32K",
          "input": "Text"
        }
      ],
      "tags_count": 17
    },
    {
      "slug": "codebooga",
      "capabilities": [],
      "pulls": 46600,
      "pulls_text": "46.6K \n  Downloads",
      "blurb": "codebooga A high-performing code instruct model created by merging two existing code models. 34b 16 Tags Updated 2 years ago",
      "name": "A high-performing code instruct model created by merging two existing code models.",
      "description": "Readme Codebooga was created by merging two existing models: Phind-CodeLlama-34B-v2 WizardCoder-Python-34B-V1.0 It was created by using the BlockMerge Gradient script. Example prompts Write the javascript react code to render a sign up form Where is the bug in this code? def fib(n): if n <= 0: return n else: return fib(n-1) + fib(n-2) Recommended parameters temperature: 1.31 top_p: 0.14 repeat_penalty: 1.17 top_k: 49 References Hugging Face Write Preview <img src=\"https://github.com/jmorganca/ollama/assets/251292/700d0dcf-3bc0-406f-b289-e48965e8f602\" width=\"280\" /> Codebooga was created by merging two existing models: 1. [Phind-CodeLlama-34B-v2](https://ollama.ai/library/phind-codellama) 2. [WizardCoder-Python-34B-V1.0](https://ollama.ai/library/wizardcoder) It was created by using the [BlockMerge Gradient](https://github.com/Gryphe/BlockMerge_Gradient) script. ### Example prompts ``` Write the javascript react code to render a sign up form ``` ``` Where is the bug in this code? def fib(n): if n <= 0: return n else: return fib(n-1) + fib(n-2) ``` ### Recommended parameters ``` temperature: 1.31 top_p: 0.14 repeat_penalty: 1.17 top_k: 49 ``` ## References [Hugging Face](https://huggingface.co/oobabooga/CodeBooga-34B-v0.1) Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "codebooga:latest",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codebooga:34b",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codebooga:34b-v0.1-q2_K",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codebooga:34b-v0.1-q3_K_S",
          "size_text": "15GB",
          "size_bytes": 16106127360,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codebooga:34b-v0.1-q3_K_M",
          "size_text": "16GB",
          "size_bytes": 17179869184,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codebooga:34b-v0.1-q3_K_L",
          "size_text": "18GB",
          "size_bytes": 19327352832,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codebooga:34b-v0.1-q4_0",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codebooga:34b-v0.1-q4_1",
          "size_text": "21GB",
          "size_bytes": 22548578304,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codebooga:34b-v0.1-q4_K_M",
          "size_text": "20GB",
          "size_bytes": 21474836480,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codebooga:34b-v0.1-q5_0",
          "size_text": "23GB",
          "size_bytes": 24696061952,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codebooga:34b-v0.1-q5_1",
          "size_text": "25GB",
          "size_bytes": 26843545600,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codebooga:34b-v0.1-q5_K_S",
          "size_text": "23GB",
          "size_bytes": 24696061952,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codebooga:34b-v0.1-q5_K_M",
          "size_text": "24GB",
          "size_bytes": 25769803776,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codebooga:34b-v0.1-q6_K",
          "size_text": "28GB",
          "size_bytes": 30064771072,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codebooga:34b-v0.1-q8_0",
          "size_text": "36GB",
          "size_bytes": 38654705664,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "codebooga:34b-v0.1-fp16",
          "size_text": "67GB",
          "size_bytes": 71940702208,
          "context": "16K",
          "input": "Text"
        }
      ],
      "tags_count": 16
    },
    {
      "slug": "bespoke-minicheck",
      "capabilities": [],
      "pulls": 46100,
      "pulls_text": "46.1K \n  Downloads",
      "blurb": "bespoke-minicheck A state-of-the-art fact-checking model developed by Bespoke Labs. 7b 17 Tags Updated 1 year ago",
      "name": "A state-of-the-art fact-checking model developed by Bespoke Labs.",
      "description": "Readme This is a grounded factuality checking model developed by Bespoke Labs . The model takes as input a document (text) and a sentence and determines whether the sentence is supported by the document. In order to fact-check a multi-sentence claim, the claim should first be broken up into sentences. The document does not need to be chunked unless it exceeds 32K tokens. Bespoke-MiniCheck is the SOTA fact-checking model despite its small size. Usage The prompt template is as follows: Document: {document} Claim: {claim} The response will either be Yes or No . Examples Prompt Document: A group of students gather in the school library to study for their upcoming final exams. Claim: The students are preparing for an examination. Response Yes Prompt Document: A group of students gather in the school library to study for their upcoming final exams. Claim: The students are on vacation. Response No Model performance The performance of these models is evaluated on our new collected benchmark (unseen by our models during training), LLM-AggreFact , from 11 recent human annotated datasets on fact-checking and grounding LLM generations. Bespoke-MiniCheck-7B is the SOTA fact-checking model despite its small size. References Website Paper LLM-AggreFact Leaderboard Write Preview <img src=\"https://ollama.com/assets/library/bespoke-minicheck/640ec552-feac-4a35-8651-a85ba3f1f9bd\" width=\"480\" /> This is a grounded factuality checking model developed by [Bespoke Labs](https://bespokelabs.ai). The model takes as input a document (text) and a sentence and determines whether the sentence is supported by the document. In order to fact-check a multi-sentence claim, the claim should first be broken up into sentences. The document does not need to be chunked unless it exceeds 32K tokens. ![bespoke-minicheck-howitworks.png](https://ollama.com/assets/library/bespoke-minicheck/4a1f8cce-a9b2-41e1-8d0a-cb4f1c6b5793) Bespoke-MiniCheck is the SOTA fact-checking model despite its small size. ## Usage ",
      "variants": [
        {
          "tag": "bespoke-minicheck:latest",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "bespoke-minicheck:7b",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "bespoke-minicheck:7b-q2_K",
          "size_text": "3.0GB",
          "size_bytes": 3221225472,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "bespoke-minicheck:7b-q3_K_S",
          "size_text": "3.5GB",
          "size_bytes": 3758096384,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "bespoke-minicheck:7b-q3_K_M",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "bespoke-minicheck:7b-q3_K_L",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "bespoke-minicheck:7b-q4_0",
          "size_text": "4.5GB",
          "size_bytes": 4831838208,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "bespoke-minicheck:7b-q4_1",
          "size_text": "4.9GB",
          "size_bytes": 5261334937,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "bespoke-minicheck:7b-q4_K_S",
          "size_text": "4.5GB",
          "size_bytes": 4831838208,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "bespoke-minicheck:7b-q4_K_M",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "bespoke-minicheck:7b-q5_0",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "bespoke-minicheck:7b-q5_1",
          "size_text": "5.8GB",
          "size_bytes": 6227702579,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "bespoke-minicheck:7b-q5_K_S",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "bespoke-minicheck:7b-q5_K_M",
          "size_text": "5.5GB",
          "size_bytes": 5905580032,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "bespoke-minicheck:7b-q6_K",
          "size_text": "6.4GB",
          "size_bytes": 6871947673,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "bespoke-minicheck:7b-q8_0",
          "size_text": "8.2GB",
          "size_bytes": 8804682956,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "bespoke-minicheck:7b-fp16",
          "size_text": "15GB",
          "size_bytes": 16106127360,
          "context": "32K",
          "input": "Text"
        }
      ],
      "tags_count": 17
    },
    {
      "slug": "nuextract",
      "capabilities": [
        "reasoning"
      ],
      "pulls": 45300,
      "pulls_text": "45.3K \n  Downloads",
      "blurb": "nuextract A 3.8B model fine-tuned on a private high-quality synthetic dataset for information extraction, based on Phi-3. 3.8b 17 Tags Updated 1 year ago",
      "name": "A 3.8B model fine-tuned on a private high-quality synthetic dataset for information extraction, based on Phi-3.",
      "description": "Readme Structure Extraction Model by NuMind 🔥 NuExtract is a version of phi-3-mini, fine-tuned on a private high-quality synthetic dataset for information extraction. To use the model, provide an input text (less than 2000 tokens) and a JSON template describing the information you need to extract. Note: This model is purely extractive, so all text output by the model is present as is in the original text. You can also provide an example of output formatting to help the model understand your task more precisely. Usage Prompt Format This model works best when using a specific prompt format to extract text: ### Template: { \"Model\": { \"Name\": \"\", \"Number of parameters\": \"\", }, \"Usage\": { \"Use case\": [], \"Licence\": \"\" } } ### Example: { \"Model\": { \"Name\": \"Llama3\", \"Number of parameters\": \"8 billion\", }, \"Usage\": { \"Use case\":[ \"chat\", \"code completion\" ], \"Licence\": \"Meta Llama3\" } } ### Text: We introduce Mistral 7B, a 7–billion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms the best open 13B model (Llama 2) across all evaluated benchmarks, and the best released 34B model (Llama 1) in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B – Instruct, that surpasses Llama 2 13B – chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license. Code: https://github.com/mistralai/mistral-src Webpage: https://mistral.ai/news/announcing-mistral-7b/ References Hugging Face Write Preview ### Structure Extraction Model by NuMind 🔥 NuExtract is a version of phi-3-mini, fine-tuned on a private high-quality synthetic dataset for information extraction. To use the model, provide an input text (less than 2000 tokens) and a JSON template",
      "variants": [
        {
          "tag": "nuextract:latest",
          "size_text": "2.2GB",
          "size_bytes": 2362232012,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nuextract:3.8b",
          "size_text": "2.2GB",
          "size_bytes": 2362232012,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nuextract:3.8b-q2_K",
          "size_text": "1.4GB",
          "size_bytes": 1503238553,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nuextract:3.8b-q3_K_S",
          "size_text": "1.7GB",
          "size_bytes": 1825361100,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nuextract:3.8b-q3_K_M",
          "size_text": "2.0GB",
          "size_bytes": 2147483648,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nuextract:3.8b-q3_K_L",
          "size_text": "2.1GB",
          "size_bytes": 2254857830,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nuextract:3.8b-q4_0",
          "size_text": "2.2GB",
          "size_bytes": 2362232012,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nuextract:3.8b-q4_1",
          "size_text": "2.4GB",
          "size_bytes": 2576980377,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nuextract:3.8b-q4_K_S",
          "size_text": "2.2GB",
          "size_bytes": 2362232012,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nuextract:3.8b-q4_K_M",
          "size_text": "2.4GB",
          "size_bytes": 2576980377,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nuextract:3.8b-q5_0",
          "size_text": "2.6GB",
          "size_bytes": 2791728742,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nuextract:3.8b-q5_1",
          "size_text": "2.9GB",
          "size_bytes": 3113851289,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nuextract:3.8b-q5_K_S",
          "size_text": "2.6GB",
          "size_bytes": 2791728742,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nuextract:3.8b-q5_K_M",
          "size_text": "2.8GB",
          "size_bytes": 3006477107,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nuextract:3.8b-q6_K",
          "size_text": "3.1GB",
          "size_bytes": 3328599654,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nuextract:3.8b-q8_0",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "nuextract:3.8b-fp16",
          "size_text": "7.6GB",
          "size_bytes": 8160437862,
          "context": "4K",
          "input": "Text"
        }
      ],
      "tags_count": 17
    },
    {
      "slug": "wizard-vicuna",
      "capabilities": [],
      "pulls": 43800,
      "pulls_text": "43.8K \n  Downloads",
      "blurb": "wizard-vicuna Wizard Vicuna is a 13B parameter model based on Llama 2 trained by MelodysDreamj. 13b 17 Tags Updated 2 years ago",
      "name": "Wizard Vicuna is a 13B parameter model based on Llama 2 trained by MelodysDreamj.",
      "description": "Readme Wizard Vicuna is a 13B parameter model based on Llama 2 trained by MelodysDreamj. CLI Open the terminal and run ollama run wizard-vicuna API Example: curl -X POST http://localhost:11434/api/generate -d '{ \"model\": \"wizard-vicuna\", \"prompt\":\"Why is the sky blue?\" }' Memory requirements 13b models generally require at least 16GB of RAM Reference The Bloke MelodysDreamj Write Preview <img src=\"https://user-images.githubusercontent.com/633681/263123848-e99cc861-483d-4b53-a8ce-5d9c372d343b.png\" width=\"500\" /> Wizard Vicuna is a 13B parameter model based on Llama 2 trained by MelodysDreamj. ### CLI Open the terminal and run `ollama run wizard-vicuna` ### API Example: ```bash curl -X POST http://localhost:11434/api/generate -d '{ \"model\": \"wizard-vicuna\", \"prompt\":\"Why is the sky blue?\" }' ``` ## Memory requirements - 13b models generally require at least 16GB of RAM ## Reference [The Bloke](https://huggingface.co/TheBloke/wizard-vicuna-13B-GGML) [MelodysDreamj](https://huggingface.co/junelee/wizard-vicuna-13b) Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "wizard-vicuna:latest",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-vicuna:13b",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-vicuna:13b-q2_K",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-vicuna:13b-q3_K_S",
          "size_text": "5.7GB",
          "size_bytes": 6120328396,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-vicuna:13b-q3_K_M",
          "size_text": "6.3GB",
          "size_bytes": 6764573491,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-vicuna:13b-q3_K_L",
          "size_text": "6.9GB",
          "size_bytes": 7408818585,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-vicuna:13b-q4_0",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-vicuna:13b-q4_1",
          "size_text": "8.2GB",
          "size_bytes": 8804682956,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-vicuna:13b-q4_K_S",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-vicuna:13b-q4_K_M",
          "size_text": "7.9GB",
          "size_bytes": 8482560409,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-vicuna:13b-q5_0",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-vicuna:13b-q5_1",
          "size_text": "9.8GB",
          "size_bytes": 10522669875,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-vicuna:13b-q5_K_S",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-vicuna:13b-q5_K_M",
          "size_text": "9.2GB",
          "size_bytes": 9878424780,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-vicuna:13b-q6_K",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-vicuna:13b-q8_0",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "wizard-vicuna:13b-fp16",
          "size_text": "26GB",
          "size_bytes": 27917287424,
          "context": "2K",
          "input": "Text"
        }
      ],
      "tags_count": 17
    },
    {
      "slug": "granite3-guardian",
      "capabilities": [],
      "pulls": 40300,
      "pulls_text": "40.3K \n  Downloads",
      "blurb": "granite3-guardian The IBM Granite Guardian 3.0 2B and 8B models are designed to detect risks in prompts and/or responses. 2b 8b 10 Tags Updated 11 months ago",
      "name": "The IBM Granite Guardian 3.0 2B and 8B models are designed to detect risks in prompts and/or responses.",
      "description": "Readme Granite guardian models The IBM Granite Guardian 3.0 2B and 8B models are designed to detect risks in prompts and/or responses. They can help with risk detection along many key dimensions catalogued in the IBM AI Risk Atlas . They are trained on unique data comprising human annotations and synthetic data informed by internal red-teaming, and they outperform other open-source models in the same space on standard benchmarks. Parameter Sizes The model will produce a single output token, either Yes or No . By default, the general-purpose harm category is used, but other categories can be selected by setting the system prompt. 2B: ollama run granite3-guardian:2b >>> /set system profanity 8B: ollama run granite3-guardian:8b >>> /set system violence Supported Uses Risk detection in prompt text or model response (i.e. as guardrails), such as: Harm ( harm ): content considered generally harmful Social Bias ( social_bias ): prejudice based on identity or characteristics Jailbreaking ( jailbreak ): deliberate instances of manipulating AI to generate harmful, undesired, or inappropriate content Violence ( violence ): content promoting physical, mental, or sexual harm Profanity ( profanity ): use of offensive language or insults Sexual Content ( sexual_content ): explicit or suggestive material of a sexual nature Unethical Behavior ( unethical_behavior ): actions that violate moral or legal standards RAG (retrieval-augmented generation) to assess: Context relevance ( relevance ): whether the retrieved context is relevant to the query Groundedness ( groundedness ): whether the response is accurate and faithful to the provided context Answer relevance ( answer_relevance ): whether the response directly addresses the user’s query Granite dense models The Granite dense models are available in 2B and 8B parameter sizes designed to support tool-based use cases and for retrieval augmented generation (RAG), streamlining code generation, translation and bug fixing. See model page ",
      "variants": [
        {
          "tag": "granite3-guardian:latest",
          "size_text": "2.7GB",
          "size_bytes": 2899102924,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "granite3-guardian:2b",
          "size_text": "2.7GB",
          "size_bytes": 2899102924,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "granite3-guardian:8b",
          "size_text": "5.8GB",
          "size_bytes": 6227702579,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "granite3-guardian:2b-q8_0",
          "size_text": "2.7GB",
          "size_bytes": 2899102924,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "granite3-guardian:2b-fp16",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "granite3-guardian:8b-q5_K_S",
          "size_text": "5.6GB",
          "size_bytes": 6012954214,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "granite3-guardian:8b-q5_K_M",
          "size_text": "5.8GB",
          "size_bytes": 6227702579,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "granite3-guardian:8b-q6_K",
          "size_text": "6.7GB",
          "size_bytes": 7194070220,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "granite3-guardian:8b-q8_0",
          "size_text": "8.7GB",
          "size_bytes": 9341553868,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "granite3-guardian:8b-fp16",
          "size_text": "16GB",
          "size_bytes": 17179869184,
          "context": "8K",
          "input": "Text"
        }
      ],
      "tags_count": 10
    },
    {
      "slug": "megadolphin",
      "capabilities": [],
      "pulls": 38300,
      "pulls_text": "38.3K \n  Downloads",
      "blurb": "megadolphin MegaDolphin-2.2-120b is a transformation of Dolphin-2.2-70b created by interleaving the model with itself. 120b 19 Tags Updated 1 year ago",
      "name": "MegaDolphin-2.2-120b is a transformation of Dolphin-2.2-70b created by interleaving the model with itself.",
      "description": "Readme MegaDolphin-120b was inspired by the amazing Goliath and Venus models. This model was created using a method of interleaving a model with itself. References Hugging Face Write Preview <img src=\"https://github.com/jmorganca/ollama/assets/251292/374b4598-4777-44ce-b848-8591a4e0a576\" width=\"320\"/> MegaDolphin-120b was inspired by the amazing [Goliath](https://ollama.ai/library/goliath) and Venus models. This model was created using a method of interleaving a model with itself. ## References [Hugging Face](https://huggingface.co/cognitivecomputations/MegaDolphin-120b) Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "megadolphin:latest",
          "size_text": "68GB",
          "size_bytes": 73014444032,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "megadolphin:v2.2",
          "size_text": "68GB",
          "size_bytes": 73014444032,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "megadolphin:120b",
          "size_text": "68GB",
          "size_bytes": 73014444032,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "megadolphin:120b-v2.2",
          "size_text": "68GB",
          "size_bytes": 73014444032,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "megadolphin:120b-v2.2-q2_K",
          "size_text": "51GB",
          "size_bytes": 54760833024,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "megadolphin:120b-v2.2-q3_K_S",
          "size_text": "52GB",
          "size_bytes": 55834574848,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "megadolphin:120b-v2.2-q3_K_M",
          "size_text": "58GB",
          "size_bytes": 62277025792,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "megadolphin:120b-v2.2-q3_K_L",
          "size_text": "63GB",
          "size_bytes": 67645734912,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "megadolphin:120b-v2.2-q4_0",
          "size_text": "68GB",
          "size_bytes": 73014444032,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "megadolphin:120b-v2.2-q4_1",
          "size_text": "75GB",
          "size_bytes": 80530636800,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "megadolphin:120b-v2.2-q4_K_S",
          "size_text": "68GB",
          "size_bytes": 73014444032,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "megadolphin:120b-v2.2-q4_K_M",
          "size_text": "72GB",
          "size_bytes": 77309411328,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "megadolphin:120b-v2.2-q5_0",
          "size_text": "83GB",
          "size_bytes": 89120571392,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "megadolphin:120b-v2.2-q5_1",
          "size_text": "90GB",
          "size_bytes": 96636764160,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "megadolphin:120b-v2.2-q5_K_S",
          "size_text": "83GB",
          "size_bytes": 89120571392,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "megadolphin:120b-v2.2-q5_K_M",
          "size_text": "85GB",
          "size_bytes": 91268055040,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "megadolphin:120b-v2.2-q6_K",
          "size_text": "99GB",
          "size_bytes": 106300440576,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "megadolphin:120b-v2.2-q8_0",
          "size_text": "128GB",
          "size_bytes": 137438953472,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "megadolphin:120b-v2.2-fp16",
          "size_text": "241GB",
          "size_bytes": 258771779584,
          "context": "4K",
          "input": "Text"
        }
      ],
      "tags_count": 19
    },
    {
      "slug": "notux",
      "capabilities": [],
      "pulls": 37700,
      "pulls_text": "37.7K \n  Downloads",
      "blurb": "notux A top-performing mixture of experts model, fine-tuned with high-quality data. 8x7b 18 Tags Updated 1 year ago",
      "name": "A top-performing mixture of experts model, fine-tuned with high-quality data.",
      "description": "Readme This model is a fine-tuned version of Mixtral using a high-quality, curated dataset. As of Dec 26th 2023, this model is the top ranked MoE (Mixture of Experts) model on the Hugging Face Open LLM Leaderboard . References HuggingFace Argilla Write Preview <img src=\"https://github.com/jmorganca/ollama/assets/251292/4d57573b-8a00-46ea-8d62-e1e6f0bd1411\" width=\"280\" /> This model is a fine-tuned version of [Mixtral](https://ollama.ai/library/mixtral) using a high-quality, curated dataset. As of Dec 26th 2023, this model is the top ranked MoE (Mixture of Experts) model on the [Hugging Face Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard). ## References [HuggingFace](https://huggingface.co/argilla/notux-8x7b-v1) [Argilla](https://argilla.io/) Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "notux:latest",
          "size_text": "26GB",
          "size_bytes": 27917287424,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "notux:8x7b",
          "size_text": "26GB",
          "size_bytes": 27917287424,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "notux:8x7b-v1",
          "size_text": "26GB",
          "size_bytes": 27917287424,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "notux:8x7b-v1-q2_K",
          "size_text": "16GB",
          "size_bytes": 17179869184,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "notux:8x7b-v1-q3_K_S",
          "size_text": "20GB",
          "size_bytes": 21474836480,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "notux:8x7b-v1-q3_K_M",
          "size_text": "20GB",
          "size_bytes": 21474836480,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "notux:8x7b-v1-q3_K_L",
          "size_text": "20GB",
          "size_bytes": 21474836480,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "notux:8x7b-v1-q4_0",
          "size_text": "26GB",
          "size_bytes": 27917287424,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "notux:8x7b-v1-q4_1",
          "size_text": "29GB",
          "size_bytes": 31138512896,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "notux:8x7b-v1-q4_K_S",
          "size_text": "26GB",
          "size_bytes": 27917287424,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "notux:8x7b-v1-q4_K_M",
          "size_text": "26GB",
          "size_bytes": 27917287424,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "notux:8x7b-v1-q5_0",
          "size_text": "32GB",
          "size_bytes": 34359738368,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "notux:8x7b-v1-q5_1",
          "size_text": "35GB",
          "size_bytes": 37580963840,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "notux:8x7b-v1-q5_K_S",
          "size_text": "32GB",
          "size_bytes": 34359738368,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "notux:8x7b-v1-q5_K_M",
          "size_text": "32GB",
          "size_bytes": 34359738368,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "notux:8x7b-v1-q6_K",
          "size_text": "38GB",
          "size_bytes": 40802189312,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "notux:8x7b-v1-q8_0",
          "size_text": "50GB",
          "size_bytes": 53687091200,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "notux:8x7b-v1-fp16",
          "size_text": "93GB",
          "size_bytes": 99857989632,
          "context": "32K",
          "input": "Text"
        }
      ],
      "tags_count": 18
    },
    {
      "slug": "open-orca-platypus2",
      "capabilities": [],
      "pulls": 37500,
      "pulls_text": "37.5K \n  Downloads",
      "blurb": "open-orca-platypus2 Merge of the Open Orca OpenChat model and the Garage-bAInd Platypus 2 model. Designed for chat and code generation. 13b 17 Tags Updated 2 years ago",
      "name": "Merge of the Open Orca OpenChat model and the Garage-bAInd Platypus 2 model. Designed for chat and code generation.",
      "description": "Readme The OpenOrca Platypus2 model is a 13 billion parameter model which is a merge of the OpenOrca OpenChat model and the Garage-bAInd Platypus2-13B model which are both fine tunings of the Llama 2 model. It is designed to be a general-use model that can be used for chat, text generation, and code generation. Get started with OpenOrca Platypus 2 The model used in the example below is the OpenOrca Platypus 2 model, with 13b parameters, which is a general-use model. API Start Ollama server (Run ollama serve ) Run the model curl -X POST http://localhost:11434/api/generate -d '{ \"model\": \"open-orca-platypus2\", \"prompt\":\"Tell me a joke about ropes.\" }' CLI Install Ollama Open the terminal and run ollama run open-orca-platypus2 Note: The ollama run command performs an ollama pull if the model is not already downloaded. To download the model without running it, use ollama pull open-orca-platypus2 Memory requirements 13b models generally require at least 16GB of RAM If you run into issues with higher quantization levels, try using the q4 model or shut down any other programs that are using a lot of memory. Model variants By default, Ollama uses 4-bit quantization. To try other quantization levels, please try the other tags. The number after the q represents the number of bits used for quantization (i.e. q4 means 4-bit quantization). The higher the number, the more accurate the model is, but the slower it runs, and the more memory it requires. Aliases latest, 13b, 13b-q4_0 Model source OpenOrca Platypus 2 source on Ollama 13b parameters source: OpenOrca References Platypus: Quick, Cheap, and Powerful Refinement of LLMs Write Preview <img src=\"https://user-images.githubusercontent.com/633681/262757252-66e5d8a6-40d8-4cd8-b4b9-30d3f7dc1466.png\" style=\"max-width:60%\" /> The OpenOrca Platypus2 model is a 13 billion parameter model which is a merge of the OpenOrca OpenChat model and the Garage-bAInd Platypus2-13B model which are both fine tunings of the Llama 2 model. It is desi",
      "variants": [
        {
          "tag": "open-orca-platypus2:latest",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "open-orca-platypus2:13b",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "open-orca-platypus2:13b-q2_K",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "open-orca-platypus2:13b-q3_K_S",
          "size_text": "5.7GB",
          "size_bytes": 6120328396,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "open-orca-platypus2:13b-q3_K_M",
          "size_text": "6.3GB",
          "size_bytes": 6764573491,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "open-orca-platypus2:13b-q3_K_L",
          "size_text": "6.9GB",
          "size_bytes": 7408818585,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "open-orca-platypus2:13b-q4_0",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "open-orca-platypus2:13b-q4_1",
          "size_text": "8.2GB",
          "size_bytes": 8804682956,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "open-orca-platypus2:13b-q4_K_S",
          "size_text": "7.4GB",
          "size_bytes": 7945689497,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "open-orca-platypus2:13b-q4_K_M",
          "size_text": "7.9GB",
          "size_bytes": 8482560409,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "open-orca-platypus2:13b-q5_0",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "open-orca-platypus2:13b-q5_1",
          "size_text": "9.8GB",
          "size_bytes": 10522669875,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "open-orca-platypus2:13b-q5_K_S",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "open-orca-platypus2:13b-q5_K_M",
          "size_text": "9.2GB",
          "size_bytes": 9878424780,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "open-orca-platypus2:13b-q6_K",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "open-orca-platypus2:13b-q8_0",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "open-orca-platypus2:13b-fp16",
          "size_text": "26GB",
          "size_bytes": 27917287424,
          "context": "4K",
          "input": "Text"
        }
      ],
      "tags_count": 17
    },
    {
      "slug": "notus",
      "capabilities": [],
      "pulls": 36900,
      "pulls_text": "36.9K \n  Downloads",
      "blurb": "notus A 7B chat model fine-tuned with high-quality data and based on Zephyr. 7b 18 Tags Updated 1 year ago",
      "name": "A 7B chat model fine-tuned with high-quality data and based on Zephyr.",
      "description": "Readme Notus is a model by Argilla , fine-tuned from Zephyr using a high-quality dataset. References HuggingFace Argilla Write Preview <img src=\"https://github.com/jmorganca/ollama/assets/251292/4d57573b-8a00-46ea-8d62-e1e6f0bd1411\" width=\"280\" /> Notus is a model by [Argilla](https://argilla.io/), fine-tuned from [Zephyr](https://ollama.ai/library/zephyr) using a high-quality dataset. ## References [HuggingFace](https://huggingface.co/argilla/notus-7b-v1) [Argilla](https://argilla.io/) Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "notus:latest",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "notus:7b",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "notus:7b-v1",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "notus:7b-v1-q2_K",
          "size_text": "3.1GB",
          "size_bytes": 3328599654,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "notus:7b-v1-q3_K_S",
          "size_text": "3.2GB",
          "size_bytes": 3435973836,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "notus:7b-v1-q3_K_M",
          "size_text": "3.5GB",
          "size_bytes": 3758096384,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "notus:7b-v1-q3_K_L",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "notus:7b-v1-q4_0",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "notus:7b-v1-q4_1",
          "size_text": "4.6GB",
          "size_bytes": 4939212390,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "notus:7b-v1-q4_K_S",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "notus:7b-v1-q4_K_M",
          "size_text": "4.4GB",
          "size_bytes": 4724464025,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "notus:7b-v1-q5_0",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "notus:7b-v1-q5_1",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "notus:7b-v1-q5_K_S",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "notus:7b-v1-q5_K_M",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "notus:7b-v1-q6_K",
          "size_text": "5.9GB",
          "size_bytes": 6335076761,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "notus:7b-v1-q8_0",
          "size_text": "7.7GB",
          "size_bytes": 8267812044,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "notus:7b-v1-fp16",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "32K",
          "input": "Text"
        }
      ],
      "tags_count": 18
    },
    {
      "slug": "firefunction-v2",
      "capabilities": [
        "tools"
      ],
      "pulls": 36800,
      "pulls_text": "36.8K \n  Downloads",
      "blurb": "firefunction-v2 An open weights function calling model based on Llama 3, competitive with GPT-4o function calling capabilities. 70b 17 Tags Updated 1 year ago",
      "name": "An open weights function calling model based on Llama 3, competitive with GPT-4o function calling capabilities.",
      "description": "Readme Firefunction-v2 is competitive with GPT-4o function calling capabilities, scoring 0.81 on a medley public benchmarks vs 0.80 for GPT-4o. Firefunction-v2 is optimized for real world scenarios including multi-turn conversation, instruction following and parallel function calling. It retains Llama 3’s multi-turn instruction capability (0.84 vs 0.89 on MT bench) while consistently outscoring Llama 3 on function calling tasks (0.51 vs 0.30 on Nexus parallel multi function eval) References Blog Post Hugging Face Write Preview <img src=\"https://ollama.com/assets/library/firefunction-v2/a2b66a3e-ff43-4eae-b95c-d71b669384a5\" width=\"280\" /> Firefunction-v2 is competitive with GPT-4o function calling capabilities, scoring 0.81 on a medley public benchmarks vs 0.80 for GPT-4o. Firefunction-v2 is optimized for real world scenarios including multi-turn conversation, instruction following and parallel function calling. It retains Llama 3’s multi-turn instruction capability (0.84 vs 0.89 on MT bench) while consistently outscoring Llama 3 on function calling tasks (0.51 vs 0.30 on Nexus parallel multi function eval) ## References [Blog Post](https://fireworks.ai/blog/firefunction-v2-launch-post) [Hugging Face](https://huggingface.co/fireworks-ai/llama-3-firefunction-v2) Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "firefunction-v2:latest",
          "size_text": "40GB",
          "size_bytes": 42949672960,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "firefunction-v2:70b",
          "size_text": "40GB",
          "size_bytes": 42949672960,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "firefunction-v2:70b-q2_K",
          "size_text": "26GB",
          "size_bytes": 27917287424,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "firefunction-v2:70b-q3_K_S",
          "size_text": "31GB",
          "size_bytes": 33285996544,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "firefunction-v2:70b-q3_K_M",
          "size_text": "34GB",
          "size_bytes": 36507222016,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "firefunction-v2:70b-q3_K_L",
          "size_text": "37GB",
          "size_bytes": 39728447488,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "firefunction-v2:70b-q4_0",
          "size_text": "40GB",
          "size_bytes": 42949672960,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "firefunction-v2:70b-q4_1",
          "size_text": "44GB",
          "size_bytes": 47244640256,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "firefunction-v2:70b-q4_K_S",
          "size_text": "40GB",
          "size_bytes": 42949672960,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "firefunction-v2:70b-q4_K_M",
          "size_text": "43GB",
          "size_bytes": 46170898432,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "firefunction-v2:70b-q5_0",
          "size_text": "49GB",
          "size_bytes": 52613349376,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "firefunction-v2:70b-q5_1",
          "size_text": "53GB",
          "size_bytes": 56908316672,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "firefunction-v2:70b-q5_K_S",
          "size_text": "49GB",
          "size_bytes": 52613349376,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "firefunction-v2:70b-q5_K_M",
          "size_text": "50GB",
          "size_bytes": 53687091200,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "firefunction-v2:70b-q6_K",
          "size_text": "58GB",
          "size_bytes": 62277025792,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "firefunction-v2:70b-q8_0",
          "size_text": "75GB",
          "size_bytes": 80530636800,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "firefunction-v2:70b-fp16",
          "size_text": "141GB",
          "size_bytes": 151397597184,
          "context": "8K",
          "input": "Text"
        }
      ],
      "tags_count": 17
    },
    {
      "slug": "sailor2",
      "capabilities": [],
      "pulls": 36400,
      "pulls_text": "36.4K \n  Downloads",
      "blurb": "sailor2 Sailor2 are multilingual language models made for South-East Asia. Available in 1B, 8B, and 20B parameter sizes. 1b 8b 20b 13 Tags Updated 11 months ago",
      "name": "Sailor2 are multilingual language models made for South-East Asia. Available in 1B, 8B, and 20B parameter sizes.",
      "description": "Readme Sailor2 is a community-driven initiative that brings cutting-edge multilingual language models to South-East Asia (SEA). Our research highlights a strong demand for models in the 8B and 20B parameter range for production use, alongside 1B models for specialized applications, such as speculative decoding and research purposes. These models, released under the Apache 2.0 license , provide enhanced accessibility to advanced language technologies across the region. Sailor2 builds upon the foundation of the awesome multilingual model Qwen 2.5 and is continuously pre-trained on 500B tokens to support 15 languages better with a unified model. These languages include English, Chinese, Burmese, Cebuano, Ilocano, Indonesian, Javanese, Khmer, Lao, Malay, Sundanese, Tagalog, Thai, Vietnamese, and Waray. By addressing the growing demand for diverse, robust, and accessible language models, Sailor2 seeks to serve the underserved in SEA areas with open, inclusive, and accessible multilingual LLMs. The Sailor2 model comes in three sizes, 1B, 8B, and 20B, which are expanded from the Qwen2.5 base models of 0.5B, 7B, and 14B, respectively. Write Preview ![logo](/assets/mchiang0610/sailor2/a76a9182-cc11-47e1-bb50-478ad4ccb157) Sailor2 is a community-driven initiative that brings cutting-edge multilingual language models to South-East Asia (SEA). Our research highlights a strong demand for models in the **8B and 20B** parameter range for production use, alongside **1B models** for specialized applications, such as speculative decoding and research purposes. These models, released under the **Apache 2.0 license**, provide enhanced accessibility to advanced language technologies across the region. Sailor2 builds upon the foundation of the awesome multilingual model Qwen 2.5 and is continuously pre-trained on 500B tokens to support 15 languages better with a unified model. These languages include English, Chinese, Burmese, Cebuano, Ilocano, Indonesian, Javanese, Khmer, Lao, Malay, Su",
      "variants": [
        {
          "tag": "sailor2:latest",
          "size_text": "5.2GB",
          "size_bytes": 5583457484,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "sailor2:1b",
          "size_text": "1.1GB",
          "size_bytes": 1181116006,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "sailor2:8b",
          "size_text": "5.2GB",
          "size_bytes": 5583457484,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "sailor2:20b",
          "size_text": "12GB",
          "size_bytes": 12884901888,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "sailor2:1b-chat-q4_K_M",
          "size_text": "739MB",
          "size_bytes": 774897664,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "sailor2:1b-chat-q8_0",
          "size_text": "1.1GB",
          "size_bytes": 1181116006,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "sailor2:1b-chat-fp16",
          "size_text": "2.0GB",
          "size_bytes": 2147483648,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "sailor2:8b-chat-q4_K_M",
          "size_text": "5.2GB",
          "size_bytes": 5583457484,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "sailor2:8b-chat-q8_0",
          "size_text": "9.1GB",
          "size_bytes": 9771050598,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "sailor2:8b-chat-fp16",
          "size_text": "17GB",
          "size_bytes": 18253611008,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "sailor2:20b-chat-q4_K_M",
          "size_text": "12GB",
          "size_bytes": 12884901888,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "sailor2:20b-chat-q8_0",
          "size_text": "20GB",
          "size_bytes": 21474836480,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "sailor2:20b-chat-fp16",
          "size_text": "38GB",
          "size_bytes": 40802189312,
          "context": "32K",
          "input": "Text"
        }
      ],
      "tags_count": 13
    },
    {
      "slug": "goliath",
      "capabilities": [],
      "pulls": 35400,
      "pulls_text": "35.4K \n  Downloads",
      "blurb": "goliath A language model created by combining two fine-tuned Llama 2 70B models into one. 16 Tags Updated 1 year ago",
      "name": "A language model created by combining two fine-tuned Llama 2 70B models into one.",
      "description": "Readme A large model used by merging the layers of two models: Xwin and Euryale. References HuggingFace Write Preview A large model used by merging the layers of two models: [Xwin](https://ollama.ai/library/xwinlm) and Euryale. ## References [HuggingFace](https://huggingface.co/alpindale/goliath-120b) Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "goliath:latest",
          "size_text": "66GB",
          "size_bytes": 70866960384,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "goliath:120b-q2_K",
          "size_text": "50GB",
          "size_bytes": 53687091200,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "goliath:120b-q3_K_S",
          "size_text": "51GB",
          "size_bytes": 54760833024,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "goliath:120b-q3_K_M",
          "size_text": "56GB",
          "size_bytes": 60129542144,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "goliath:120b-q3_K_L",
          "size_text": "62GB",
          "size_bytes": 66571993088,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "goliath:120b-q4_0",
          "size_text": "66GB",
          "size_bytes": 70866960384,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "goliath:120b-q4_1",
          "size_text": "74GB",
          "size_bytes": 79456894976,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "goliath:120b-q4_K_S",
          "size_text": "66GB",
          "size_bytes": 70866960384,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "goliath:120b-q4_K_M",
          "size_text": "71GB",
          "size_bytes": 76235669504,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "goliath:120b-q5_0",
          "size_text": "81GB",
          "size_bytes": 86973087744,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "goliath:120b-q5_1",
          "size_text": "88GB",
          "size_bytes": 94489280512,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "goliath:120b-q5_K_S",
          "size_text": "81GB",
          "size_bytes": 86973087744,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "goliath:120b-q5_K_M",
          "size_text": "83GB",
          "size_bytes": 89120571392,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "goliath:120b-q6_K",
          "size_text": "97GB",
          "size_bytes": 104152956928,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "goliath:120b-q8_0",
          "size_text": "125GB",
          "size_bytes": 134217728000,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "goliath:120b-fp16",
          "size_text": "236GB",
          "size_bytes": 253403070464,
          "context": "4K",
          "input": "Text"
        }
      ],
      "tags_count": 16
    },
    {
      "slug": "alfred",
      "capabilities": [],
      "pulls": 30000,
      "pulls_text": "30K \n  Downloads",
      "blurb": "alfred A robust conversational model designed to be used for both chat and instruct use cases. 40b 7 Tags Updated 1 year ago",
      "name": "A robust conversational model designed to be used for both chat and instruct use cases.",
      "description": "Readme Alfred is a robust conversational model created by LightOn, based on the Falcon 40B model. It’s designed to be used for both chat and instruct use cases. Some of its features include: Reduced Hallucinations: ability to minimize hallucinations, ensuring more accurate and reliable outputs. Enhanced Self-Awareness: In situations where the model lacks a definitive answer, Alfred-40B-1023 is now programmed to state, “I don’t know”, enhancing its transparency and trustworthiness. Superior ‘Chat with Docs’ Capability: Alfred-40B-1023 is trained to perform ‘Chat with Docs’ tasks like no other, streamlining document interaction and information retrieval. Expanded Context: With an increased context of 8K tokens, Alfred-40B-1023 can comprehend and generate longer and more intricate content, ensuring detailed and comprehensive responses. References HuggingFace Blog Post Write Preview <img src=\"https://github.com/jmorganca/ollama/assets/251292/a689335b-a2a1-4be9-a132-aaaf7171e6b8\" width=\"300\"> Alfred is a robust conversational model created by LightOn, based on the [Falcon](https://ollama.ai/library/falcon) 40B model. It's designed to be used for both chat and instruct use cases. Some of its features include: * Reduced Hallucinations: ability to minimize hallucinations, ensuring more accurate and reliable outputs. * Enhanced Self-Awareness: In situations where the model lacks a definitive answer, Alfred-40B-1023 is now programmed to state, \"I don't know\", enhancing its transparency and trustworthiness. * Superior 'Chat with Docs' Capability: Alfred-40B-1023 is trained to perform 'Chat with Docs' tasks like no other, streamlining document interaction and information retrieval. * Expanded Context: With an increased context of 8K tokens, Alfred-40B-1023 can comprehend and generate longer and more intricate content, ensuring detailed and comprehensive responses. ## References [HuggingFace](https://huggingface.co/lightonai/alfred-40b-1023) [Blog Post](https://www.lighton.ai/bl",
      "variants": [
        {
          "tag": "alfred:latest",
          "size_text": "24GB",
          "size_bytes": 25769803776,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "alfred:40b",
          "size_text": "24GB",
          "size_bytes": 25769803776,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "alfred:40b-1023-q4_0",
          "size_text": "24GB",
          "size_bytes": 25769803776,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "alfred:40b-1023-q4_1",
          "size_text": "26GB",
          "size_bytes": 27917287424,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "alfred:40b-1023-q5_0",
          "size_text": "29GB",
          "size_bytes": 31138512896,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "alfred:40b-1023-q5_1",
          "size_text": "32GB",
          "size_bytes": 34359738368,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "alfred:40b-1023-q8_0",
          "size_text": "44GB",
          "size_bytes": 47244640256,
          "context": "2K",
          "input": "Text"
        }
      ],
      "tags_count": 7
    },
    {
      "slug": "command-r7b-arabic",
      "capabilities": [
        "tools"
      ],
      "pulls": 21500,
      "pulls_text": "21.5K \n  Downloads",
      "blurb": "command-r7b-arabic A new state-of-the-art version of the lightweight Command R7B model that excels in advanced Arabic language capabilities for enterprises in the Middle East and Northern Africa. 7b 5 Tags Updated 8 months ago",
      "name": "A new state-of-the-art version of the lightweight Command R7B model that excels in advanced Arabic language capabilities for enterprises in the Middle East and Northern Africa.",
      "description": "Readme Command R7B Arabic is a fast and highly efficient model that can be served on low-end GPUs, a MacBook, or even CPUs. Similar to other models in the R series, it offers a context length of 128k and industry-leading performance in its class across capabilities that matter most to businesses like regional language understanding and strong accuracy with citations using retrieval-augmented generation (RAG). Its compact size enables businesses to more easily scale Arabic language AI applications to production. References Blog Post Hugging Face Write Preview ![Hero--1-.webp](/assets/library/command-r7b-arabic/e96310ca-5a9a-4e86-8eda-5a0a68513fc6) Command R7B Arabic is a fast and highly efficient model that can be served on low-end GPUs, a MacBook, or even CPUs. Similar to other models in the R series, it offers a context length of 128k and industry-leading performance in its class across capabilities that matter most to businesses like regional language understanding and strong accuracy with citations using retrieval-augmented generation (RAG). Its compact size enables businesses to more easily scale Arabic language AI applications to production. ## References [Blog Post](https://cohere.com/blog/command-r7b-arabic) [Hugging Face](https://huggingface.co/CohereForAI/c4ai-command-r7b-arabic-02-2025) Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "command-r7b-arabic:latest",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "command-r7b-arabic:7b",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "command-r7b-arabic:7b-02-2025-q4_K_M",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "command-r7b-arabic:7b-02-2025-q8_0",
          "size_text": "8.5GB",
          "size_bytes": 9126805504,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "command-r7b-arabic:7b-02-2025-fp16",
          "size_text": "16GB",
          "size_bytes": 17179869184,
          "context": "16K",
          "input": "Text"
        }
      ],
      "tags_count": 5
    },
    {
      "slug": "glm-4.6",
      "capabilities": [
        "reasoning"
      ],
      "pulls": 13300,
      "pulls_text": "13.3K \n  Downloads",
      "blurb": "glm-4.6 Advanced agentic, and coding capabilities. cloud 1 Tag Updated 2 weeks ago",
      "name": "Advanced agentic, reasoning and coding capabilities.",
      "description": "Readme Model Introduction GLM-4.6 brings several key improvements: Longer context window: The context window has been expanded from 128K to 200K tokens, enabling the model to handle more complex agentic tasks. Superior coding performance: The model achieves higher scores on code benchmarks and demonstrates better real-world performance in applications such as Claude Code、Cline、Roo Code and Kilo Code, including improvements in generating visually polished front-end pages. Advanced reasoning: GLM-4.6 shows a clear improvement in reasoning performance and supports tool use during inference, leading to stronger overall capability. More capable agents: GLM-4.6 exhibits stronger performance in tool using and search-based agents, and integrates more effectively within agent frameworks. Refined writing: Better aligns with human preferences in style and readability, and performs more naturally in role-playing scenarios. We evaluated GLM-4.6 across eight public benchmarks covering agents, reasoning, and coding. Results show clear gains over GLM-4.5, with GLM-4.6 also holding competitive advantages over leading domestic and international models such as DeepSeek-V3.1-Terminus and Claude Sonnet 4 . Write Preview <img src=\"/assets/library/glm-4.6/a2820004-9cb2-45ea-af09-7074224ddd49\" width=\"120\" /> ## Model Introduction **GLM-4.6** brings several key improvements: * **Longer context window:** The context window has been expanded from 128K to 200K tokens, enabling the model to handle more complex agentic tasks. * **Superior coding performance:** The model achieves higher scores on code benchmarks and demonstrates better real-world performance in applications such as Claude Code、Cline、Roo Code and Kilo Code, including improvements in generating visually polished front-end pages. * **Advanced reasoning:** GLM-4.6 shows a clear improvement in reasoning performance and supports tool use during inference, leading to stronger overall capability. * **More capable agents:** GLM-4.6 exhibi",
      "variants": [
        {
          "tag": "glm-4.6:cloud",
          "size_text": null,
          "size_bytes": null,
          "context": "198K",
          "input": "Text"
        }
      ],
      "tags_count": 1
    },
    {
      "slug": "kimi-k2",
      "capabilities": [],
      "pulls": 418,
      "pulls_text": "418 \n  Downloads",
      "blurb": "kimi-k2 A state-of-the-art mixture-of-experts (MoE) language model. Kimi K2-Instruct-0905 demonstrates significant improvements in performance on public benchmarks and real-world coding agent tasks. cloud 9, 1 Tag Updated 1 month ago",
      "name": "A state-of-the-art mixture-of-experts (MoE) language model. Kimi K2-Instruct-0905 demonstrates significant improvements in performance on public benchmarks and real-world coding agent tasks.",
      "description": "Readme Kimi K2-Instruct-0905 is the latest, most capable version of Kimi K2. It is a state-of-the-art mixture-of-experts (MoE) language model, featuring 32 billion activated parameters and a total of 1 trillion parameters. Key Features Enhanced agentic coding intelligence: Kimi K2-Instruct-0905 demonstrates significant improvements in performance on public benchmarks and real-world coding agent tasks. Improved frontend coding experience: Kimi K2-Instruct-0905 offers advancements in both the aesthetics and practicality of frontend programming. Extended context length: Kimi K2-Instruct-0905’s context window has been increased from 128k to 256k tokens, providing better support for long-horizon tasks. Write Preview <img src=\"/assets/library/kimi-k2/02232216-54d9-449f-8e3a-1054f17f3b18\" width=\"320\" /> Kimi K2-Instruct-0905 is the latest, most capable version of Kimi K2. It is a state-of-the-art mixture-of-experts (MoE) language model, featuring 32 billion activated parameters and a total of 1 trillion parameters. ### Key Features - Enhanced agentic coding intelligence: Kimi K2-Instruct-0905 demonstrates significant improvements in performance on public benchmarks and real-world coding agent tasks. - Improved frontend coding experience: Kimi K2-Instruct-0905 offers advancements in both the aesthetics and practicality of frontend programming. - Extended context length: Kimi K2-Instruct-0905’s context window has been increased from 128k to 256k tokens, providing better support for long-horizon tasks. Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "kimi-k2:1t-cloud",
          "size_text": null,
          "size_bytes": null,
          "context": "256K",
          "input": "Text"
        }
      ],
      "tags_count": 1
    },
    {
      "slug": "gpt-oss-safeguard",
      "capabilities": [
        "reasoning",
        "thinking",
        "tools"
      ],
      "pulls": 283,
      "pulls_text": "283 \n  Downloads",
      "blurb": "gpt-oss-safeguard gpt-oss-safeguard-20b and gpt-oss-safeguard-120b are safety models built-upon gpt-oss 20b 120b 1, 3 Tags Updated 3 days ago",
      "name": "gpt-oss-safeguard-20b and gpt-oss-safeguard-120b are safety reasoning models built-upon gpt-oss",
      "description": "Readme Get started 20B: ollama run gpt-oss-safeguard:20b This model is designed to fit into GPUs with 16GB of VRAM. (21B parameters with 3.6B active parameters). 120B: ollama run gpt-oss-safeguard:120b This model is designed to fit into a single NVIDIA H100 GPU (117B parameters with 5.1B active parameters). Highlights Trained to reason about safety : Trained and tuned for safety reasoning to accommodate use cases like LLM input-output filtering, online content labeling and offline labeling for Trust and Safety use cases. Bring your own policy: Interprets your written policy, so it generalizes across products and use cases with minimal engineering. Reasoned decisions, not just scores: Gain complete access to the model’s reasoning process, facilitating easier debugging and increased trust in policy decisions. Keep in mind Raw CoT is meant for developers and safety practitioners. It’s not intended for exposure to general users or use cases outside of safety contexts. Configurable reasoning effort: Easily adjust the reasoning effort (low, medium, high) based on your specific use case and latency needs. Permissive Apache 2.0 license: Build freely without copyleft restrictions or patent risk—ideal for experimentation, customization, and commercial deployment. Join the ROOST Model Community gpt-oss-safeguard is a model partner of the Robust Open Online Safety Tools (ROOST) Model Community. The ROOST Model Community (RMC) is a group of safety practitioners exploring open source AI models to protect online spaces. As an RMC model partner, OpenAI is committed to incorporating user feedback and jointly iterating on future releases in pursuit of open safety. Visit the RMC GitHub repo to learn more about this partnership and how to get involved. References OpenAI blog OpenAI gpt-oss-safeguard developer cookbook ROOST’s model community repository on GitHub Write Preview ### Get started **20B:** ``` ollama run gpt-oss-safeguard:20b ``` This model is designed to fit into GPUs with ",
      "variants": [
        {
          "tag": "gpt-oss-safeguard:latest",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "gpt-oss-safeguard:20b",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "gpt-oss-safeguard:120b",
          "size_text": "65GB",
          "size_bytes": 69793218560,
          "context": "128K",
          "input": "Text"
        }
      ],
      "tags_count": 3
    },
    {
      "slug": "minimax-m2",
      "capabilities": [
        "thinking",
        "tools"
      ],
      "pulls": 195,
      "pulls_text": "195 \n  Downloads",
      "blurb": "minimax-m2 MiniMax M2 is a high-efficiency large language model built for coding and agentic workflows. cloud 8, 1 Tag Updated 3 days ago",
      "name": "MiniMax M2 is a high-efficiency large language model built for coding and agentic workflows.",
      "description": "Readme MiniMax M2 is a high-efficiency large language model built for coding and agentic workflows. Get started ollama run minimax-m2:cloud Highlights Superior Intelligence . According to benchmarks from Artificial Analysis, MiniMax-M2 demonstrates highly competitive general intelligence across mathematics, science, instruction following, coding, and agentic tool use. Its composite score ranks #1 among open-source models globally . Advanced Coding . Engineered for end-to-end developer workflows, MiniMax-M2 excels at multi-file edits, coding-run-fix loops, and test-validated repairs. Strong performance on Terminal-Bench and (Multi-)SWE-Bench–style tasks demonstrates practical effectiveness in terminals, IDEs, and CI across languages. Agent Performance . MiniMax-M2 plans and executes complex, long-horizon toolchains across shell, browser, retrieval, and code runners. In BrowseComp-style evaluations, it consistently locates hard-to-surface sources, maintains evidence traceable, and gracefully recovers from flaky steps. Efficient Design . With 10 billion activated parameters (230 billion in total), MiniMax-M2 delivers lower latency, lower cost, and higher throughput for interactive agents and batched sampling—perfectly aligned with the shift toward highly deployable models that still shine on coding and agentic tasks. Coding & Agentic Benchmarks These comprehensive evaluations test real-world end-to-end coding and agentic tool use: editing real repos, executing commands, browsing the web, and delivering functional solutions. Performance on this suite correlates with day-to-day developer experience in terminals, IDEs, and CI. Benchmark MiniMax-M2 Claude Sonnet 4 Claude Sonnet 4.5 Gemini 2.5 Pro GPT-5 (thinking) GLM-4.6 Kimi K2 0905 DeepSeek-V3.2 SWE-bench Verified 69.4 72.7 * 77.2 * 63.8 * 74.9 * 68 * 69.2 * 67.8 * Multi-SWE-Bench 36.2 35.7 * 44.3 / / 30 33.5 30.6 SWE-bench Multilingual 56.5 56.9 * 68 / / 53.8 55.9 * 57.9 * Terminal-Bench 46.3 36.4 * 50 * 25.3 * 43.8 * 4",
      "variants": [
        {
          "tag": "minimax-m2:cloud",
          "size_text": null,
          "size_bytes": null,
          "context": "200K",
          "input": "Text"
        }
      ],
      "tags_count": 1
    }
  ]
}